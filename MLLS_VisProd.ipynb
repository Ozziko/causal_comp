{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* wandb\n",
    "* print -> logger.info\n",
    "* cache loading\n",
    "* add closed metric in neural training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"vscode\"\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join as pjoin\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from time import sleep\n",
    "from simple_parsing import Serializable\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring, Checkpoint\n",
    "from skorch.dataset import ValidSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VisProd(Serializable):\n",
    "    # decoupling_loss: str = 'MI g.t.' # calculating MI loss with ground-truth labels for joint dist\n",
    "    # lambda_decoupling: float = 0\n",
    "    \n",
    "    # MI_mode: str = 'MI est' # training an additional net for joint classification, to calculate MI loss with estimated probability for joint dist\n",
    "    # lambda_decoupling: float = 10\n",
    "    # lambda_joint: float = 1\n",
    "\n",
    "    # decoupling_loss: str = 'HSIC,L' # Yuval's HSIC with Linear kernel\n",
    "    decoupling_loss: str = 'HSIC,G' # Yuval's HSIC with Gaussian kernel and std=1\n",
    "    # decoupling_loss: str = 'HSIC,N' # ProtoProp's normalized Gaussian HSIC\n",
    "    lambda_decoupling: float = 3e5\n",
    "\n",
    "    epochs: int = 50\n",
    "    lambda_color: float = 1\n",
    "    lambda_shape: float = 1\n",
    "\n",
    "    optimizer: str = 'Adam'\n",
    "    lr: float = 1e-2\n",
    "    # optimizer: str = 'SGD'\n",
    "    # lr: float = 1e-4\n",
    "\n",
    "    early_stop_phase: str = 'val' # also for plateau scheduler\n",
    "    early_stop_on: str = 'harmonic acc' # also for plateau scheduler\n",
    "    # early_stop_mode: str = 'off'\n",
    "    early_stop_mode: str = 'max'\n",
    "    early_stop_patience: int = 8\n",
    "    early_stop_threshold: float = 1e-3 # abs diff \n",
    "    \n",
    "    # plateau_mode: str = 'off'\n",
    "    plateau_mode: str = 'max'\n",
    "    plateau_factor: float = 0.2\n",
    "    plateau_patience: int = 5\n",
    "    plateau_threshold: float = 1e-3\n",
    "    plateau_threshold_mode: str = 'abs'\n",
    "    # plateau_threshold_mode: str = 'rel'\n",
    "\n",
    "    calibrate: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args(Serializable):\n",
    "    outputs_dir: str = 'analysis/MLLS/HSIC_long'\n",
    "\n",
    "    dataset_name: str = 'ao_clevr'\n",
    "    data_dir: str = '' # empty = f'data/{args.dataset_name}'\n",
    "    dataset_variant: str = 'VT'\n",
    "    # dataset_variant: str = 'OZ'\n",
    "    \n",
    "    seen_seed:int = 0\n",
    "    num_split:int = 5000\n",
    "    \n",
    "    init_seed: int = 2 # for torch seeding\n",
    "\n",
    "    # val_unseen_mode: str = 'leave' # leave Yuval's val as is (with 5 images for unseen combinations)\n",
    "    val_unseen_mode: str = 'complete' # add images to val unseen combinations to reach the mean val size in seen combinations \n",
    "    # val_unseen_mode: str = 'drop' # drop any val images from unseen combinations \n",
    "\n",
    "\n",
    "    # only for OZ splits\n",
    "    train_size: int = int(80e3)\n",
    "    train_unseen_ovr_seen: float = 0\n",
    "    test_size: int = int(8e3)\n",
    "\n",
    "    # for training\n",
    "    skorch_val_ratio: float = 0.3 # split is done inside skorch training; NOT related to 'val' dataset used for calibration, hypertuning\n",
    "    batch_size: int = 1024 # also for sampling multi-shifts (multi-shifted train) for MLLS training\n",
    "    pin_memory: bool = True\n",
    "    workers: int = 0\n",
    "\n",
    "    VP: VisProd = VisProd()\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data_dir == '':\n",
    "    args.data_dir = f'data/{args.dataset_name}'\n",
    "    \n",
    "num_split_str = str(args.num_split)\n",
    "unseen_ovr_tot = int(num_split_str[:2])/100\n",
    "data_seed = int(num_split_str[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_setup(deterministic: bool = True):\n",
    "    # Setting up torch, seeding (https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "    # ///////////// Making pytorch deterministic (reproducible)  ////////////////////////////////////////////////////\n",
    "    if deterministic:\n",
    "        # read WELL: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "        # the order of execution is important!\n",
    "\n",
    "        # https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n",
    "        #     os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\" # may limit overall performance\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # will increase library footprint in GPU memory by approximately 24MiB\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        print(f\"executed: torch.backends.cudnn.benchmark = False\")\n",
    "\n",
    "        torch_set_deterministic = False\n",
    "        # try:\n",
    "        #     torch.set_deterministic(True) # beta in torch==1.7\n",
    "        #     print(f\"executed torch.set_deterministic(True)\")\n",
    "        #     torch_set_deterministic = True\n",
    "        # except Exception as ex:\n",
    "        #     logger.warning(f\"torch.set_deterministic(True) failed: {ex}\")\n",
    "        try:\n",
    "            torch.use_deterministic_algorithms(True)  # beta in torch==1.8\n",
    "            print(f\"executed: torch.use_deterministic_algorithms(True)\")\n",
    "            torch_set_deterministic = True\n",
    "        except Exception as ex:\n",
    "            print(f\"torch.use_deterministic_algorithms(True) failed: {ex}\")\n",
    "\n",
    "        if not torch_set_deterministic:  # already contained in torch.use_deterministic_algorithms(True)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            print(f\"executed: torch.backends.cudnn.deterministic = True\")\n",
    "\n",
    "        print(f\"torch and cuda will be deterministic (after seeding)\")\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f\"executed: torch.backends.cudnn.benchmark = True (torch is not determinisitc!)\")\n",
    "        \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print('torch is using %s (%s)'%(device, torch.cuda.get_device_name(device=0)))\n",
    "    else:\n",
    "        print('torch is using %s'%(device))\n",
    "    return device\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed + 1000)\n",
    "    torch.manual_seed(seed + 2000)\n",
    "    torch.cuda.manual_seed(seed + 3000)\n",
    "    print(f\"completed: seeding with seed={seed} in steps of 1000 for random,np,torch,cuda\")\n",
    "\n",
    "\n",
    "def basic_EM(Y, Y_probs, prior_source, EM_iterations: int = 100):\n",
    "    # init\n",
    "    EM_results = pd.DataFrame(index=range(EM_iterations))\n",
    "    EM_results.index.name = 'iteration'\n",
    "    shift_probs_0 = Y_probs\n",
    "    shift_probs = shift_probs_0\n",
    "    prior_shift_pred = prior_source\n",
    "\n",
    "    # eval\n",
    "    shift_preds = shift_probs.argmax(axis=1)\n",
    "    positives = shift_preds==Y\n",
    "    # EM_results.loc[0, [f'accuracy ({label})' for label in range(args.n_labels)]] = positives.mean(axis=0)\n",
    "    EM_results.loc[0, 'accuracy'] = positives.mean()\n",
    "    Y_true_pos_probs = np.take_along_axis(shift_probs, Y.reshape(-1,1), axis=1)\n",
    "    EM_results.loc[0, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "    # EM\n",
    "    for i_EM in tqdm(range(1, 1+EM_iterations)):        \n",
    "        # E-step\n",
    "        shift_probs = shift_probs_0 * prior_shift_pred / prior_source\n",
    "        shift_probs = (shift_probs.T / shift_probs.sum(axis=1)).T\n",
    "        \n",
    "        shift_preds = shift_probs.argmax(axis=1)\n",
    "        \n",
    "        # eval\n",
    "        positives = shift_preds==Y\n",
    "        EM_results.loc[i_EM, 'accuracy'] = positives.mean()\n",
    "        Y_true_pos_probs = np.take_along_axis(shift_probs, Y.reshape(-1,1), axis=1)\n",
    "        EM_results.loc[i_EM, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "        # M-step\n",
    "        prior_shift_pred = shift_probs.mean(axis=0)\n",
    "    assert (~np.isfinite(shift_probs)).sum() == 0\n",
    "    return EM_results, shift_probs\n",
    "\n",
    "\n",
    "def EM(Y:dict, Y_probs:dict, EM_iterations: int = 100, prior_source_phase: str = 'est. soft train',\n",
    "        init_prior: str = 'source prior', n_combs: int = None,\n",
    "        soft_prior: bool = True, over_update_probs: bool = False):\n",
    "    \"\"\"\n",
    "    Y and Y_probs: dictionaries with 'train' and 'test' keys, EM adaptation is done on 'test'\n",
    "    soft_prior:\n",
    "        (for proper EM) True: SOFT prior_shift_pred = shift_probs.mean(axis=0)\n",
    "        (for improper EM) False: HARD prior_shift_pred (calculated from hist)\n",
    "    over_update_probs:\n",
    "        (for proper EM) False: shift_probs = shift_probs_0 * prior_shift_pred / prior_train\n",
    "        (for improper EM) True: shift_probs = shift_probs * prior_shift_pred / prior_train\n",
    "    \"\"\"\n",
    "    \n",
    "    if (not soft_prior) or over_update_probs or (prior_source_phase != 'est. soft train'):\n",
    "        confirm_ui = input(\"confirm performing an IMPROPER EM for debugging, \" \\\n",
    "            \"(for a proper EM, set: soft_prior = True, over_update_probs = False, \"\\\n",
    "            \"prior_source_phase = 'est. soft train'): y/[n] \")\n",
    "        if confirm_ui != 'y':\n",
    "            raise RuntimeError(\"terminated by user\")\n",
    "\n",
    "    if 'est. soft' in prior_source_phase:\n",
    "        Y_probs_source = Y_probs['test' if 'test' in prior_source_phase else 'train']\n",
    "        prior_source = Y_probs_source.mean(axis=0)\n",
    "    elif 'true' in prior_source_phase:\n",
    "        Y_source = Y['test' if 'test' in prior_source_phase else 'train']\n",
    "        hist = np.bincount(Y_source, minlength=n_combs)\n",
    "        prior_source = hist / hist.sum()\n",
    "    elif prior_source_phase == 'uniform':\n",
    "        prior_source = 1/n_combs * np.ones(n_combs)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    print(f\"source prior ('{prior_source_phase}'):\")\n",
    "    display(prior_source)\n",
    "\n",
    "    # init\n",
    "    EM_results = pd.DataFrame(index=range(EM_iterations))\n",
    "    EM_results.index.name = 'iteration'\n",
    "    shift_labels = Y['test']\n",
    "    shift_probs_0 = Y_probs['test']\n",
    "    shift_probs = shift_probs_0\n",
    "\n",
    "    if init_prior == 'source prior':\n",
    "        prior_shift_pred = prior_source\n",
    "    elif init_prior == 'uniform':\n",
    "        prior_shift_pred = 1/n_combs * np.ones(n_combs)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # eval\n",
    "    shift_preds = shift_probs.argmax(axis=1)\n",
    "    positives = shift_preds==shift_labels\n",
    "    # EM_results.loc[0, [f'accuracy ({label})' for label in range(args.n_labels)]] = positives.mean(axis=0)\n",
    "    EM_results.loc[0, 'accuracy'] = positives.mean()\n",
    "    # likelihood\n",
    "    Y_true_pos_probs = np.take_along_axis(shift_probs, Y['test'].reshape(-1,1), axis=1)\n",
    "    EM_results.loc[0, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "    # EM\n",
    "    for i_EM in tqdm(range(1, 1+EM_iterations)):        \n",
    "        # E-step\n",
    "        if over_update_probs:\n",
    "            shift_probs = shift_probs * prior_shift_pred / prior_source\n",
    "        else:\n",
    "            shift_probs = shift_probs_0 * prior_shift_pred / prior_source\n",
    "        shift_probs = (shift_probs.T / shift_probs.sum(axis=1)).T\n",
    "        \n",
    "        shift_preds = shift_probs.argmax(axis=1)\n",
    "        \n",
    "        # eval\n",
    "        positives = shift_preds==shift_labels\n",
    "        EM_results.loc[i_EM, 'accuracy'] = positives.mean()\n",
    "        # likelihood\n",
    "        Y_true_pos_probs = np.take_along_axis(shift_probs, Y['test'].reshape(-1,1), axis=1)\n",
    "        EM_results.loc[i_EM, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "        # M-step\n",
    "        if soft_prior:\n",
    "            prior_shift_pred = shift_probs.mean(axis=0)\n",
    "        else:\n",
    "            hist_shift_pred = np.bincount(shift_preds, minlength=args.n_classes_per_label * args.n_labels)\n",
    "            prior_shift_pred = hist_shift_pred / hist_shift_pred.sum()\n",
    "\n",
    "    assert (~np.isfinite(shift_probs)).sum() == 0\n",
    "    return EM_results, shift_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yuval's defitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant in ['VT', 'UV']:\n",
    "    from typing import NamedTuple\n",
    "    from pathlib import Path\n",
    "    from ATTOP.data.dataset import sample_negative as ATTOP_sample_negative\n",
    "    from torch.utils import data\n",
    "    from COSMO_utils import temporary_random_numpy_seed\n",
    "\n",
    "\n",
    "    def get_and_update_num_calls(func_ptr):\n",
    "        try:\n",
    "            get_and_update_num_calls.num_calls_cnt[func_ptr] += 1\n",
    "        except AttributeError as e:\n",
    "            if 'num_calls_cnt' in repr(e):\n",
    "                get_and_update_num_calls.num_calls_cnt = defaultdict(int)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        return get_and_update_num_calls.num_calls_cnt[func_ptr]\n",
    "\n",
    "    def categorical_histogram(data, labels_list, plot=True, frac=True, plt_show=False):\n",
    "        import matplotlib.pyplot as plt\n",
    "        s_counts = pd.Series(data).value_counts()\n",
    "        s_frac = s_counts/s_counts.sum()\n",
    "        hist_dict = s_counts.to_dict()\n",
    "        if frac:\n",
    "            hist_dict = s_frac.to_dict()\n",
    "        hist = []\n",
    "        for ix, _ in enumerate(labels_list):\n",
    "            hist.append(hist_dict.get(ix, 0))\n",
    "\n",
    "        if plot:\n",
    "            pd.Series(hist, index=labels_list).plot(kind='bar')\n",
    "            if frac:\n",
    "                plt.ylim((0,1))\n",
    "            if plt_show:\n",
    "                plt.show()\n",
    "        else:\n",
    "            return np.array(hist, dtype='float32')\n",
    "\n",
    "    class DataItem(NamedTuple):\n",
    "        \"\"\" A NamedTuple for returning a Dataset item \"\"\"\n",
    "        feat: torch.Tensor\n",
    "        pos_attr_id: int\n",
    "        pos_obj_id: int\n",
    "        neg_attr_id: int\n",
    "        neg_obj_id: int\n",
    "        image_fname: str\n",
    "\n",
    "\n",
    "    class CompDataFromDict():\n",
    "        # noinspection PyMissingConstructor\n",
    "        def __init__(self, dict_data: dict, data_subset: str, data_dir: str):\n",
    "\n",
    "            # define instance variables to be retrieved from struct_data_dict\n",
    "            self.split: str = 'TBD'\n",
    "            self.phase: str = 'TBD'\n",
    "            self.feat_dim: int = -1\n",
    "            self.objs: list = []\n",
    "            self.attrs: list = []\n",
    "            self.attr2idx: dict = {}\n",
    "            self.obj2idx: dict = {}\n",
    "            self.pair2idx: dict = {}\n",
    "            self.seen_pairs: list = []\n",
    "            self.all_open_pairs: list = []\n",
    "            self.closed_unseen_pairs: list = []\n",
    "            self.unseen_closed_val_pairs: list = []\n",
    "            self.unseen_closed_test_pairs: list = []\n",
    "            self.train_data: tuple = tuple()\n",
    "            self.val_data: tuple = tuple()\n",
    "            self.test_data: tuple = tuple()\n",
    "\n",
    "            self.data_dir: str = data_dir\n",
    "\n",
    "            # retrieve instance variables from struct_data_dict\n",
    "            vars(self).update(dict_data)\n",
    "            self.data = dict_data[data_subset]\n",
    "\n",
    "            self.activations = {}\n",
    "            features_dict = torch.load(Path(data_dir) / 'features.t7')\n",
    "            for i, img_filename in enumerate(features_dict['files']):\n",
    "                self.activations[img_filename] = features_dict['features'][i]\n",
    "\n",
    "            self.input_shape = (self.feat_dim,)\n",
    "            self.num_objs = len(self.objs)\n",
    "            self.num_attrs = len(self.attrs)\n",
    "            self.num_seen_pairs = len(self.seen_pairs)\n",
    "            self.shape_obj_attr = (self.num_objs, self.num_attrs)\n",
    "\n",
    "            self.flattened_seen_pairs_mask = self.get_flattened_pairs_mask(self.seen_pairs)\n",
    "            self.flattened_closed_unseen_pairs_mask = self.get_flattened_pairs_mask(self.closed_unseen_pairs)\n",
    "            self.flattened_all_open_pairs_mask = self.get_flattened_pairs_mask(self.all_open_pairs)\n",
    "            self.seen_pairs_joint_class_ids = np.where(self.flattened_seen_pairs_mask)\n",
    "\n",
    "            self.y1_freqs, self.y2_freqs, self.pairs_freqs = self._calc_freqs()\n",
    "            self._just_load_labels = False\n",
    "\n",
    "            self.train_pairs = self.seen_pairs\n",
    "\n",
    "        def sample_negative(self, attr, obj):\n",
    "            return ATTOP_sample_negative(self, attr, obj)\n",
    "\n",
    "        def get_flattened_pairs_mask(self, pairs):\n",
    "            pairs_ids = np.array([(self.obj2idx[obj], self.attr2idx[attr]) for attr, obj in pairs])\n",
    "            flattened_pairs = np.zeros(self.shape_obj_attr, dtype=bool)  # init an array of False\n",
    "            flattened_pairs[tuple(zip(*pairs_ids))] = True\n",
    "            flattened_pairs = flattened_pairs.flatten()\n",
    "            return flattened_pairs\n",
    "\n",
    "        def just_load_labels(self, just_load_labels=True):\n",
    "            self._just_load_labels = just_load_labels\n",
    "\n",
    "        def get_all_labels(self):\n",
    "            attrs = []\n",
    "            objs = []\n",
    "            joints = []\n",
    "            self.just_load_labels(True)\n",
    "            for attrs_batch, objs_batch in self:\n",
    "                if isinstance(attrs_batch, torch.Tensor):\n",
    "                    attrs_batch = attrs_batch.cpu().numpy()\n",
    "                if isinstance(objs_batch, torch.Tensor):\n",
    "                    objs_batch = objs_batch.cpu().numpy()\n",
    "                joint = self.to_joint_label(objs_batch, attrs_batch)\n",
    "\n",
    "                attrs.append(attrs_batch)\n",
    "                objs.append(objs_batch)\n",
    "                joints.append(joint)\n",
    "\n",
    "            self.just_load_labels(False)\n",
    "            attrs = np.array(attrs)\n",
    "            objs = np.array(objs)\n",
    "            return attrs, objs, joints\n",
    "\n",
    "        def _calc_freqs(self):\n",
    "            y2_train, y1_train, ys_joint_train = self.get_all_labels()\n",
    "            y1_freqs = categorical_histogram(y1_train, range(self.num_objs), plot=False, frac=True)\n",
    "            y1_freqs[y1_freqs == 0] = np.nan\n",
    "            y2_freqs = categorical_histogram(y2_train, range(self.num_attrs), plot=False, frac=True)\n",
    "            y2_freqs[y2_freqs == 0] = np.nan\n",
    "\n",
    "            pairs_freqs = categorical_histogram(ys_joint_train,\n",
    "                                                range(self.num_objs * self.num_attrs),\n",
    "                                                plot=False, frac=True)\n",
    "            pairs_freqs[pairs_freqs == 0] = np.nan\n",
    "            return y1_freqs, y2_freqs, pairs_freqs\n",
    "\n",
    "        def get(self, name):\n",
    "            return vars(self).get(name)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_fname, attr, obj = self.data[idx]\n",
    "            pos_attr_id, pos_obj_id = self.attr2idx[attr], self.obj2idx[obj]\n",
    "            if self._just_load_labels:\n",
    "                return pos_attr_id, pos_obj_id\n",
    "\n",
    "            num_calls_cnt = get_and_update_num_calls(self.__getitem__)\n",
    "\n",
    "            negative_attr_id, negative_obj_id = -1, -1  # default values\n",
    "            if self.phase == 'train':\n",
    "                # we set a temp np seed to override a weird issue with\n",
    "                # sample_negative() at __getitem__, where the sampled pairs\n",
    "                # could not be deterministically reproduced:\n",
    "                # Now at each call to _getitem_ we set the seed to a 834276 (chosen randomly) + the number of calls to _getitem_\n",
    "                with temporary_random_numpy_seed(834276 + num_calls_cnt):\n",
    "                    # draw a negative pair\n",
    "                    negative_attr_id, negative_obj_id = self.sample_negative(attr, obj)\n",
    "\n",
    "            item = DataItem(\n",
    "                feat=self.activations[image_fname],\n",
    "                pos_attr_id=pos_attr_id,\n",
    "                pos_obj_id=pos_obj_id,\n",
    "                neg_attr_id=negative_attr_id,\n",
    "                neg_obj_id=negative_obj_id,\n",
    "                image_fname=image_fname,\n",
    "            )\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def to_joint_label(self, y1_batch, y2_batch):\n",
    "            return (y1_batch * self.num_attrs + y2_batch)\n",
    "\n",
    "\n",
    "    def get_data_loaders(train_dataset, valid_dataset, test_dataset, batch_size,\n",
    "                        num_workers=10, test_batchsize=None, shuffle_eval_set=True):\n",
    "        if test_batchsize is None:\n",
    "            test_batchsize = batch_size\n",
    "\n",
    "        pin_memory = True\n",
    "        if num_workers == 0:\n",
    "            pin_memory = False\n",
    "        print('num_workers = ', num_workers)\n",
    "        print('pin_memory = ', pin_memory)\n",
    "        train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                                    pin_memory=pin_memory)\n",
    "        valid_loader = None\n",
    "        if valid_dataset is not None and len(valid_dataset) > 0:\n",
    "            valid_loader = data.DataLoader(valid_dataset, batch_size=test_batchsize, shuffle=shuffle_eval_set,\n",
    "                                        num_workers=num_workers, pin_memory=pin_memory)\n",
    "        test_loader = data.DataLoader(test_dataset, batch_size=test_batchsize, shuffle=shuffle_eval_set,\n",
    "                                    num_workers=num_workers, pin_memory=pin_memory)\n",
    "        return test_loader, train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def ns_profiling_label(label):\n",
    "    \"\"\"\n",
    "    A do nothing version of ns_profiling_label()\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        yield None\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "def pairwise_distances(x):\n",
    "  x_distances = torch.sum(x**2,-1).reshape((-1,1))\n",
    "  return -2*torch.mm(x,x.t()) + x_distances + x_distances.t() \n",
    "\n",
    "def kernelMatrixGaussian(x, sigma=1):\n",
    "\n",
    "    pairwise_distances_ = pairwise_distances(x)\n",
    "    gamma = -1.0 / (sigma ** 2)\n",
    "    return torch.exp(gamma * pairwise_distances_)\n",
    "\n",
    "def kernelMatrixLinear(x):\n",
    "  return torch.matmul(x,x.t())\n",
    "\n",
    "\n",
    "def median_pairwise_distance(X):\n",
    "    t = pairwise_distances(X).detach()\n",
    "    triu_indices = t.triu(diagonal=1).nonzero().T\n",
    "\n",
    "    if triu_indices[0].shape[0] == 0 or triu_indices[1].shape[0] == 0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return torch.median(t[triu_indices[0], triu_indices[1]]).item()\n",
    "\n",
    "def HSIC(X, Y, kernelX=\"Gaussian\", kernelY=\"Gaussian\", sigmaX=1, sigmaY=1,\n",
    "         log_median_pairwise_distance=False):\n",
    "  m,_ = X.shape\n",
    "  assert(m>1)\n",
    "\n",
    "  median_pairwise_distanceX, median_pairwise_distanceY = np.nan, np.nan\n",
    "  if log_median_pairwise_distance:\n",
    "      # This calc takes a long time. It is used for debugging and disabled by default.\n",
    "      with ns_profiling_label('dist'):\n",
    "          median_pairwise_distanceX = median_pairwise_distance(X)\n",
    "          median_pairwise_distanceY = median_pairwise_distance(Y)\n",
    "\n",
    "  with ns_profiling_label('Hkernel'):\n",
    "      K = kernelMatrixGaussian(X,sigmaX) if kernelX == \"Gaussian\" else kernelMatrixLinear(X)\n",
    "      L = kernelMatrixGaussian(Y,sigmaY) if kernelY == \"Gaussian\" else kernelMatrixLinear(Y)\n",
    "\n",
    "  with ns_profiling_label('Hfinal'):\n",
    "      H = torch.eye(m, device='cuda') - 1.0/m * torch.ones((m,m), device='cuda')\n",
    "      H = H.float().cuda()\n",
    "\n",
    "      Kc = torch.mm(H,torch.mm(K,H))\n",
    "\n",
    "      HSIC = torch.trace(torch.mm(L,Kc))/((m-1)**2)\n",
    "  return HSIC, median_pairwise_distanceX, median_pairwise_distanceY\n",
    "\n",
    "def conditional_indep_losses(repr1, repr2, ys, indep_coeff, indep_coeff2=None, num_classes1=None, num_classes2=None,\n",
    "                             Hkernel='L', Hkernel_sigma_obj=None, Hkernel_sigma_attr=None,\n",
    "                             log_median_pairwise_distance=False, device=None):\n",
    "    # check readability\n",
    "\n",
    "    normalize_to_mean = (num_classes1, num_classes2)\n",
    "\n",
    "    if indep_coeff2 is None:\n",
    "        indep_coeff2 = indep_coeff\n",
    "\n",
    "    HSIC_loss_terms = []\n",
    "    HSIC_mean_of_median_pairwise_dist_terms = []\n",
    "    with ns_profiling_label('HSIC/d loss calc'):\n",
    "        # iterate on both heads\n",
    "        for m, num_class in enumerate((num_classes1, num_classes2)):\n",
    "            with ns_profiling_label(f'iter m={m}'):\n",
    "                HSIC_tmp_loss = 0.\n",
    "                HSIC_median_pw_y1 = []\n",
    "                HSIC_median_pw_y2 = []\n",
    "\n",
    "                labels_in_batch_sorted, indices = torch.sort(ys[m])\n",
    "                unique_ixs = 1 + (labels_in_batch_sorted[1:] - labels_in_batch_sorted[:-1]).nonzero()\n",
    "                unique_ixs = [0] + unique_ixs.flatten().cpu().numpy().tolist() + [len(ys[m])]\n",
    "\n",
    "                for j in range(len(unique_ixs)-1):\n",
    "                    current_class_indices = unique_ixs[j], unique_ixs[j + 1]\n",
    "                    count = current_class_indices[1] - current_class_indices[0]\n",
    "                    if count < 2:\n",
    "                        continue\n",
    "                    curr_class_slice = slice(*current_class_indices)\n",
    "                    curr_class_indices = indices[curr_class_slice].sort()[0]\n",
    "\n",
    "                    with ns_profiling_label(f'iter j={j}'):\n",
    "                        HSIC_kernel = dict(G='Gaussian', L='Linear')[Hkernel]\n",
    "                        with ns_profiling_label('HSIC call'):\n",
    "                            hsic_loss_i, median_pairwise_distance_y1, median_pairwise_distance_y2 = \\\n",
    "                                HSIC(repr1[curr_class_indices, :].float(), repr2[curr_class_indices, :].float(),\n",
    "                                     kernelX=HSIC_kernel, kernelY=HSIC_kernel,\n",
    "                                     sigmaX=Hkernel_sigma_obj, sigmaY=Hkernel_sigma_attr,\n",
    "                                     log_median_pairwise_distance=log_median_pairwise_distance)\n",
    "                        HSIC_tmp_loss += hsic_loss_i\n",
    "                        HSIC_median_pw_y1.append(median_pairwise_distance_y1)\n",
    "                        HSIC_median_pw_y2.append(median_pairwise_distance_y2)\n",
    "\n",
    "                HSIC_tmp_loss = HSIC_tmp_loss / normalize_to_mean[m]\n",
    "                HSIC_loss_terms.append(HSIC_tmp_loss)\n",
    "                HSIC_mean_of_median_pairwise_dist_terms.append([np.mean(HSIC_median_pw_y1), np.mean(HSIC_median_pw_y2)])\n",
    "\n",
    "    indep_loss = torch.tensor(0.).to(device)\n",
    "    if indep_coeff > 0:\n",
    "        indep_loss = (indep_coeff * HSIC_loss_terms[0] + indep_coeff2 * HSIC_loss_terms[1]) / 2\n",
    "    return indep_loss, HSIC_loss_terms, HSIC_mean_of_median_pairwise_dist_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProtoProp HSIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/FrankRuis/ProtoProp/blob/0b60866bab619d39072f97bb96eb3ec713b6f51b/model/hsic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_estimation(X, Y):\n",
    "    \"\"\" sigma from median distance\n",
    "    \"\"\"\n",
    "    D = distmat(torch.cat([X, Y]))\n",
    "    D = D.detach().cpu().numpy()\n",
    "    Itri = np.tril_indices(D.shape[0], -1)\n",
    "    Tri = D[Itri]\n",
    "    med = np.median(Tri)\n",
    "    if med <= 0:\n",
    "        med = np.mean(Tri)\n",
    "    if med < 1E-2:\n",
    "        med = 1E-2\n",
    "    return med\n",
    "\n",
    "\n",
    "def distmat(X):\n",
    "    \"\"\" distance matrix\n",
    "    \"\"\"\n",
    "    r = torch.sum(X * X, 1)\n",
    "    r = r.view([-1, 1])\n",
    "    a = torch.mm(X, torch.transpose(X, 0, 1))\n",
    "    D = r.expand_as(a) - 2 * a + torch.transpose(r, 0, 1).expand_as(a)\n",
    "    D = torch.abs(D)\n",
    "    return D\n",
    "\n",
    "\n",
    "def kernelmat(X, sigma):\n",
    "    \"\"\" kernel matrix baker\n",
    "    \"\"\"\n",
    "    m = int(X.size()[0])\n",
    "    H = torch.eye(m) - (1. / m) * torch.ones([m, m])\n",
    "    Dxx = distmat(X)\n",
    "\n",
    "    if sigma:\n",
    "        variance = 2. * sigma * sigma * X.size()[1]\n",
    "        Kx = torch.exp(-Dxx / variance).type(torch.FloatTensor)  # kernel matrices\n",
    "    else:\n",
    "        try:\n",
    "            sx = sigma_estimation(X, X)\n",
    "            Kx = torch.exp(-Dxx / (2. * sx * sx)).type(torch.FloatTensor)\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(\"Unstable sigma {} with maximum/minimum input ({},{})\".format(\n",
    "                sx, torch.max(X), torch.min(X)))\n",
    "\n",
    "    Kxc = torch.mm(Kx, H)\n",
    "\n",
    "    return Kxc\n",
    "\n",
    "\n",
    "def distcorr(X, sigma=1.0):\n",
    "    X = distmat(X)\n",
    "    X = torch.exp(-X / (2. * sigma * sigma))\n",
    "    return torch.mean(X)\n",
    "\n",
    "\n",
    "def compute_kernel(x, y):\n",
    "    x_size = x.size(0)\n",
    "    y_size = y.size(0)\n",
    "    dim = x.size(1)\n",
    "    x = x.unsqueeze(1)  # (x_size, 1, dim)\n",
    "    y = y.unsqueeze(0)  # (1, y_size, dim)\n",
    "    tiled_x = x.expand(x_size, y_size, dim)\n",
    "    tiled_y = y.expand(x_size, y_size, dim)\n",
    "    kernel_input = (tiled_x - tiled_y).pow(2).mean(2) / float(dim)\n",
    "    return torch.exp(-kernel_input)  # (x_size, y_size)\n",
    "\n",
    "\n",
    "def mmd(x, y, sigma=None, use_cuda=True, to_numpy=False):\n",
    "    Dxx = distmat(x)\n",
    "    Dyy = distmat(y)\n",
    "\n",
    "    if sigma:\n",
    "        Kx = torch.exp(-Dxx / (2. * sigma * sigma))  # kernel matrices\n",
    "        Ky = torch.exp(-Dyy / (2. * sigma * sigma))\n",
    "        sxy = sigma\n",
    "    else:\n",
    "        sx = sigma_estimation(x, x)\n",
    "        sy = sigma_estimation(y, y)\n",
    "        sxy = sigma_estimation(x, y)\n",
    "        Kx = torch.exp(-Dxx / (2. * sx * sx))\n",
    "        Ky = torch.exp(-Dyy / (2. * sy * sy))\n",
    "\n",
    "    Dxy = distmat(torch.cat([x, y]))\n",
    "    Dxy = Dxy[:x.size()[0], x.size()[0]:]\n",
    "    Kxy = torch.exp(-Dxy / (1. * sxy * sxy))\n",
    "\n",
    "    mmdval = torch.mean(Kx) + torch.mean(Ky) - 2 * torch.mean(Kxy)\n",
    "\n",
    "    return mmdval\n",
    "\n",
    "\n",
    "def mmd_pxpy_pxy(x, y, sigma=None, use_cuda=True, to_numpy=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    Dxx = distmat(x)\n",
    "    Dyy = distmat(y)\n",
    "    if sigma:\n",
    "        Kx = torch.exp(-Dxx / (2. * sigma * sigma))  # kernel matrices\n",
    "        Ky = torch.exp(-Dyy / (2. * sigma * sigma))\n",
    "    else:\n",
    "        sx = sigma_estimation(x, x)\n",
    "        sy = sigma_estimation(y, y)\n",
    "        Kx = torch.exp(-Dxx / (2. * sx * sx))\n",
    "        Ky = torch.exp(-Dyy / (2. * sy * sy))\n",
    "    A = torch.mean(Kx * Ky)\n",
    "    B = torch.mean(torch.mean(Kx, dim=0) * torch.mean(Ky, dim=0))\n",
    "    C = torch.mean(Kx) * torch.mean(Ky)\n",
    "    mmd_pxpy_pxy_val = A - 2 * B + C\n",
    "    return mmd_pxpy_pxy_val\n",
    "\n",
    "\n",
    "def hsic_regular(x, y, sigma=None, use_cuda=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Kxc = kernelmat(x, sigma)\n",
    "    Kyc = kernelmat(y, sigma)\n",
    "    KtK = torch.mul(Kxc, Kyc.t())\n",
    "    Pxy = torch.mean(KtK)\n",
    "    return Pxy\n",
    "\n",
    "\n",
    "def hsic_normalized(x, y, sigma=None, use_cuda=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Pxy = hsic_regular(x, y, sigma, use_cuda)\n",
    "    Px = torch.sqrt(hsic_regular(x, x, sigma, use_cuda))\n",
    "    Py = torch.sqrt(hsic_regular(y, y, sigma, use_cuda))\n",
    "    if Py == 0:\n",
    "        print(y)\n",
    "        exit()\n",
    "    thehsic = Pxy / (Px * Py)\n",
    "    return thehsic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading global metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available samples: len(meta_df) = 180000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_filename</th>\n",
       "      <th>split</th>\n",
       "      <th>shape</th>\n",
       "      <th>color</th>\n",
       "      <th>material</th>\n",
       "      <th>size</th>\n",
       "      <th>pixel_coords</th>\n",
       "      <th>3d_coords</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ao_clevr_cube_026006.png</td>\n",
       "      <td>cube</td>\n",
       "      <td>cube</td>\n",
       "      <td>gray</td>\n",
       "      <td>metal</td>\n",
       "      <td>small</td>\n",
       "      <td>[52, 39, 11.845768928527832]</td>\n",
       "      <td>[-0.7997756004333496, 1.3228751420974731, 0.34...</td>\n",
       "      <td>ao_clevr_cube_026006.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ao_clevr_cube_091063.png</td>\n",
       "      <td>cube</td>\n",
       "      <td>cube</td>\n",
       "      <td>gray</td>\n",
       "      <td>rubber</td>\n",
       "      <td>small</td>\n",
       "      <td>[62, 55, 9.742987632751465]</td>\n",
       "      <td>[2.3687503337860107, -0.3344546854496002, 0.34...</td>\n",
       "      <td>ao_clevr_cube_091063.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ao_clevr_cylinder_044993.png</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>brown</td>\n",
       "      <td>rubber</td>\n",
       "      <td>small</td>\n",
       "      <td>[63, 47, 10.751688003540039]</td>\n",
       "      <td>[1.2539156675338745, 0.9125927686691284, 0.349...</td>\n",
       "      <td>ao_clevr_cylinder_044993.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ao_clevr_cube_029146.png</td>\n",
       "      <td>cube</td>\n",
       "      <td>cube</td>\n",
       "      <td>brown</td>\n",
       "      <td>metal</td>\n",
       "      <td>large</td>\n",
       "      <td>[87, 43, 10.785911560058594]</td>\n",
       "      <td>[2.821303606033325, 2.7795522212982178, 0.6999...</td>\n",
       "      <td>ao_clevr_cube_029146.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ao_clevr_cube_029340.png</td>\n",
       "      <td>cube</td>\n",
       "      <td>cube</td>\n",
       "      <td>blue</td>\n",
       "      <td>metal</td>\n",
       "      <td>small</td>\n",
       "      <td>[20, 44, 11.521142959594727]</td>\n",
       "      <td>[-2.1865415573120117, -2.1958963871002197, 0.3...</td>\n",
       "      <td>ao_clevr_cube_029340.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179995</th>\n",
       "      <td>ao_clevr_sphere_015978.png</td>\n",
       "      <td>sphere</td>\n",
       "      <td>sphere</td>\n",
       "      <td>green</td>\n",
       "      <td>metal</td>\n",
       "      <td>large</td>\n",
       "      <td>[26, 48, 9.729835510253906]</td>\n",
       "      <td>[-0.46921536326408386, -2.3235161304473877, 0....</td>\n",
       "      <td>ao_clevr_sphere_015978.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179996</th>\n",
       "      <td>ao_clevr_sphere_138770.png</td>\n",
       "      <td>sphere</td>\n",
       "      <td>sphere</td>\n",
       "      <td>purple</td>\n",
       "      <td>metal</td>\n",
       "      <td>large</td>\n",
       "      <td>[34, 51, 9.169315338134766]</td>\n",
       "      <td>[0.42953187227249146, -1.9918320178985596, 0.6...</td>\n",
       "      <td>ao_clevr_sphere_138770.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179997</th>\n",
       "      <td>ao_clevr_sphere_008994.png</td>\n",
       "      <td>sphere</td>\n",
       "      <td>sphere</td>\n",
       "      <td>yellow</td>\n",
       "      <td>metal</td>\n",
       "      <td>small</td>\n",
       "      <td>[57, 60, 8.973981857299805]</td>\n",
       "      <td>[2.533961057662964, -0.9791346192359924, 0.349...</td>\n",
       "      <td>ao_clevr_sphere_008994.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179998</th>\n",
       "      <td>ao_clevr_cylinder_118260.png</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>green</td>\n",
       "      <td>rubber</td>\n",
       "      <td>small</td>\n",
       "      <td>[15, 45, 11.007580757141113]</td>\n",
       "      <td>[-2.2124664783477783, -2.709893226623535, 0.34...</td>\n",
       "      <td>ao_clevr_cylinder_118260.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179999</th>\n",
       "      <td>ao_clevr_cube_155333.png</td>\n",
       "      <td>cube</td>\n",
       "      <td>cube</td>\n",
       "      <td>gray</td>\n",
       "      <td>metal</td>\n",
       "      <td>small</td>\n",
       "      <td>[72, 50, 10.531397819519043]</td>\n",
       "      <td>[2.408088207244873, 1.0896811485290527, 0.3499...</td>\n",
       "      <td>ao_clevr_cube_155333.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image_filename     split     shape   color material  \\\n",
       "0           ao_clevr_cube_026006.png      cube      cube    gray    metal   \n",
       "1           ao_clevr_cube_091063.png      cube      cube    gray   rubber   \n",
       "2       ao_clevr_cylinder_044993.png  cylinder  cylinder   brown   rubber   \n",
       "3           ao_clevr_cube_029146.png      cube      cube   brown    metal   \n",
       "4           ao_clevr_cube_029340.png      cube      cube    blue    metal   \n",
       "...                              ...       ...       ...     ...      ...   \n",
       "179995    ao_clevr_sphere_015978.png    sphere    sphere   green    metal   \n",
       "179996    ao_clevr_sphere_138770.png    sphere    sphere  purple    metal   \n",
       "179997    ao_clevr_sphere_008994.png    sphere    sphere  yellow    metal   \n",
       "179998  ao_clevr_cylinder_118260.png  cylinder  cylinder   green   rubber   \n",
       "179999      ao_clevr_cube_155333.png      cube      cube    gray    metal   \n",
       "\n",
       "         size                  pixel_coords  \\\n",
       "0       small  [52, 39, 11.845768928527832]   \n",
       "1       small   [62, 55, 9.742987632751465]   \n",
       "2       small  [63, 47, 10.751688003540039]   \n",
       "3       large  [87, 43, 10.785911560058594]   \n",
       "4       small  [20, 44, 11.521142959594727]   \n",
       "...       ...                           ...   \n",
       "179995  large   [26, 48, 9.729835510253906]   \n",
       "179996  large   [34, 51, 9.169315338134766]   \n",
       "179997  small   [57, 60, 8.973981857299805]   \n",
       "179998  small  [15, 45, 11.007580757141113]   \n",
       "179999  small  [72, 50, 10.531397819519043]   \n",
       "\n",
       "                                                3d_coords  \\\n",
       "0       [-0.7997756004333496, 1.3228751420974731, 0.34...   \n",
       "1       [2.3687503337860107, -0.3344546854496002, 0.34...   \n",
       "2       [1.2539156675338745, 0.9125927686691284, 0.349...   \n",
       "3       [2.821303606033325, 2.7795522212982178, 0.6999...   \n",
       "4       [-2.1865415573120117, -2.1958963871002197, 0.3...   \n",
       "...                                                   ...   \n",
       "179995  [-0.46921536326408386, -2.3235161304473877, 0....   \n",
       "179996  [0.42953187227249146, -1.9918320178985596, 0.6...   \n",
       "179997  [2.533961057662964, -0.9791346192359924, 0.349...   \n",
       "179998  [-2.2124664783477783, -2.709893226623535, 0.34...   \n",
       "179999  [2.408088207244873, 1.0896811485290527, 0.3499...   \n",
       "\n",
       "                               image  \n",
       "0           ao_clevr_cube_026006.png  \n",
       "1           ao_clevr_cube_091063.png  \n",
       "2       ao_clevr_cylinder_044993.png  \n",
       "3           ao_clevr_cube_029146.png  \n",
       "4           ao_clevr_cube_029340.png  \n",
       "...                              ...  \n",
       "179995    ao_clevr_sphere_015978.png  \n",
       "179996    ao_clevr_sphere_138770.png  \n",
       "179997    ao_clevr_sphere_008994.png  \n",
       "179998  ao_clevr_cylinder_118260.png  \n",
       "179999      ao_clevr_cube_155333.png  \n",
       "\n",
       "[180000 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = pd.read_csv(pjoin(args.data_dir, 'objects_metadata.csv'))\n",
    "print(f\"available samples: len(meta_df) = {len(meta_df)}\")\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building global combinations distribution (global_label_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape': {'cube': 0, 'cylinder': 1, 'sphere': 2},\n",
       " 'color': {'blue': 0,\n",
       "  'brown': 1,\n",
       "  'cyan': 2,\n",
       "  'gray': 3,\n",
       "  'green': 4,\n",
       "  'purple': 5,\n",
       "  'red': 6,\n",
       "  'yellow': 7}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>color</th>\n",
       "      <th>global samples</th>\n",
       "      <th>global freq</th>\n",
       "      <th>shape idx</th>\n",
       "      <th>color idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cube</td>\n",
       "      <td>blue</td>\n",
       "      <td>7520</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cube</td>\n",
       "      <td>brown</td>\n",
       "      <td>7508</td>\n",
       "      <td>0.041711</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cube</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7344</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cube</td>\n",
       "      <td>gray</td>\n",
       "      <td>7552</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cube</td>\n",
       "      <td>green</td>\n",
       "      <td>7346</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>cube</td>\n",
       "      <td>purple</td>\n",
       "      <td>7565</td>\n",
       "      <td>0.042028</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>cube</td>\n",
       "      <td>red</td>\n",
       "      <td>7458</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>cube</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7707</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>blue</td>\n",
       "      <td>7338</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>brown</td>\n",
       "      <td>7455</td>\n",
       "      <td>0.041417</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7521</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>gray</td>\n",
       "      <td>7582</td>\n",
       "      <td>0.042122</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>green</td>\n",
       "      <td>7475</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>purple</td>\n",
       "      <td>7513</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>red</td>\n",
       "      <td>7446</td>\n",
       "      <td>0.041367</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7670</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>sphere</td>\n",
       "      <td>blue</td>\n",
       "      <td>7525</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>sphere</td>\n",
       "      <td>brown</td>\n",
       "      <td>7430</td>\n",
       "      <td>0.041278</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>sphere</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7628</td>\n",
       "      <td>0.042378</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>sphere</td>\n",
       "      <td>gray</td>\n",
       "      <td>7376</td>\n",
       "      <td>0.040978</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>sphere</td>\n",
       "      <td>green</td>\n",
       "      <td>7559</td>\n",
       "      <td>0.041994</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>sphere</td>\n",
       "      <td>purple</td>\n",
       "      <td>7476</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>sphere</td>\n",
       "      <td>red</td>\n",
       "      <td>7425</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>sphere</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7581</td>\n",
       "      <td>0.042117</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comb idx     shape   color  global samples  global freq  shape idx  \\\n",
       "0          0      cube    blue            7520     0.041778          0   \n",
       "1          1      cube   brown            7508     0.041711          0   \n",
       "2          2      cube    cyan            7344     0.040800          0   \n",
       "3          3      cube    gray            7552     0.041956          0   \n",
       "4          4      cube   green            7346     0.040811          0   \n",
       "5          5      cube  purple            7565     0.042028          0   \n",
       "6          6      cube     red            7458     0.041433          0   \n",
       "7          7      cube  yellow            7707     0.042817          0   \n",
       "8          8  cylinder    blue            7338     0.040767          1   \n",
       "9          9  cylinder   brown            7455     0.041417          1   \n",
       "10        10  cylinder    cyan            7521     0.041783          1   \n",
       "11        11  cylinder    gray            7582     0.042122          1   \n",
       "12        12  cylinder   green            7475     0.041528          1   \n",
       "13        13  cylinder  purple            7513     0.041739          1   \n",
       "14        14  cylinder     red            7446     0.041367          1   \n",
       "15        15  cylinder  yellow            7670     0.042611          1   \n",
       "16        16    sphere    blue            7525     0.041806          2   \n",
       "17        17    sphere   brown            7430     0.041278          2   \n",
       "18        18    sphere    cyan            7628     0.042378          2   \n",
       "19        19    sphere    gray            7376     0.040978          2   \n",
       "20        20    sphere   green            7559     0.041994          2   \n",
       "21        21    sphere  purple            7476     0.041533          2   \n",
       "22        22    sphere     red            7425     0.041250          2   \n",
       "23        23    sphere  yellow            7581     0.042117          2   \n",
       "\n",
       "    color idx  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           4  \n",
       "5           5  \n",
       "6           6  \n",
       "7           7  \n",
       "8           0  \n",
       "9           1  \n",
       "10          2  \n",
       "11          3  \n",
       "12          4  \n",
       "13          5  \n",
       "14          6  \n",
       "15          7  \n",
       "16          0  \n",
       "17          1  \n",
       "18          2  \n",
       "19          3  \n",
       "20          4  \n",
       "21          5  \n",
       "22          6  \n",
       "23          7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>color</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>cyan</th>\n",
       "      <th>gray</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>red</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cube</th>\n",
       "      <td>7520</td>\n",
       "      <td>7508</td>\n",
       "      <td>7344</td>\n",
       "      <td>7552</td>\n",
       "      <td>7346</td>\n",
       "      <td>7565</td>\n",
       "      <td>7458</td>\n",
       "      <td>7707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinder</th>\n",
       "      <td>7338</td>\n",
       "      <td>7455</td>\n",
       "      <td>7521</td>\n",
       "      <td>7582</td>\n",
       "      <td>7475</td>\n",
       "      <td>7513</td>\n",
       "      <td>7446</td>\n",
       "      <td>7670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sphere</th>\n",
       "      <td>7525</td>\n",
       "      <td>7430</td>\n",
       "      <td>7628</td>\n",
       "      <td>7376</td>\n",
       "      <td>7559</td>\n",
       "      <td>7476</td>\n",
       "      <td>7425</td>\n",
       "      <td>7581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "color     blue  brown  cyan  gray  green  purple   red  yellow\n",
       "shape                                                         \n",
       "cube      7520   7508  7344  7552   7346    7565  7458    7707\n",
       "cylinder  7338   7455  7521  7582   7475    7513  7446    7670\n",
       "sphere    7525   7430  7628  7376   7559    7476  7425    7581"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'shape': 3, 'color': 8}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_cols = ['shape', 'color']\n",
    "# --------------------------------\n",
    "\n",
    "global_label_combs = meta_df[label_cols].groupby(label_cols).size()\n",
    "global_label_combs.name = 'global samples'\n",
    "global_label_combs = global_label_combs.reset_index()\n",
    "global_label_combs['global freq'] = global_label_combs['global samples']/global_label_combs['global samples'].sum()\n",
    "global_label_combs.index.name = 'comb idx'\n",
    "global_label_combs = global_label_combs.reset_index()\n",
    "\n",
    "label_maps = {label:{} for label in label_cols}\n",
    "for label in label_cols:\n",
    "    label_map = label_maps[label]\n",
    "    for val in global_label_combs[label].unique():\n",
    "        label_map[val] = len(label_map)\n",
    "display(label_maps)\n",
    "\n",
    "for label in label_cols:\n",
    "    global_label_combs[f'{label} idx'] = global_label_combs[label].map(label_maps[label])\n",
    "display(global_label_combs)\n",
    "\n",
    "assert len(label_cols) == 2\n",
    "global_label_combs_pivot = global_label_combs.pivot(index=label_cols[0], columns=label_cols[1], values='global samples')\n",
    "display(global_label_combs_pivot)\n",
    "\n",
    "distinct_label_n_vals = {label: len(global_label_combs[label].unique()) for label in label_cols}\n",
    "display(distinct_label_n_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Oz's splits, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distributions\n",
    "Verifying compositional sampling is valid - each label has at least one shared combination with any other label, unseen pivot shape is the same as global pivot - no empty columns or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    assert len(label_cols) == 2 # for the pivot below used to decide if the sampling is valid\n",
    "\n",
    "    invalid_sampling = True\n",
    "    max_i_sampling = 100\n",
    "    i_sampling = 0\n",
    "\n",
    "    while invalid_sampling:\n",
    "        seen_label_dist = global_label_combs.sample(frac=unseen_ovr_tot,\n",
    "            random_state=max_i_sampling*data_seed+i_sampling)\n",
    "        seen_label_dist_pivot = seen_label_dist.pivot(index=label_cols[0], columns=label_cols[1], values='global samples')\n",
    "        \n",
    "        if seen_label_dist_pivot.shape == global_label_combs_pivot.shape:\n",
    "            invalid_sampling = False\n",
    "            print(f\"succeeded sampling with all unique label values in iteration {i_sampling}\")\n",
    "        i_sampling += 1\n",
    "        if i_sampling >= max_i_sampling:\n",
    "            print(f\"reached max iterations ({max_i_sampling}) without success in sampling with all unique label values \")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    unseen_combs_idx = set(global_label_combs.index) - set(seen_label_dist.index)\n",
    "    unseen_label_dist = global_label_combs.loc[list(unseen_combs_idx)]\n",
    "    assert len(set(seen_label_dist.index) & set(unseen_label_dist.index)) == 0\n",
    "\n",
    "    label_dists_df = global_label_combs.copy()\n",
    "    label_dists_df.loc[seen_label_dist.index, 'train prob'] = 1/len(seen_label_dist)\n",
    "    label_dists_df.loc[seen_label_dist.index, 'state'] = 'seen'\n",
    "    label_dists_df.loc[unseen_label_dist.index, 'train prob'] =\\\n",
    "        args.train_unseen_ovr_seen / len(seen_label_dist)\n",
    "    label_dists_df.loc[unseen_label_dist.index, 'state'] = 'unseen'\n",
    "    label_dists_df['train prob'] = label_dists_df['train prob'] / label_dists_df['train prob'].sum()\n",
    "    label_dists_df['test prob'] =  1/len(label_dists_df)\n",
    "    display(label_dists_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    meta_df_ = meta_df.merge(label_dists_df, on=label_cols)\n",
    "    meta_df_ = meta_df_.sample(frac=1, random_state=data_seed) # shuffling\n",
    "\n",
    "    for i_row in range(len(label_dists_df)):\n",
    "        row = label_dists_df.iloc[i_row]\n",
    "        comb_idx = row['comb idx']\n",
    "        comb_samples = meta_df_.query(f\"`comb idx` == {comb_idx}\")\n",
    "\n",
    "        # selecting train\n",
    "        n_samples_train = round(args.train_size * row['train prob'])\n",
    "        assert n_samples_train <= len(comb_samples)\n",
    "        meta_df_.loc[comb_samples.index[:n_samples_train], 'phase'] = 'train'\n",
    "\n",
    "        # selecting test\n",
    "        n_samples_test = round(args.test_size * row['test prob'])\n",
    "        assert n_samples_test <= len(comb_samples) - n_samples_train\n",
    "        meta_df_.loc[comb_samples.index[\n",
    "            n_samples_train: n_samples_train+n_samples_test], 'phase'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    for phase in ['train', 'test']:\n",
    "        label_dists_df[f'{phase} samples'] = meta_df_.query(f\"phase == '{phase}'\")['comb idx'].value_counts()\n",
    "        label_dists_df[f'{phase} freq'] = label_dists_df[f'{phase} samples']/label_dists_df[f'{phase} samples'].sum()\n",
    "    display(label_dists_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    features_dict = torch.load(pjoin(args.data_dir, 'features.t7'))\n",
    "    features_dict_ = {file:tensor for file, tensor in zip(\n",
    "        features_dict['files'], features_dict['features'])}\n",
    "\n",
    "    idx = 5\n",
    "    assert torch.equal(features_dict_[features_dict['files'][idx]], features_dict['features'][idx])\n",
    "    del features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant == 'OZ':\n",
    "    X, Y_comb = {}, {}\n",
    "    Y_shape, Y_color = {}, {}\n",
    "    Y_shape_onehot, Y_color_onehot = {}, {}\n",
    "    for phase in ['train', 'test']:\n",
    "        data_df = meta_df_.query(f\"phase == '{phase}'\")\n",
    "        print(f\"{phase} contains {len(data_df)} samples\")\n",
    "\n",
    "        Y_comb[phase] = data_df['comb idx'].values.astype('int64')\n",
    "\n",
    "        Y_shape[phase] = data_df['shape idx'].values.astype('int64')\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False, categories=[range(distinct_label_n_vals['shape'])])\n",
    "        Y_shape_onehot[phase] = one_hot_encoder.fit_transform(data_df['shape idx'].values.reshape(-1, 1)).astype('float32') # float32 required in skorch for multi-label learning\n",
    "        \n",
    "        Y_color[phase] = data_df['color idx'].values.astype('int64')\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False, categories=[range(distinct_label_n_vals['color'])])\n",
    "        Y_color_onehot[phase] = one_hot_encoder.fit_transform(data_df['color idx'].values.reshape(-1, 1)).astype('float32') # float32 required in skorch for multi-label learning     \n",
    "\n",
    "        X[phase] = torch.cat([features_dict_[filename].unsqueeze(0)\n",
    "            for filename in data_df['image_filename']], dim=0)\n",
    "    del features_dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Yuval's splits, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_variant in ['VT', 'UV']:\n",
    "    meta_path = Path(f\"{args.data_dir}/metadata_pickles\")\n",
    "    random_state_path = Path(f\"{args.data_dir}/np_random_state_pickles\")\n",
    "    # meta_path = meta_path.expanduser()\n",
    "\n",
    "    dict_data = dict()\n",
    "\n",
    "    for subset in ['train', 'valid', 'test']:\n",
    "        metadata_full_filename = meta_path / f\"metadata_{args.dataset_name}__{args.dataset_variant}_random__comp_seed_{args.num_split}__seen_seed_{args.seen_seed}__{subset}.pkl\"\n",
    "        dict_data[f'{subset}'] = deepcopy(pickle.load(open(metadata_full_filename, 'rb')))\n",
    "\n",
    "    # np_rnd_state_fname = random_state_path / f\"np_random_state_{args.dataset_name}__{args.dataset_variant}_random__comp_seed_{args.num_split}__seen_seed_{args.seen_seed}.pkl\"\n",
    "    # np_seed_state = pickle.load(open(np_rnd_state_fname, 'rb'))\n",
    "    # np.random.set_state(np_seed_state)\n",
    "\n",
    "    datasets = {}\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        datasets[phase] = CompDataFromDict(dict_data[phase if phase!='val' else 'valid'],\n",
    "            data_subset=f'{phase}_data', data_dir=args.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train contains 80958 samples\n",
      "test contains 7720 samples\n",
      "val contains 9238 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>color</th>\n",
       "      <th>global samples</th>\n",
       "      <th>global freq</th>\n",
       "      <th>shape idx</th>\n",
       "      <th>color idx</th>\n",
       "      <th>train samples</th>\n",
       "      <th>train freq</th>\n",
       "      <th>test samples</th>\n",
       "      <th>test freq</th>\n",
       "      <th>val samples</th>\n",
       "      <th>val freq</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cube</td>\n",
       "      <td>blue</td>\n",
       "      <td>7520</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6841.0</td>\n",
       "      <td>0.084501</td>\n",
       "      <td>306</td>\n",
       "      <td>0.039637</td>\n",
       "      <td>373</td>\n",
       "      <td>0.040377</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cube</td>\n",
       "      <td>brown</td>\n",
       "      <td>7508</td>\n",
       "      <td>0.041711</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6790.0</td>\n",
       "      <td>0.083871</td>\n",
       "      <td>342</td>\n",
       "      <td>0.044301</td>\n",
       "      <td>376</td>\n",
       "      <td>0.040701</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cube</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7344</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cube</td>\n",
       "      <td>gray</td>\n",
       "      <td>7552</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6793.0</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>354</td>\n",
       "      <td>0.045855</td>\n",
       "      <td>405</td>\n",
       "      <td>0.043841</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cube</td>\n",
       "      <td>green</td>\n",
       "      <td>7346</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6611.0</td>\n",
       "      <td>0.081660</td>\n",
       "      <td>345</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>390</td>\n",
       "      <td>0.042217</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cube</td>\n",
       "      <td>purple</td>\n",
       "      <td>7565</td>\n",
       "      <td>0.042028</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cube</td>\n",
       "      <td>red</td>\n",
       "      <td>7458</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cube</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7707</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>blue</td>\n",
       "      <td>7338</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6658.0</td>\n",
       "      <td>0.082240</td>\n",
       "      <td>298</td>\n",
       "      <td>0.038601</td>\n",
       "      <td>382</td>\n",
       "      <td>0.041351</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>brown</td>\n",
       "      <td>7455</td>\n",
       "      <td>0.041417</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7521</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>gray</td>\n",
       "      <td>7582</td>\n",
       "      <td>0.042122</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6809.0</td>\n",
       "      <td>0.084105</td>\n",
       "      <td>378</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>395</td>\n",
       "      <td>0.042758</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>green</td>\n",
       "      <td>7475</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>purple</td>\n",
       "      <td>7513</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6733.0</td>\n",
       "      <td>0.083167</td>\n",
       "      <td>376</td>\n",
       "      <td>0.048705</td>\n",
       "      <td>404</td>\n",
       "      <td>0.043732</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>red</td>\n",
       "      <td>7446</td>\n",
       "      <td>0.041367</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cylinder</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7670</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sphere</td>\n",
       "      <td>blue</td>\n",
       "      <td>7525</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6786.0</td>\n",
       "      <td>0.083821</td>\n",
       "      <td>355</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>384</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sphere</td>\n",
       "      <td>brown</td>\n",
       "      <td>7430</td>\n",
       "      <td>0.041278</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6717.0</td>\n",
       "      <td>0.082969</td>\n",
       "      <td>328</td>\n",
       "      <td>0.042487</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sphere</td>\n",
       "      <td>cyan</td>\n",
       "      <td>7628</td>\n",
       "      <td>0.042378</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sphere</td>\n",
       "      <td>gray</td>\n",
       "      <td>7376</td>\n",
       "      <td>0.040978</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6656.0</td>\n",
       "      <td>0.082215</td>\n",
       "      <td>334</td>\n",
       "      <td>0.043264</td>\n",
       "      <td>386</td>\n",
       "      <td>0.041784</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sphere</td>\n",
       "      <td>green</td>\n",
       "      <td>7559</td>\n",
       "      <td>0.041994</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sphere</td>\n",
       "      <td>purple</td>\n",
       "      <td>7476</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>385</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sphere</td>\n",
       "      <td>red</td>\n",
       "      <td>7425</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6681.0</td>\n",
       "      <td>0.082524</td>\n",
       "      <td>358</td>\n",
       "      <td>0.046373</td>\n",
       "      <td>386</td>\n",
       "      <td>0.041784</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sphere</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7581</td>\n",
       "      <td>0.042117</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6883.0</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>346</td>\n",
       "      <td>0.044819</td>\n",
       "      <td>352</td>\n",
       "      <td>0.038103</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             shape   color  global samples  global freq  shape idx  color idx  \\\n",
       "comb idx                                                                        \n",
       "0             cube    blue            7520     0.041778          0          0   \n",
       "1             cube   brown            7508     0.041711          0          1   \n",
       "2             cube    cyan            7344     0.040800          0          2   \n",
       "3             cube    gray            7552     0.041956          0          3   \n",
       "4             cube   green            7346     0.040811          0          4   \n",
       "5             cube  purple            7565     0.042028          0          5   \n",
       "6             cube     red            7458     0.041433          0          6   \n",
       "7             cube  yellow            7707     0.042817          0          7   \n",
       "8         cylinder    blue            7338     0.040767          1          0   \n",
       "9         cylinder   brown            7455     0.041417          1          1   \n",
       "10        cylinder    cyan            7521     0.041783          1          2   \n",
       "11        cylinder    gray            7582     0.042122          1          3   \n",
       "12        cylinder   green            7475     0.041528          1          4   \n",
       "13        cylinder  purple            7513     0.041739          1          5   \n",
       "14        cylinder     red            7446     0.041367          1          6   \n",
       "15        cylinder  yellow            7670     0.042611          1          7   \n",
       "16          sphere    blue            7525     0.041806          2          0   \n",
       "17          sphere   brown            7430     0.041278          2          1   \n",
       "18          sphere    cyan            7628     0.042378          2          2   \n",
       "19          sphere    gray            7376     0.040978          2          3   \n",
       "20          sphere   green            7559     0.041994          2          4   \n",
       "21          sphere  purple            7476     0.041533          2          5   \n",
       "22          sphere     red            7425     0.041250          2          6   \n",
       "23          sphere  yellow            7581     0.042117          2          7   \n",
       "\n",
       "          train samples  train freq  test samples  test freq  val samples  \\\n",
       "comb idx                                                                    \n",
       "0                6841.0    0.084501           306   0.039637          373   \n",
       "1                6790.0    0.083871           342   0.044301          376   \n",
       "2                   NaN         NaN           300   0.038860          385   \n",
       "3                6793.0    0.083908           354   0.045855          405   \n",
       "4                6611.0    0.081660           345   0.044689          390   \n",
       "5                   NaN         NaN           300   0.038860          385   \n",
       "6                   NaN         NaN           300   0.038860          385   \n",
       "7                   NaN         NaN           300   0.038860          385   \n",
       "8                6658.0    0.082240           298   0.038601          382   \n",
       "9                   NaN         NaN           300   0.038860          385   \n",
       "10                  NaN         NaN           300   0.038860          385   \n",
       "11               6809.0    0.084105           378   0.048964          395   \n",
       "12                  NaN         NaN           300   0.038860          385   \n",
       "13               6733.0    0.083167           376   0.048705          404   \n",
       "14                  NaN         NaN           300   0.038860          385   \n",
       "15                  NaN         NaN           300   0.038860          385   \n",
       "16               6786.0    0.083821           355   0.045984          384   \n",
       "17               6717.0    0.082969           328   0.042487          385   \n",
       "18                  NaN         NaN           300   0.038860          385   \n",
       "19               6656.0    0.082215           334   0.043264          386   \n",
       "20                  NaN         NaN           300   0.038860          385   \n",
       "21                  NaN         NaN           300   0.038860          385   \n",
       "22               6681.0    0.082524           358   0.046373          386   \n",
       "23               6883.0    0.085019           346   0.044819          352   \n",
       "\n",
       "          val freq   state  \n",
       "comb idx                    \n",
       "0         0.040377    seen  \n",
       "1         0.040701    seen  \n",
       "2         0.041676  unseen  \n",
       "3         0.043841    seen  \n",
       "4         0.042217    seen  \n",
       "5         0.041676  unseen  \n",
       "6         0.041676  unseen  \n",
       "7         0.041676  unseen  \n",
       "8         0.041351    seen  \n",
       "9         0.041676  unseen  \n",
       "10        0.041676  unseen  \n",
       "11        0.042758    seen  \n",
       "12        0.041676  unseen  \n",
       "13        0.043732    seen  \n",
       "14        0.041676  unseen  \n",
       "15        0.041676  unseen  \n",
       "16        0.041567    seen  \n",
       "17        0.041676    seen  \n",
       "18        0.041676  unseen  \n",
       "19        0.041784    seen  \n",
       "20        0.041676  unseen  \n",
       "21        0.041676  unseen  \n",
       "22        0.041784    seen  \n",
       "23        0.038103    seen  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.dataset_variant in ['VT', 'UV']:\n",
    "    X, Y_comb = {}, {}\n",
    "    Y_shape, Y_color = {}, {}\n",
    "    Y_shape_onehot, Y_color_onehot = {}, {}\n",
    "    label_dists_df = global_label_combs.copy()\n",
    "    meta_df_ = meta_df.copy()\n",
    "\n",
    "    for phase in ['train', 'test', 'val']: # val is last for args.val_unseen_mode == 'complete'\n",
    "        dataset = datasets[phase]\n",
    "        data_df = pd.DataFrame(dataset.data, columns=['filename', 'color', 'shape'])\n",
    "        data_df = data_df.merge(label_dists_df, on=label_cols)\n",
    "\n",
    "        data_df[phase] = 1\n",
    "        meta_df_ = meta_df_.merge(data_df.set_index('filename')[phase], left_on='image_filename', right_on='filename', how='outer')\n",
    "        \n",
    "        if phase == 'val':\n",
    "            if args.val_unseen_mode == 'drop':\n",
    "                seen_combs = label_dists_df.index[~label_dists_df['train freq'].isna()]\n",
    "                data_df = data_df[data_df['comb idx'].isin(seen_combs)]\n",
    "                data_df[phase] = 1\n",
    "                meta_df_[phase] = data_df[phase]\n",
    "            elif args.val_unseen_mode == 'complete':\n",
    "                phases_sum = meta_df_[['train', 'val', 'test']].sum(1)\n",
    "                unallocated_samples = meta_df_[phases_sum==0]\n",
    "\n",
    "                unseen_combs = label_dists_df.index[label_dists_df['train freq'].isna()]\n",
    "                seen_val_comb_counts = data_df[~data_df['comb idx'].isin(unseen_combs)]['comb idx'].value_counts()\n",
    "                mean_seen_val_samples = round(seen_val_comb_counts.mean())\n",
    "\n",
    "                unallocated_samples_ = unallocated_samples.merge(label_dists_df.reset_index()[['shape', 'color', 'shape idx', 'color idx', 'comb idx']], on=label_cols)\n",
    "                for comb in unseen_combs:\n",
    "                    unallocated_samples_in = unallocated_samples_.query(f\"`comb idx` == {comb}\").rename(columns={'image':'filename'})\n",
    "                    n_samples_to_add = mean_seen_val_samples - len(data_df.query(f\"`comb idx` == {comb}\"))\n",
    "                    if len(unallocated_samples_in) > n_samples_to_add:\n",
    "                        unallocated_samples_in = unallocated_samples_in[:n_samples_to_add]\n",
    "                    data_df = pd.concat([data_df, unallocated_samples_in[['shape', 'color', 'shape idx', 'color idx', 'comb idx', 'filename']]], axis=0)\n",
    "                \n",
    "                data_df[phase] = 1\n",
    "                del meta_df_[phase]\n",
    "                meta_df_ = meta_df_.merge(data_df.set_index('filename')[phase], left_on='image_filename', right_on='filename', how='outer')\n",
    "                \n",
    "                data_df = data_df.sample(frac=1, random_state=data_seed) # shuffling\n",
    "        \n",
    "        print(f\"{phase} contains {len(data_df)} samples\")\n",
    "\n",
    "        Y_comb[phase] = data_df['comb idx'].values.astype('int64')\n",
    "\n",
    "        Y_shape[phase] = data_df['shape idx'].values.astype('int64')\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False, categories=[range(distinct_label_n_vals['shape'])])\n",
    "        Y_shape_onehot[phase] = one_hot_encoder.fit_transform(data_df['shape idx'].values.reshape(-1, 1)).astype('float32') # float32 required in skorch for multi-label learning\n",
    "        \n",
    "        Y_color[phase] = data_df['color idx'].values.astype('int64')\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False, categories=[range(distinct_label_n_vals['color'])])\n",
    "        Y_color_onehot[phase] = one_hot_encoder.fit_transform(data_df['color idx'].values.reshape(-1, 1)).astype('float32') # float32 required in skorch for multi-label learning\n",
    "\n",
    "        features = dataset.activations\n",
    "        X[phase] = torch.cat([features[filename].unsqueeze(0)\n",
    "            for filename in data_df['filename']], dim=0)\n",
    "\n",
    "        dist_df = data_df.groupby(label_cols).size()\n",
    "        dist_df.name = f'{phase} samples'\n",
    "        dist_df = dist_df.reset_index()\n",
    "        dist_df[f'{phase} freq'] = dist_df[f'{phase} samples']/dist_df[f'{phase} samples'].sum()\n",
    "        label_dists_df = label_dists_df.merge(dist_df, on=label_cols, how='outer')\n",
    "\n",
    "\n",
    "    label_dists_df = label_dists_df.set_index('comb idx')\n",
    "    label_dists_df.loc[label_dists_df['train freq'].isna(), 'state'] = 'unseen'\n",
    "    label_dists_df.loc[~label_dists_df['train freq'].isna(), 'state'] = 'seen'\n",
    "    \n",
    "    display(label_dists_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images that don't belong to any phase (train/val/test):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape     color \n",
       "cube      cyan      6659\n",
       "          purple    6880\n",
       "          red       6773\n",
       "          yellow    7022\n",
       "cylinder  brown     6770\n",
       "          cyan      6836\n",
       "          green     6790\n",
       "          red       6761\n",
       "          yellow    6985\n",
       "sphere    cyan      6943\n",
       "          green     6874\n",
       "          purple    6791\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.dataset_variant in ['VT', 'UV']:\n",
    "    phases_sum = meta_df_[['train', 'val', 'test']].sum(1)\n",
    "    assert (phases_sum > 1).sum() == 0 # no phase overlap (train/val/test)\n",
    "\n",
    "    print(\"images that don't belong to any phase (train/val/test):\")\n",
    "    unallocated_samples = meta_df_[phases_sum==0]\n",
    "    display(unallocated_samples.groupby(label_cols).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_combs = label_dists_df.query(\"state == 'unseen'\").index\n",
    "unseen_combs_idx_to_comb_idx = {i: comb_idx for i, comb_idx in enumerate(unseen_combs)}\n",
    "seen_combs = label_dists_df.query(\"state == 'seen'\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_unseen / n_total = 12 / 24 = 0.5\n"
     ]
    }
   ],
   "source": [
    "n_unseen = len(label_dists_df.query(f\"state == 'seen'\"))\n",
    "n_seen = len(label_dists_df.query(f\"state == 'unseen'\"))\n",
    "assert len(label_dists_df) == n_unseen + n_seen\n",
    "print(f\"n_unseen / n_total = {n_unseen} / {len(label_dists_df)} = {n_unseen/len(label_dists_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train freq:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>color</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>cyan</th>\n",
       "      <th>gray</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>red</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cube</th>\n",
       "      <td>6,841.0</td>\n",
       "      <td>6,790.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,793.0</td>\n",
       "      <td>6,611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinder</th>\n",
       "      <td>6,658.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,809.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,733.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sphere</th>\n",
       "      <td>6,786.0</td>\n",
       "      <td>6,717.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,656.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,681.0</td>\n",
       "      <td>6,883.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "color       blue   brown  cyan    gray   green  purple     red  yellow\n",
       "shape                                                                 \n",
       "cube     6,841.0 6,790.0   NaN 6,793.0 6,611.0     NaN     NaN     NaN\n",
       "cylinder 6,658.0     NaN   NaN 6,809.0     NaN 6,733.0     NaN     NaN\n",
       "sphere   6,786.0 6,717.0   NaN 6,656.0     NaN     NaN 6,681.0 6,883.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test freq:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>color</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>cyan</th>\n",
       "      <th>gray</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>red</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cube</th>\n",
       "      <td>306</td>\n",
       "      <td>342</td>\n",
       "      <td>300</td>\n",
       "      <td>354</td>\n",
       "      <td>345</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinder</th>\n",
       "      <td>298</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>378</td>\n",
       "      <td>300</td>\n",
       "      <td>376</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sphere</th>\n",
       "      <td>355</td>\n",
       "      <td>328</td>\n",
       "      <td>300</td>\n",
       "      <td>334</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>358</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "color     blue  brown  cyan  gray  green  purple  red  yellow\n",
       "shape                                                        \n",
       "cube       306    342   300   354    345     300  300     300\n",
       "cylinder   298    300   300   378    300     376  300     300\n",
       "sphere     355    328   300   334    300     300  358     346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val freq:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>color</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>cyan</th>\n",
       "      <th>gray</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>red</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cube</th>\n",
       "      <td>373</td>\n",
       "      <td>376</td>\n",
       "      <td>385</td>\n",
       "      <td>405</td>\n",
       "      <td>390</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinder</th>\n",
       "      <td>382</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "      <td>395</td>\n",
       "      <td>385</td>\n",
       "      <td>404</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sphere</th>\n",
       "      <td>384</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "      <td>386</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "      <td>386</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "color     blue  brown  cyan  gray  green  purple  red  yellow\n",
       "shape                                                        \n",
       "cube       373    376   385   405    390     385  385     385\n",
       "cylinder   382    385   385   395    385     404  385     385\n",
       "sphere     384    385   385   386    385     385  386     352"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show_total = True\n",
    "show_total = False\n",
    "vals = 'samples'\n",
    "# vals = 'freq'\n",
    "# ----------------------\n",
    "if vals == 'freq':\n",
    "    format = \"{:,.1%}\".format\n",
    "elif vals == 'samples':\n",
    "    format = \"{:,}\".format\n",
    "\n",
    "for phase in X:\n",
    "# for phase in ['train', 'test']:\n",
    "    print(f\"{phase} freq:\")\n",
    "    # with pd.option_context('display.float_format',\"{:,.4f}\".format):\n",
    "    #     display(label_dists_df.pivot(index=label_cols[0], columns=label_cols[1], values=f'{phase} freq'))\n",
    "    with pd.option_context('display.float_format', format):\n",
    "        df = label_dists_df.pivot(index=label_cols[0], columns=label_cols[1], values=f'{phase} samples')\n",
    "        df_ = df.copy()\n",
    "        if show_total:\n",
    "            df_['total'] = df.sum(1)\n",
    "            df_.loc['total'] = df.sum(0)\n",
    "        if vals == 'freq':\n",
    "            df_ /= label_dists_df[f'{phase} samples'].sum()\n",
    "        display(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dists_df.query(\"`val samples` > 5\")['val samples'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input(\"manual VisProd args? y/[n] \") == 'y':\n",
    "#     args.VP.lambda_MI = 0\n",
    "#     args.VP.lambda_color = 1\n",
    "#     args.VP.lambda_shape = 1\n",
    "#     args.VP.epochs = 10\n",
    "#     args.VP.lr = 5e-2\n",
    "#     args.VP.optimizer = 'Adam'\n",
    "#     # args.VP.lr = 1e-4\n",
    "#     # args.VP.optimizer = 'SGD'\n",
    "    \n",
    "\n",
    "#     print(\"set manual VisProd args:\", args.VP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <__main__.Dataset at 0x1ced9fc84c0>,\n",
       " 'test': <__main__.Dataset at 0x1cec913de20>,\n",
       " 'val': <__main__.Dataset at 0x1cee541c4f0>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y_color, y_shape, y_comb, y_seen):\n",
    "        assert len(X) == len(y_color) == len(y_shape) == len(y_comb) == len(y_seen)\n",
    "        \n",
    "        self.X = X\n",
    "        self.y_color = y_color\n",
    "        self.y_shape = y_shape\n",
    "        self.y_comb = y_comb\n",
    "        self.y_seen = y_seen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_color)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y_color[i], self.y_shape[i], self.y_comb[i], self.y_seen[i]\n",
    "\n",
    "datasets = {phase: Dataset(X=X[phase], y_color=Y_color[phase], y_shape=Y_shape[phase],\n",
    "                            y_comb=Y_comb[phase], y_seen=np.isin(Y_comb[phase], seen_combs)) for phase in X}\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1cee5439f10>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x1cee5439190>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x1cee5439610>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {phase: torch.utils.data.DataLoader(datasets[phase],\n",
    "                                                batch_size=args.batch_size, shuffle=True,\n",
    "                                                num_workers=args.workers,\n",
    "                                                pin_memory=args.pin_memory) for phase in datasets}\n",
    "dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nets, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed: torch.backends.cudnn.benchmark = False\n",
      "executed: torch.use_deterministic_algorithms(True)\n",
      "torch and cuda will be deterministic (after seeding)\n",
      "torch is using cuda:0 (NVIDIA GeForce GTX 960M)\n",
      "completed: seeding with seed=2 in steps of 1000 for random,np,torch,cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch_setup(deterministic=True)\n",
    "seed_all(args.init_seed)\n",
    "\n",
    "in_dim = X['train'].shape[1]\n",
    "assert in_dim == 512\n",
    "\n",
    "models = {space: nn.Sequential(\n",
    "    nn.Linear(in_dim, 128),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(64, distinct_label_n_vals[space]),\n",
    "    nn.LogSoftmax(dim=1)).to(device)\n",
    "for space in ['color', 'shape']}\n",
    "if args.VP.decoupling_loss == 'MI est':\n",
    "    models['joint'] = nn.Sequential(\n",
    "    nn.Linear(in_dim, 128),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(64, distinct_label_n_vals['color'] * distinct_label_n_vals['shape']),\n",
    "    nn.LogSoftmax(dim=1)).to(device)\n",
    "\n",
    "clss_loss = nn.NLLLoss(reduction='sum').to(device)\n",
    "\n",
    "if args.VP.optimizer == 'SGD':\n",
    "    optimizers = {space: optim.SGD(model.parameters(), lr=args.VP.lr) for space, model in models.items()}\n",
    "elif args.VP.optimizer == 'Adam':\n",
    "    optimizers = {space: optim.Adam(model.parameters(), lr=args.VP.lr) for space, model in models.items()}\n",
    "else:\n",
    "    raise NotImplementedError(f\"args.VP.optimizer = '{args.VP.optimizer}' not implemented\")\n",
    "\n",
    "if args.VP.plateau_mode != 'off':\n",
    "    schedulers = {space: optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=args.VP.plateau_mode, factor=args.VP.plateau_factor, verbose=True,\n",
    "                                patience=args.VP.plateau_patience, threshold=args.VP.plateau_threshold, threshold_mode=args.VP.plateau_threshold_mode)\n",
    "                    for space, optimizer in optimizers.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_color, y_shape, y_comb, y_seen = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_pos: {'color': 5, 'shape': 438, 'joint': 5}\n",
      "seen_acc: 0.0048828125\n",
      "unseen_acc: nan\n",
      "losses:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'color': tensor(2195.9766, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'shape': tensor(1104.7301, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'MI g.t.': tensor(3300.7065, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " 'HSIC': tensor(5.4186e-07, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = x.to(device)\n",
    "y_s = {}\n",
    "y_s['color'] = y_color.to(device)\n",
    "y_s['shape'] = y_shape.to(device)\n",
    "y_s['joint'] = y_comb.to(device)\n",
    "\n",
    "log_probs = {space: model(x) for space, model in models.items()}\n",
    "probs = {space: torch.exp(space_log_probs) for space, space_log_probs in log_probs.items()}\n",
    "losses = {space: clss_loss(log_probs[space], y_s[space]) for space in models}\n",
    "\n",
    "# probs_on_true = {space: torch.exp(torch.take_along_dim(input=log_probs[space], indices=y_s[space].unsqueeze(-1), dim=1)) for space in models}\n",
    "probs_on_true = {space: torch.take_along_dim(input=probs[space], indices=y_s[space].unsqueeze(-1), dim=1) for space in models}\n",
    "losses['MI g.t.'] = torch.log(1/(probs_on_true['color']*probs_on_true['shape'])).sum()\n",
    "\n",
    "preds = {space: log_probs.argmax(1) for space,log_probs in log_probs.items()}\n",
    "true_pos = {space: (preds[space]==y_s[space]).sum().item() for space in models}\n",
    "join_true_pos = (preds['color']==y_s['color']) & (preds['shape']==y_s['shape'])\n",
    "true_pos['joint'] = (join_true_pos).sum().item()\n",
    "print(\"true_pos:\", true_pos)\n",
    "\n",
    "seen_pos = (join_true_pos[y_seen]).sum()\n",
    "seen_acc = (seen_pos / y_seen.sum()).item()\n",
    "print(\"seen_acc:\", seen_acc)\n",
    "\n",
    "unseen_pos = (join_true_pos[~y_seen]).sum()\n",
    "unseen_acc = (unseen_pos / (~y_seen).sum()).item()\n",
    "print(\"unseen_acc:\", unseen_acc)\n",
    "\n",
    "if args.VP.decoupling_loss == 'MI est':\n",
    "    batch_n_samples = x.shape[0]\n",
    "    # VisProd_probs = []\n",
    "    # for i_sample in range(batch_n_samples):\n",
    "    #     sample_VisProd_probs = torch.outer(probs['shape'][i_sample], probs['color'][i_sample]).reshape(1,-1)\n",
    "    #     sample_VisProd_probs /= sample_VisProd_probs.sum()\n",
    "    #     VisProd_probs.append(sample_VisProd_probs)\n",
    "    # VisProd_probs = torch.cat(VisProd_probs, dim=0)\n",
    "    VisProd_probs = torch.bmm(probs['shape'].unsqueeze(2), probs['color'].unsqueeze(1)).reshape(batch_n_samples, -1) # https://discuss.pytorch.org/t/batch-outer-product/4025\n",
    "    # VisProd_probs = (VisProd_probs.T / VisProd_probs.sum(1)).T\n",
    "    VisProd_probs = (VisProd_probs.T / VisProd_probs[:, seen_combs].sum(1)).T\n",
    "    print(\"VisProd_probs.sum(1):\", VisProd_probs.sum(1))\n",
    "    print(\"VisProd_probs[:, seen_combs].sum(1):\", VisProd_probs[:, seen_combs].sum(1))\n",
    "    \n",
    "    MI_per_comb = probs['joint'] * torch.log(probs['joint']/VisProd_probs)\n",
    "    losses['MI est'] = MI_per_comb[:, seen_combs].sum()\n",
    "\n",
    "if 'HSIC' in args.VP.decoupling_loss:\n",
    "    if args.VP.decoupling_loss in ['HSIC,L', 'HSIC,G']:\n",
    "        if args.VP.decoupling_loss == 'HSIC,L':\n",
    "            Hkernel = 'L'\n",
    "            Hkernel_sigma_obj = None\n",
    "            Hkernel_sigma_attr = None\n",
    "        else:\n",
    "            Hkernel = 'G'\n",
    "            Hkernel_sigma_obj = 1\n",
    "            Hkernel_sigma_attr = 1\n",
    "            \n",
    "        losses['HSIC'], HSIC_rep_loss_terms, HSIC_mean_of_median_pairwise_dist_terms =\\\n",
    "            conditional_indep_losses(probs['shape'], probs['color'], ys=[y_s['shape'], y_s['color']], indep_coeff=1, indep_coeff2=None,\n",
    "                                    num_classes1=distinct_label_n_vals['shape'], num_classes2=distinct_label_n_vals['color'],\n",
    "                                    Hkernel=Hkernel, Hkernel_sigma_obj=Hkernel_sigma_obj, Hkernel_sigma_attr=Hkernel_sigma_attr,\n",
    "                                    log_median_pairwise_distance=False, device=device)\n",
    "    elif args.VP.decoupling_loss == 'HSIC,N':\n",
    "        losses['HSIC'] = hsic_normalized(x=probs['shape'], y=probs['color'], sigma=None, use_cuda=True)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "\n",
    "print(\"losses:\")\n",
    "display(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(training: bool, models:dict, dataloader, clss_loss, args: VisProd, optimizers:dict={}, seen_combs=None):\n",
    "    if training:\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "    else:\n",
    "        for model in models.values():\n",
    "            model.eval()\n",
    "\n",
    "    epoch_true_pos = defaultdict(float)\n",
    "    epoch_losses = defaultdict(float)\n",
    "    n_samples, n_seen = 0, 0 # n_unseen = n_samples- n_seen\n",
    "    epoch_log_probs = defaultdict(list)\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch in dataloader:\n",
    "            x, y_color, y_shape, y_comb, y_seen = batch\n",
    "            batch_n_samples = x.shape[0]\n",
    "            n_samples += batch_n_samples\n",
    "            x = x.to(device)\n",
    "            y_s = {}\n",
    "            y_s['color'] = y_color.to(device)\n",
    "            y_s['shape'] = y_shape.to(device)\n",
    "            if args.decoupling_loss == 'MI est':\n",
    "                y_s['joint'] = y_comb.to(device)\n",
    "\n",
    "            if training:\n",
    "                for optimizer in optimizers.values():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            log_probs = {space: model(x) for space, model in models.items()}\n",
    "            probs = {space: torch.exp(space_log_probs) for space, space_log_probs in log_probs.items()}\n",
    "            for space in log_probs:\n",
    "                epoch_log_probs[space].append(log_probs[space])\n",
    "\n",
    "            # eval\n",
    "            preds = {space: log_probs.argmax(1) for space,log_probs in log_probs.items()}\n",
    "            for space in models:\n",
    "                epoch_true_pos[space] += (preds[space]==y_s[space]).sum().item()\n",
    "            join_true_pos = (preds['color']==y_s['color']) & (preds['shape']==y_s['shape'])\n",
    "            epoch_true_pos['joint'] += (join_true_pos).sum().item()\n",
    "            epoch_true_pos['seen'] += (join_true_pos[y_seen]).sum().item()\n",
    "            n_seen += y_seen.sum().item()\n",
    "            epoch_true_pos['unseen'] +=(join_true_pos[~y_seen]).sum().item()\n",
    "\n",
    "            # losses\n",
    "            losses = {space: clss_loss(log_probs[space], y_s[space]) for space in models}\n",
    "            # probs_on_true = {space: torch.exp(torch.take_along_dim(input=log_probs[space], indices=y_s[space].unsqueeze(-1), dim=1)) for space in y_s}\n",
    "            probs_on_true = {space: # moving to cpu and back since it doens't have a deterministic implementation for backprop\n",
    "                torch.take_along_dim(input=probs[space].cpu(), indices=y_s[space].unsqueeze(-1).cpu(), dim=1).to(device) for space in models}\n",
    "\n",
    "            losses['MI g.t.'] = torch.log(1/(probs_on_true['color']*probs_on_true['shape'])).sum()\n",
    "            if args.decoupling_loss == 'MI est':\n",
    "                # VisProd_probs = []\n",
    "                # for i_sample in range(batch_n_samples):\n",
    "                #     sample_VisProd_probs = torch.outer(probs['shape'][i_sample], probs['color'][i_sample]).reshape(1,-1)\n",
    "                #     sample_VisProd_probs /= sample_VisProd_probs.sum()\n",
    "                #     VisProd_probs.append(sample_VisProd_probs)\n",
    "                # VisProd_probs = torch.cat(VisProd_probs, dim=0)\n",
    "                VisProd_probs = torch.bmm(probs['shape'].unsqueeze(2), probs['color'].unsqueeze(1)).reshape(batch_n_samples, -1) # https://discuss.pytorch.org/t/batch-outer-product/4025\n",
    "                # VisProd_probs = (VisProd_probs.T / VisProd_probs.sum(1)).T\n",
    "                VisProd_probs = (VisProd_probs.T / VisProd_probs[:, seen_combs].sum(1)).T\n",
    "\n",
    "                MI_per_comb = probs['joint'] * torch.log(probs['joint']/VisProd_probs)\n",
    "                losses['MI est'] = MI_per_comb[:, seen_combs].sum()\n",
    "            elif 'HSIC' in args.decoupling_loss:\n",
    "                if args.decoupling_loss in ['HSIC,L', 'HSIC,G']:\n",
    "                    if args.decoupling_loss == 'HSIC,L':\n",
    "                        Hkernel = 'L'\n",
    "                        Hkernel_sigma_obj = None\n",
    "                        Hkernel_sigma_attr = None\n",
    "                    else:\n",
    "                        Hkernel = 'G'\n",
    "                        Hkernel_sigma_obj = 1\n",
    "                        Hkernel_sigma_attr = 1\n",
    "                        \n",
    "                    losses['HSIC'], HSIC_rep_loss_terms, HSIC_mean_of_median_pairwise_dist_terms =\\\n",
    "                        conditional_indep_losses(probs['shape'], probs['color'], ys=[y_s['shape'], y_s['color']], indep_coeff=1, indep_coeff2=None,\n",
    "                                                num_classes1=distinct_label_n_vals['shape'], num_classes2=distinct_label_n_vals['color'],\n",
    "                                                Hkernel=Hkernel, Hkernel_sigma_obj=Hkernel_sigma_obj, Hkernel_sigma_attr=Hkernel_sigma_attr,\n",
    "                                                log_median_pairwise_distance=False, device=device)\n",
    "                elif args.decoupling_loss == 'HSIC,N':\n",
    "                    losses['HSIC'] = hsic_normalized(x=probs['shape'], y=probs['color'], sigma=None, use_cuda=True)\n",
    "                else:\n",
    "                    raise NotImplemented\n",
    "\n",
    "            loss = 0\n",
    "            if args.lambda_color > 0:\n",
    "                loss += args.lambda_color * losses['color']\n",
    "            if args.lambda_shape > 0:\n",
    "                loss += args.lambda_shape * losses['shape']\n",
    "            if args.decoupling_loss == 'MI est' and args.lambda_joint > 0:\n",
    "                loss += args.lambda_joint * losses['joint']\n",
    "            if args.lambda_decoupling > 0:\n",
    "                if 'HSIC' in args.decoupling_loss:\n",
    "                    loss += args.lambda_decoupling * losses['HSIC']\n",
    "                else:\n",
    "                    loss += args.lambda_decoupling * losses[args.decoupling_loss]\n",
    "            losses['total'] = loss\n",
    "            for space in losses:\n",
    "                epoch_losses[space] += losses[space].item()\n",
    "\n",
    "            # backward\n",
    "            if training:\n",
    "                if torch.isfinite(loss):\n",
    "                    loss.backward()\n",
    "                    for optimizer in optimizers.values():\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    raise RuntimeError(f\"reached non-finite loss\")\n",
    "\n",
    "        for space in epoch_log_probs:\n",
    "            epoch_log_probs[space] = torch.cat(epoch_log_probs[space], dim=0)\n",
    "        results = {}\n",
    "        results.update({f'{space} acc': epoch_true_pos[space]/n_samples for space in ['color', 'shape', 'joint']})\n",
    "        results['seen acc'] = epoch_true_pos['seen']/n_seen\n",
    "        n_unseen = n_samples-n_seen\n",
    "        if n_unseen > 0:\n",
    "            results['unseen acc'] = epoch_true_pos['unseen']/n_unseen\n",
    "            results['harmonic acc'] = 2*results['seen acc']*results['unseen acc']/(results['seen acc']+results['unseen acc'])\n",
    "        else:\n",
    "            results['unseen acc'] = np.nan\n",
    "            results['harmonic acc'] = np.nan\n",
    "        results.update({f'{space} loss / sample': loss/n_samples for space, loss in epoch_losses.items()})\n",
    "    return results, epoch_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1/51 [00:05<04:58,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 2/51 [00:18<08:14, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 43.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3/51 [00:32<09:12, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 45.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4/51 [00:40<08:05, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 59.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 5/51 [00:51<07:57, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 58.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6/51 [01:02<08:07, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val harmonic acc: 57.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6/51 [01:09<08:39, 11.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PythonProjects\\CasualComp\\causal_comp\\MLLS_VisProd.ipynb Cell 59'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mVP\u001b[39m.\u001b[39mepochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=3'>4</a>\u001b[0m     \u001b[39m# training\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=5'>6</a>\u001b[0m         results[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][epoch], _ \u001b[39m=\u001b[39m run_epoch(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, models\u001b[39m=\u001b[39;49mmodels, dataloader\u001b[39m=\u001b[39;49mdataloaders[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=6'>7</a>\u001b[0m             clss_loss\u001b[39m=\u001b[39;49mclss_loss, args\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mVP, optimizers\u001b[39m=\u001b[39;49moptimizers, seen_combs\u001b[39m=\u001b[39;49mseen_combs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=8'>9</a>\u001b[0m     \u001b[39m# eval\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000054?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[1;32md:\\PythonProjects\\CasualComp\\causal_comp\\MLLS_VisProd.ipynb Cell 58'\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(training, models, dataloader, clss_loss, args, optimizers, seen_combs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000053?line=102'>103</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000053?line=103'>104</a>\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misfinite(loss):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000053?line=104'>105</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000053?line=105'>106</a>\u001b[0m         \u001b[39mfor\u001b[39;00m optimizer \u001b[39min\u001b[39;00m optimizers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PythonProjects/CasualComp/causal_comp/MLLS_VisProd.ipynb#ch0000053?line=106'>107</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {phase: {} for phase in ['train', 'val', 'test']}\n",
    "early_stopped = False\n",
    "for epoch in tqdm(range(args.VP.epochs+1)):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        results['train'][epoch], _ = run_epoch(training=True, models=models, dataloader=dataloaders['train'],\n",
    "            clss_loss=clss_loss, args=args.VP, optimizers=optimizers, seen_combs=seen_combs)\n",
    "    \n",
    "    # eval\n",
    "    for phase in ['train', 'val']:\n",
    "        results[phase][epoch], _ = run_epoch(training=False, models=models, dataloader=dataloaders[phase],\n",
    "            clss_loss=clss_loss, args=args.VP, seen_combs=seen_combs)\n",
    "        if phase == 'val':\n",
    "            print(\"val harmonic acc: %.1f%%\"%(100*results[phase][epoch]['harmonic acc']))\n",
    "\n",
    "        # early stopping\n",
    "        if args.VP.early_stop_mode != 'off' and args.VP.early_stop_phase == phase:\n",
    "            if epoch == 0:\n",
    "                best_epoch = 0\n",
    "                best_metric = results[phase][epoch][args.VP.early_stop_on]\n",
    "                best_model_states = {space: deepcopy(model.state_dict()) for space, model in models.items()}\n",
    "                no_improv_epochs = 0\n",
    "            else:\n",
    "                current_metric = results[phase][epoch][args.VP.early_stop_on]\n",
    "                delta = current_metric - best_metric\n",
    "                if (args.VP.early_stop_mode == 'max' and delta > args.VP.early_stop_threshold) or (args.VP.early_stop_mode == 'min' and delta < args.VP.early_stop_threshold):\n",
    "                    best_epoch = epoch\n",
    "                    best_metric = current_metric\n",
    "                    best_model_states = {space: deepcopy(model.state_dict()) for space, model in models.items()}\n",
    "                else:\n",
    "                    no_improv_epochs += 1\n",
    "                    if no_improv_epochs >= args.VP.early_stop_patience:\n",
    "                        print(f\"early stopping on epoch {epoch}: '{args.VP.early_stop_on}' has stopped improving \"\n",
    "                                    f\"(patience = {args.VP.early_stop_patience}),\"\n",
    "                                    f\" delta ({delta}) is {'less' if args.VP.early_stop_mode == 'max' else 'more'} \"\n",
    "                                    f\"than min_delta ({args.VP.early_stop_threshold}) for more than {args.VP.early_stop_patience} epochs\")\n",
    "                        early_stopped = True\n",
    "                        break\n",
    "    if early_stopped:\n",
    "        break\n",
    "    \n",
    "    # schedulers\n",
    "    if args.VP.plateau_mode != 'off':\n",
    "        for scheduler in schedulers.values():\n",
    "            scheduler.step(results[args.VP.early_stop_phase][epoch][args.VP.early_stop_on])\n",
    "\n",
    "# returning to best epoch\n",
    "if args.VP.early_stop_mode != 'off':\n",
    "    print(f\"loading best model states to have results['{args.VP.early_stop_phase}'][epoch={best_epoch}]['{args.VP.early_stop_on}'] = {best_metric}\")\n",
    "    for space, model in models.items():\n",
    "        model.load_state_dict(best_model_states[space])\n",
    "        print(f\"loaded '{space}' state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dfs = {phase: pd.DataFrame.from_dict(results[phase], orient='index') for phase in results}\n",
    "for phase, results_df in results_dfs.items():\n",
    "    results_df.index.name = 'epoch'\n",
    "#     results_df['phase'] = phase\n",
    "# results_dfs_cat = pd.concat([results_dfs['train'], results_dfs['val']], axis=0)\n",
    "# results_dfs_cat.sort_index(inplace=True)\n",
    "# results_dfs_cat = results_dfs_cat[['phase'] + list(set(results_dfs_cat.columns)-set(['phase']))]\n",
    "# results_dfs_cat\n",
    "print(\"train results:\")\n",
    "display(results_dfs['train'])\n",
    "print(\"val results:\")\n",
    "display(results_dfs['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity = 0.8\n",
    "# -----------------------------\n",
    "acc_cols = [col for col in results_dfs['val'].columns if 'acc' in col]\n",
    "main_acc_cols = ['seen acc', 'unseen acc', 'harmonic acc']\n",
    "sec_acc_cols = ['color acc', 'shape acc', 'joint acc']\n",
    "loss_cols = [col for col in results_dfs['val'].columns if 'loss' in col]\n",
    "cols_tuples = [(main_acc_cols, 'accuracy'),\n",
    "                (sec_acc_cols, 'accuracy'),\n",
    "                (loss_cols, 'loss'),\n",
    "            ]\n",
    "\n",
    "for cols, ylabel in cols_tuples:\n",
    "    fig = plt.figure()\n",
    "    for col in cols:\n",
    "        df = results_dfs['train']\n",
    "        ax = plt.plot(df.index, df[col], label=f'{col} | train', alpha=opacity)\n",
    "        \n",
    "        df = results_dfs['val']\n",
    "        plt.plot(df.index, df[col], '--', color=ax[0].get_color(), label=f'{col} | val', alpha=opacity)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    if ylabel == 'accuracy':\n",
    "        plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "        plt.locator_params(axis='y', nbins=5)\n",
    "    elif ylabel == 'loss':\n",
    "        plt.yscale('log')\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(0.2, -0.2), frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unshuffled_dataloaders = {phase: torch.utils.data.DataLoader(datasets[phase],\n",
    "                                                batch_size=args.batch_size, shuffle=False,\n",
    "                                                num_workers=args.workers,\n",
    "                                                pin_memory=args.pin_memory) for phase in datasets}\n",
    "Y_color_probs, Y_shape_probs  = {}, {}\n",
    "\n",
    "for phase in unshuffled_dataloaders:\n",
    "    _, log_probs = run_epoch(training=False, models=models, dataloader=unshuffled_dataloaders[phase], clss_loss=clss_loss, args=args.VP, seen_combs=seen_combs)\n",
    "    Y_color_probs[phase] = torch.exp(log_probs['color']).cpu().numpy()\n",
    "    Y_shape_probs[phase] = torch.exp(log_probs['shape']).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_shape_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_shape_probs['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginals EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = list(dataloaders.keys()) + ['m.adapted test']\n",
    "Y_comb['m.adapted test'] = Y_comb['test']\n",
    "label_dists_df['m.adapted test samples'] = label_dists_df['test samples']\n",
    "Y_color['m.adapted test'] = Y_color['test']\n",
    "Y_shape['m.adapted test'] = Y_shape['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_iterations = 100\n",
    "# EM_iterations = 10\n",
    "# -------------------------------------\n",
    "\n",
    "shape_EM_evolution, Y_shape_probs['m.adapted test'] = basic_EM(Y=Y_shape['test'],\n",
    "                                    Y_probs=Y_shape_probs['test'],\n",
    "                                    prior_source = Y_shape_probs['train'].mean(0),\n",
    "                                    EM_iterations=EM_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_shape_probs['m.adapted test'].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'accuracy'\n",
    "metric = 'log-likelihood'\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(4,3))\n",
    "shape_EM_evolution[metric].plot(ax=plt.gca(), linewidth=2)\n",
    "plt.ylabel(metric)\n",
    "plt.title(\"Shape EM evolution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_iterations = 100\n",
    "# EM_iterations = 10\n",
    "# -------------------------------------\n",
    "\n",
    "color_EM_evolution, Y_color_probs['m.adapted test'] = basic_EM(Y=Y_color['test'],\n",
    "                                    Y_probs=Y_color_probs['test'],\n",
    "                                    prior_source = Y_color_probs['train'].mean(0),\n",
    "                                    EM_iterations=EM_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_color_probs['m.adapted test'].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'accuracy'\n",
    "metric = 'log-likelihood'\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(4,3))\n",
    "color_EM_evolution[metric].plot(ax=plt.gca(), linewidth=2)\n",
    "plt.ylabel(metric)\n",
    "plt.title(\"Color EM evolution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginals Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_acc = {phase: (probs.argmax(1) == Y_shape[phase]).sum() / len(Y_shape[phase]) for phase, probs in Y_shape_probs.items()}\n",
    "print(\"shape_acc:\", shape_acc)\n",
    "\n",
    "color_acc = {phase: (probs.argmax(1) == Y_color[phase]).sum() / len(Y_shape[phase]) for phase, probs in Y_color_probs.items()}\n",
    "print(\"color_acc:\", color_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Y_shape_probs['test'][0]\n",
    "b = Y_color_probs['test'][0]\n",
    "c = []\n",
    "for a_ in a:\n",
    "    for b_ in b:\n",
    "        c.append(a_*b_)\n",
    "np.array_equal(np.array(c), np.tensordot(a, b, axes=0).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_VisProd_probs = {phase: np.zeros((len(Y_comb[phase]), distinct_label_n_vals['color']*distinct_label_n_vals['shape'])) for phase in phases}\n",
    "for phase, probs in Y_VisProd_probs.items():\n",
    "    for i in tqdm(range(len(Y_comb[phase]))):\n",
    "        outer_product = np.tensordot(Y_shape_probs[phase][i], Y_color_probs[phase][i], axes=0).flatten()\n",
    "        probs[i,:] = outer_product / outer_product.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM requires probs, verify normalization:\")\n",
    "for phase in phases:\n",
    "    print(f\"(Y_VisProd_probs[{phase}].sum(axis=1) - 1).mean(): %.12f\"%(Y_VisProd_probs[phase].sum(axis=1) - 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_VisProd_preds = {phase: probs.argmax(1) for phase, probs in Y_VisProd_probs.items()}\n",
    "Y_VisProd_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisProd_acc = {phase: (preds == Y_comb[phase]).sum() / len(Y_comb[phase]) for phase, preds in Y_VisProd_preds.items()}\n",
    "VisProd_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase in ['train', 'test', 'm.adapted test']:\n",
    "for phase in Y_VisProd_probs:\n",
    "    y = Y_comb[phase]\n",
    "    y_pred = Y_VisProd_preds[phase]\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'VisProd: {phase} soft pred prior'] = pd.Series(Y_VisProd_probs[phase].mean(0))\n",
    "    label_dists_df[f'VisProd: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'VisProd: {phase} acc'] = label_dists_df[f'VisProd: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "label_dists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_preds = pd.DataFrame(Y_VisProd_probs['test'][:, unseen_combs].argmax(1), columns=['unseen comb idx'])\n",
    "closed_preds['comb idx'] = closed_preds['unseen comb idx'].map(unseen_combs_idx_to_comb_idx)\n",
    "closed_preds['closed true pos'] = closed_preds['comb idx'] == Y_comb['test']\n",
    "closed_true_pos = closed_preds.query(\"`closed true pos`\")['comb idx'].value_counts()\n",
    "label_dists_df[f'VisProd: test CLOSED true pos'] = closed_true_pos\n",
    "label_dists_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted VisProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_source = Y_VisProd_probs['train'].mean(0)\n",
    "prior_shift_pred = 1/len(label_dists_df) * np.ones((len(label_dists_df)))\n",
    "\n",
    "adapted_VisProd_probs = Y_VisProd_probs['test'] * prior_shift_pred / prior_source\n",
    "adapted_VisProd_probs = (adapted_VisProd_probs.T / adapted_VisProd_probs.sum(axis=1)).T\n",
    "adapted_VisProd_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'test'\n",
    "\n",
    "y = Y_comb[phase]\n",
    "y_pred = adapted_VisProd_probs.argmax(1)\n",
    "true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "label_dists_df[f'adapted VisProd: {phase} soft pred prior'] = pd.Series(adapted_VisProd_probs.mean(0))\n",
    "label_dists_df[f'adapted VisProd: {phase} true pos'] = true_pos\n",
    "label_dists_df[f'adapted VisProd: {phase} acc'] = label_dists_df[f'adapted VisProd: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "\n",
    "closed_preds = pd.DataFrame(adapted_VisProd_probs[:, unseen_combs].argmax(1), columns=['unseen comb idx'])\n",
    "closed_preds['comb idx'] = closed_preds['unseen comb idx'].map(unseen_combs_idx_to_comb_idx)\n",
    "closed_preds['closed true pos'] = closed_preds['comb idx'] == Y_comb['test']\n",
    "closed_true_pos = closed_preds.query(\"`closed true pos`\")['comb idx'].value_counts()\n",
    "label_dists_df[f'adapted VisProd: test CLOSED true pos'] = closed_true_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisProd+EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisProd_EM_probs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM_iterations = 100\n",
    "# # EM_iterations = 2\n",
    "\n",
    "# init_prior = 'source prior'\n",
    "# # init_prior = 'uniform'\n",
    "\n",
    "# prior_source_phase = 'est. soft train'\n",
    "# # prior_source_phase = 'est. soft test'\n",
    "# # prior_source_phase = 'true train'\n",
    "# # prior_source_phase = 'true test'\n",
    "# # prior_source_phase = 'uniform'\n",
    "\n",
    "# soft_prior = True # proper EM: SOFT prior_shift_pred = shift_probs.mean(axis=0)\n",
    "# # soft_prior = False # improper EM: HARD prior_shift_pred (calculated from hist)\n",
    "# over_update_probs = False # proper EM shift_probs = shift_probs_0 * prior_shift_pred / prior_train\n",
    "# # over_update_probs = True # improper EM shift_probs = shift_probs * prior_shift_pred / prior_train\n",
    "# # -------------------------------------\n",
    "\n",
    "# VisProd_EM_evolution, VisProd_EM_shift_probs = EM(Y=Y_comb, Y_probs=Y_VisProd_probs, EM_iterations=EM_iterations,\n",
    "#     prior_source_phase=prior_source_phase, init_prior=init_prior, n_combs=len(label_dists_df),\n",
    "#     soft_prior=soft_prior, over_update_probs=over_update_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_iterations = 100\n",
    "# EM_iterations = 10\n",
    "\n",
    "EM_metric_to_plot = 'accuracy'\n",
    "# EM_metric_to_plot = 'log-likelihood'\n",
    "# -------------------------------------\n",
    "prior_source = Y_VisProd_probs['train'].mean(0)\n",
    "\n",
    "for phase in set(Y_VisProd_probs.keys()) - set(['train']):\n",
    "    VisProd_EM_evolution, VisProd_EM_probs[phase] = basic_EM(Y=Y_comb[phase],\n",
    "                                                Y_probs=Y_VisProd_probs[phase],\n",
    "                                                prior_source=prior_source,\n",
    "                                                EM_iterations=EM_iterations)\n",
    "\n",
    "    # plotting EM evolution                                      \n",
    "    plt.figure(figsize=(4,3))\n",
    "    VisProd_EM_evolution[EM_metric_to_plot].plot(ax=plt.gca(), linewidth=2)\n",
    "    plt.ylabel(EM_metric_to_plot)\n",
    "    plt.title(f\"VisProd-EM evolution: {phase}\")\n",
    "\n",
    "    # eval\n",
    "    y = Y_comb[phase]\n",
    "    y_probs = VisProd_EM_probs[phase]\n",
    "    # ----------------------------------\n",
    "\n",
    "    y_pred = y_probs.argmax(1)\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'VisProd+EM: {phase} soft pred prior'] = pd.Series(y_probs.mean(0))\n",
    "    label_dists_df[f'VisProd+EM: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'VisProd+EM: {phase} acc'] = label_dists_df[f'VisProd+EM: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "\n",
    "    closed_preds = pd.DataFrame(y_probs[:, unseen_combs].argmax(1), columns=['unseen comb idx'])\n",
    "    closed_preds['comb idx'] = closed_preds['unseen comb idx'].map(unseen_combs_idx_to_comb_idx)\n",
    "    closed_preds['closed true pos'] = closed_preds['comb idx'] == Y_comb[phase]\n",
    "    closed_true_pos = closed_preds.query(\"`closed true pos`\")['comb idx'].value_counts()\n",
    "    label_dists_df[f'VisProd+EM: {phase} CLOSED true pos'] = closed_true_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisProd + unseen-EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in ['val', 'test']:\n",
    "\n",
    "    VisProd_unseen_EM_probs = VisProd_EM_probs[phase].copy()\n",
    "    VisProd_unseen_EM_probs[:, seen_combs] = Y_VisProd_probs[phase][:, seen_combs]\n",
    "    VisProd_unseen_EM_probs = (VisProd_unseen_EM_probs.T / VisProd_unseen_EM_probs.sum(1)).T\n",
    "\n",
    "    y = Y_comb[phase]\n",
    "    y_probs = VisProd_unseen_EM_probs\n",
    "    # ----------------------------------------\n",
    "\n",
    "    y_pred = y_probs.argmax(1)\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'VisProd+unseen-EM: {phase} soft pred prior'] = pd.Series(y_probs.mean(0))\n",
    "    label_dists_df[f'VisProd+unseen-EM: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'VisProd+unseen-EM: {phase} acc'] = label_dists_df[f'VisProd+unseen-EM: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "\n",
    "    closed_preds = pd.DataFrame(y_probs[:, unseen_combs].argmax(1), columns=['unseen comb idx'])\n",
    "    closed_preds['comb idx'] = closed_preds['unseen comb idx'].map(unseen_combs_idx_to_comb_idx)\n",
    "    closed_preds['closed true pos'] = closed_preds['comb idx'] == Y_comb[phase]\n",
    "    closed_true_pos = closed_preds.query(\"`closed true pos`\")['comb idx'].value_counts()\n",
    "    label_dists_df[f'VisProd+unseen-EM: {phase} CLOSED true pos'] = closed_true_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration evaluation before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "# ----------------\n",
    "\n",
    "plt.figure()\n",
    "ref = np.linspace(0, 1, n_bins)\n",
    "\n",
    "for i_comb in seen_combs:\n",
    "    prob_true, prob_pred = calibration_curve(y_true=Y_comb['test'] == i_comb,\n",
    "        y_prob=Y_VisProd_probs['test'][:,i_comb], n_bins=n_bins)\n",
    "    plt.plot(prob_pred, prob_true, 'o--', label=f\"comb {i_comb}\")\n",
    "\n",
    "plt.plot(ref, ref, '-', label='perfect calibration', linewidth=2)\n",
    "\n",
    "plt.title(\"test calibration (before calibration)\")\n",
    "# plt.legend(fontsize=12, frameon=False)\n",
    "plt.xlabel(\"mean predicted probability\")\n",
    "plt.ylabel(\"fraction of positives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_class = 'ovr'\n",
    "multi_class = 'multinomial'\n",
    "# multi_class = 'none'\n",
    "\n",
    "max_iter = 1000 # default: 100\n",
    "# ------------------------------\n",
    "\n",
    "if args.VP.calibrate:\n",
    "    if multi_class == 'none':\n",
    "        for phase in phases:\n",
    "            Y_VisProd_probs[f'{phase}_calib'] = np.zeros_like(Y_VisProd_probs[phase])\n",
    "            Y_comb_VP[f'{phase}_calib'] = Y_comb_VP[phase]\n",
    "\n",
    "        for i_comb in tqdm(range(Y_VisProd_probs['held-out'].shape[1])):\n",
    "            if (Y_comb_VP['train'] == i_comb).sum() > 0:\n",
    "                comb_seen = True\n",
    "            else:\n",
    "                comb_seen = False\n",
    "\n",
    "            y = (Y_comb_VP['held-out'] == i_comb)\n",
    "            if comb_seen: # can calibrate only if i_comb is seen - contains samples\n",
    "                calibrator = LogisticRegression(penalty='none', max_iter=max_iter)\n",
    "                calibrator.fit(X=Y_VisProd_probs['held-out'][:,i_comb].reshape(-1, 1), y=y)\n",
    "            \n",
    "            for phase in phases:\n",
    "                if comb_seen:\n",
    "                    Y_VisProd_probs[f'{phase}_calib'][:,i_comb] = calibrator.predict_proba(\n",
    "                        Y_VisProd_probs[phase][:,i_comb].reshape(-1, 1))[:,1]\n",
    "                else:\n",
    "                    Y_VisProd_probs[f'{phase}_calib'][:,i_comb] = Y_VisProd_probs[phase][:,i_comb]\n",
    "    else:\n",
    "        # raise NotImplemented\n",
    "        for phase in phases:\n",
    "            Y_VisProd_probs[f'{phase}_calib'] = Y_VisProd_probs[phase].copy()\n",
    "            Y_comb_VP[f'{phase}_calib'] = Y_comb_VP[phase]\n",
    "        \n",
    "        hard_seen_combs = label_dists_df.query('`train samples` > 0').index\n",
    "        y = Y_comb_VP['held-out']\n",
    "        hard_seen_combs_y = y.copy()\n",
    "        hard_seen_combs_y[~np.isin(y, hard_seen_combs)] = 0\n",
    "        calibrator = LogisticRegression(penalty='none', max_iter=max_iter, multi_class=multi_class)\n",
    "        calibrator.fit(X=Y_VisProd_probs['held-out'][:,hard_seen_combs], y=hard_seen_combs_y)\n",
    "        for phase in phases:\n",
    "                Y_VisProd_probs[f'{phase}_calib'][:,hard_seen_combs] = calibrator.predict_proba(\n",
    "                    Y_VisProd_probs[phase][:,hard_seen_combs])\n",
    "        \n",
    "    \n",
    "    for phase in ['train', 'test']:\n",
    "        y = Y_comb_VP[f'{phase}_calib']\n",
    "        y_pred = Y_VisProd_probs[f'{phase}_calib'].argmax(1)\n",
    "        true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "        label_dists_df[f'calib VisProd: {phase} acc'] = label_dists_df[f'VisProd: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "    # display(label_dists_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration evaluation after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "# ----------------\n",
    "\n",
    "if args.VP.calibrate:\n",
    "    for phase in ['held-out', 'test']:\n",
    "        plt.figure()\n",
    "        ref = np.linspace(0, 1, n_bins)\n",
    "\n",
    "        for i_comb in seen_combs:\n",
    "            prob_true, prob_pred = calibration_curve(y_true=Y_comb_VP[f'{phase}_calib'] == i_comb,\n",
    "                y_prob=Y_VisProd_probs[f'{phase}_calib'][:,i_comb], n_bins=n_bins)\n",
    "            plt.plot(prob_pred, prob_true, 'o--', label=f\"comb {i_comb}\")\n",
    "\n",
    "        plt.plot(ref, ref, '-', label='perfect calibration', linewidth=2)\n",
    "\n",
    "        plt.title(f\"{phase} calibration (after calibration)\")\n",
    "        # plt.legend(fontsize=12, frameon=False)\n",
    "        plt.xlabel(\"mean predicted probability\")\n",
    "        plt.ylabel(\"fraction of positives\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calib. VisProd+EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_iterations = 100\n",
    "# EM_iterations = 10\n",
    "# -------------------------------------\n",
    "\n",
    "if args.VP.calibrate:\n",
    "    prior_source = Y_VisProd_probs['train_calib'].mean(0)\n",
    "\n",
    "    calib_VisProd_EM_evolution, calib_VisProd_EM_shift_probs = basic_EM(Y=Y_comb_VP['test_calib'],\n",
    "                                                        Y_probs=Y_VisProd_probs['test_calib'],\n",
    "                                                        prior_source = prior_source,\n",
    "                                                        EM_iterations=EM_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'accuracy'\n",
    "metric = 'log-likelihood'\n",
    "# -----------------------------\n",
    "\n",
    "if args.VP.calibrate:\n",
    "    plt.figure(figsize=(4,3))\n",
    "    calib_VisProd_EM_evolution[metric].plot(ax=plt.gca(), linewidth=2)\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(\"Calib. VisProd-EM evolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.VP.calibrate:\n",
    "    phase = 'test'\n",
    "\n",
    "    y = Y_comb_VP[phase]\n",
    "    y_pred = calib_VisProd_EM_shift_probs.argmax(1)\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'calib VisProd-EM: {phase} soft pred prior'] = pd.Series(VisProd_EM_shift_probs.mean(0))\n",
    "    label_dists_df[f'calib VisProd-EM: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'calib VisProd-EM: {phase} acc'] = label_dists_df[f'calib VisProd-EM: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "\n",
    "    closed_preds = pd.DataFrame(calib_VisProd_EM_shift_probs[:, unseen_combs].argmax(1), columns=['unseen comb idx'])\n",
    "    closed_preds['comb idx'] = closed_preds['unseen comb idx'].map(unseen_combs_idx_to_comb_idx)\n",
    "    closed_preds['closed true pos'] = closed_preds['comb idx'] == Y_comb_VP['test']\n",
    "    closed_true_pos = closed_preds.query(\"`closed true pos`\")['comb idx'].value_counts()\n",
    "    label_dists_df[f'calib VisProd-EM: test CLOSED true pos'] = closed_true_pos\n",
    "    label_dists_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro (per-combination) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_result_cols = [col for col in label_dists_df.columns if 'acc' in col]\n",
    "# macro_result_cols = label_cols + ['state', 'train freq', 'test freq'] + acc_result_cols\n",
    "\n",
    "# macro_results_df = label_dists_df[macro_result_cols]\n",
    "# # results_df = results_df.sort_values(by='train freq', ascending=False)\n",
    "# with pd.option_context('display.float_format',\"{:,.3%}\".format):\n",
    "#     display(macro_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format',\"{:,.3%}\".format):\n",
    "#     # display(results_df.groupby(by=label_cols).mean())\n",
    "#     display(macro_results_df.groupby(by='color').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_result_summary_dict = {}\n",
    "# for state in ['seen', 'unseen']:\n",
    "#     macro_result_summary_dict[state] = macro_results_df.query(f\"state == '{state}'\")[acc_result_cols].fillna(0).mean(0)\n",
    "# macro_result_summary_dict['all'] = macro_results_df[acc_result_cols].fillna(0).mean(0)\n",
    "# macro_result_summary_df = pd.DataFrame.from_dict(macro_result_summary_dict, orient='index')\n",
    "# macro_result_summary_df.columns = macro_result_summary_df.columns.str.replace('acc', '')\n",
    "\n",
    "# acc_seen = macro_result_summary_df.loc['seen']\n",
    "# acc_unseen = macro_result_summary_df.loc['unseen']\n",
    "# macro_result_summary_df.loc['harmonic'] = 2*acc_seen*acc_unseen/(acc_seen+acc_unseen)\n",
    "\n",
    "# print(\"macro accuracy results:\")\n",
    "# with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "#     display(macro_result_summary_df)\n",
    "#     print(\"DON'T USE, macro results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN: Micro metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_result_cols = [col for col in label_dists_df.columns if 'true pos' in col]\n",
    "open_pos_result_cols = [col for col in pos_result_cols if 'CLOSED' not in col]\n",
    "closed_pos_result_cols = [col for col in pos_result_cols if 'CLOSED' in col]\n",
    "samples_cols = [col for col in label_dists_df.columns if 'samples' in col]\n",
    "micro_result_cols = label_cols + ['state'] + samples_cols + pos_result_cols\n",
    "\n",
    "micro_results_df = label_dists_df[micro_result_cols]\n",
    "with pd.option_context('display.float_format',\"{:,.0f}\".format):\n",
    "    display(micro_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = set([name.split(':')[0] for name in pos_result_cols])\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_result_summary_dict = {}\n",
    "for state in ['seen', 'unseen']:\n",
    "    micro_result_summary_dict[state] = micro_results_df.query(f\"state == '{state}'\")[open_pos_result_cols].sum(0)\n",
    "micro_result_summary_dict['all'] = micro_results_df[open_pos_result_cols].sum(0)\n",
    "micro_result_summary_df = pd.DataFrame.from_dict(micro_result_summary_dict, orient='index')\n",
    "\n",
    "# from positives to accuracy\n",
    "for col in micro_result_summary_df.columns:\n",
    "    phase = col.split(': ')[1].split(' true')[0]\n",
    "    micro_result_summary_df.loc['seen', col] /= micro_results_df.query(\"state == 'seen'\")[f'{phase} samples'].sum()\n",
    "    micro_result_summary_df.loc['unseen', col] /= micro_results_df.query(\"state == 'unseen'\")[f'{phase} samples'].sum()\n",
    "    micro_result_summary_df.loc['all', col] /= micro_results_df[f'{phase} samples'].sum()\n",
    "micro_result_summary_df.columns = micro_result_summary_df.columns.str.replace(\"true pos\", '')\n",
    "\n",
    "acc_seen = micro_result_summary_df.loc['seen']\n",
    "acc_unseen = micro_result_summary_df.loc['unseen']\n",
    "micro_result_summary_df.loc['harmonic'] = 2*acc_seen*acc_unseen/(acc_seen+acc_unseen)\n",
    "\n",
    "for col in closed_pos_result_cols:\n",
    "    micro_result_summary_df.loc['closed', col.replace('CLOSED true pos', '')] = micro_results_df[col].sum(0) / micro_results_df.query(\"state == 'unseen'\")['test samples'].sum()\n",
    "\n",
    "print(\"micro accuracy results:\")\n",
    "with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "    display(micro_result_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_seen = 0.8\n",
    "# acc_unseen = 0.4\n",
    "# 2*acc_seen*acc_unseen/(acc_seen+acc_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.outputs_dir == '':\n",
    "    print(\"args.outputs_dir == '' -> skip saving\")\n",
    "else:\n",
    "    if not os.path.isdir(args.outputs_dir):\n",
    "        os.makedirs(args.outputs_dir)\n",
    "        print(f\"created '{args.outputs_dir}'\")\n",
    "\n",
    "    exp_name = \"VT=%d init_seed=%d lambda_d=%.1e lr=%.1e epochs=%d\"%(\n",
    "        args.num_split, args.init_seed, args.VP.lambda_decoupling, args.VP.lr, args.VP.epochs \n",
    "    )\n",
    "    exp_path = pjoin(args.outputs_dir, f'{exp_name}.xlsx')\n",
    "    if os.path.isfile(exp_path):\n",
    "        ovrwrt = input(f\"over-write '{exp_path}'? y/[n] \")\n",
    "    if (not os.path.isfile(exp_path)) or ovrwrt=='y':\n",
    "        micro_result_summary_df.to_excel(exp_path)\n",
    "        print(f\"saved '{exp_path}'\")\n",
    "\n",
    "        args_path = pjoin(args.outputs_dir, f'{exp_name} args.json')\n",
    "        json.dump(args.to_dict(), open(args_path, 'w'))\n",
    "        print(f\"saved '{args_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_prior_cols = [col for col in label_dists_df.columns if 'soft' in col]\n",
    "with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "    # display(label_dists_df[label_cols + ['state', 'train freq', 'test freq'] + soft_prior_cols].query(\"state == 'unseen'\"))\n",
    "    display(label_dists_df[label_cols + ['state', 'train freq', 'test freq'] + soft_prior_cols].sort_values(by='state'))\n",
    "# label_dists_df[label_cols + ['state', 'train freq', 'test freq'] + soft_prior_cols].sort_values(by='state').style.format(\"{:.1%}\").background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = label_dists_df.copy()\n",
    "df = df.sort_values(by='train freq', ascending=False)\n",
    "df.index = range(len(df))\n",
    "\n",
    "df[['train freq', 'test freq']].plot(linewidth=3)\n",
    "df[soft_prior_cols].plot(ax=plt.gca())\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(0, -0.2))\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
    "plt.locator_params(axis='y', nbins=4)\n",
    "plt.xlabel('comb idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_total = True\n",
    "# ----------------------\n",
    "\n",
    "for prior in soft_prior_cols:\n",
    "    print(f\"{prior}:\")\n",
    "    with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "        df = label_dists_df.pivot(index=label_cols[0], columns=label_cols[1], values=prior)\n",
    "        df_ = df.copy()\n",
    "        if show_total:\n",
    "            df_['total'] = df.sum(1)\n",
    "            df_.loc['total'] = df.sum(0)\n",
    "        display(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTENTIONAL BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"intentionally stopping here before next sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen-ratio repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.read_excel('analysis\\AO-Clevr 5000.xlsx')\n",
    "comparison_df = comparison_df.fillna(0)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'full EM: test acc'\n",
    "# y_col = 'multi-class: test acc'\n",
    "# ---------------------------\n",
    "sns.lineplot(data=comparison_df, x='ratio', y=y_col, hue='state', marker=\"o\")\n",
    "plt.legend(title='', frameon=False)\n",
    "plt.xscale('symlog', linthresh=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'analysis/MLLS/HSIC_new'\n",
    "# ----------------------------------------\n",
    "exp_names = [name for name in os.listdir(exp_dir) if '.xlsx' in name]\n",
    "exp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exp_name in exp_names:\n",
    "#     name_parts = exp_name.split()\n",
    "#     num_split = name_parts[0].split('=')[0]\n",
    "#     init_seed = name_parts[1].split('=')[0]\n",
    "results_dfs = []\n",
    "# for num_split in [5000]:\n",
    "for init_seed in [0, 1, 2]:\n",
    "    exp_name = f\"VT=5000 init_seed={init_seed} lambda_d=3.0e+05 lr=1.0e-02 epochs=10.xlsx\"\n",
    "\n",
    "    results_df = pd.read_excel(pjoin(exp_dir,exp_name), index_col=0)\n",
    "    \n",
    "    # results_df['num_split'] = num_split\n",
    "    results_df['init_seed'] = init_seed\n",
    "    results_dfs.append(results_df)\n",
    "\n",
    "results_df = pd.concat(results_dfs, axis=0)\n",
    "results_df.index.name = 'metric'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cols = results_df.columns[~results_df.columns.isin(['num_split', 'init_seed'])]\n",
    "result_cols = results_df.columns[~results_df.columns.isin(['init_seed'])]\n",
    "means_df = results_df.groupby('metric').mean()\n",
    "sems_df = results_df.groupby('metric').sem(ddof=0)\n",
    "\n",
    "agg_results_dict = {}\n",
    "for metric in results_df.index:\n",
    "    agg_results_dict[metric] = {}\n",
    "    for model in result_cols:\n",
    "        mean = means_df.loc[metric, model]\n",
    "        sem = sems_df.loc[metric, model]\n",
    "        agg_results_dict[metric][model] = \"%.1f Â± %.1f\"%(100*mean, 100*sem)\n",
    "agg_results_df = pd.DataFrame(agg_results_dict)\n",
    "df = agg_results_df[['seen', 'unseen', 'harmonic', 'closed']]\n",
    "df.to_clipboard()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class on combinations (CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch_setup(deterministic=True)\n",
    "seed_all(args.init_seed)\n",
    "\n",
    "in_dim = X['train'].shape[1]\n",
    "out_dim = len(global_label_combs)\n",
    "\n",
    "assert in_dim == 512\n",
    "Net_combs = nn.Sequential(\n",
    "    nn.Linear(in_dim, 128),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(64, out_dim),\n",
    ")\n",
    "cp = Checkpoint(dirname='checkpoints\\combs_net', monitor='valid_acc_best', load_best=True)\n",
    "net_combs = NeuralNetClassifier(\n",
    "    Net_combs,\n",
    "    max_epochs=10,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.SGD,\n",
    "    lr=2e-1,\n",
    "    batch_size=args.batch_size,\n",
    "    iterator_train__shuffle=True,\n",
    "    train_split=ValidSplit(cv=args.skorch_val_ratio, stratified=True),\n",
    "    device=device,\n",
    "    callbacks=[EpochScoring(scoring='accuracy', lower_is_better=False, on_train=True), cp],\n",
    ")\n",
    "net_combs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_combs.fit(X['train'], Y_comb['train']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_comb_probs = {phase: net_combs.predict_proba(X[phase]) for phase in ['train', 'test']}\n",
    "for phase in ['train', 'test']:\n",
    "#     assert set(np.unique(Y[phase])) == {0,1} # \"EM below assumes labels are binary!\"\n",
    "    print(f\"(Y_comb_probs[{phase}].sum(axis=1) - 1).mean(): %.12f\"%(Y_comb_probs[phase].sum(axis=1) - 1).mean())\n",
    "#     print(f\"Y_comb_probs[{phase}].sum(axis=1)\")\n",
    "#     display(Y_comb_probs[phase].sum(axis=1))\n",
    "    # assert (Y_comb_probs[phase].sum(axis=1) - 1).mean() == 0, 'EM requires probs (normalized)!'\n",
    "Y_comb_probs['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_comb_pred = {phase: Y_comb_probs[phase].argmax(axis=1) for phase in ['train', 'test']}\n",
    "Y_comb_pred['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in ['train', 'test']:\n",
    "    y = Y_comb[phase]\n",
    "    y_pred = Y_comb_pred[phase]\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'multi-class: {phase} soft pred prior'] = pd.Series(Y_comb_probs[phase].mean(0))\n",
    "    label_dists_df[f'multi-class: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'multi-class: {phase} acc'] = label_dists_df[f'multi-class: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "# label_dists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cols = ['train freq', 'test freq', 'multi-class: train acc', 'multi-class: test acc']\n",
    "# results_df = label_dists_df[result_cols].sort_values(by='train freq', ascending=False)\n",
    "# results_df_styled = results_df.style.format(\"{:.1%}\").background_gradient(cmap='Greens')\n",
    "# results_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cols = label_cols + ['train freq', 'test freq', 'multi-class: train acc', 'multi-class: test acc']\n",
    "# results_df = label_dists_df[result_cols]\n",
    "# # results_df = results_df.sort_values(by='train freq', ascending=False)\n",
    "# with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "#     display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase in ['train', 'test']:\n",
    "#     print(f'multi-class combs net - {phase} soft predicted prior (mean of probs):')\n",
    "#     with pd.option_context('display.float_format',\"{:,.1%}\".format):\n",
    "#         display(label_dists_df.pivot(index='shape', columns='color', values=f'multi-class: {phase} soft pred prior'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information - MI, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/148N5UL_qCk7LT10OyFp4wG9QvipNJ-Mphieo5Yu6TzA/edit#heading=h.eonun6y2wp8l\n",
    "* https://en.wikipedia.org/wiki/Information_theory \n",
    "* https://en.wikipedia.org/wiki/Mutual_information\n",
    "* https://en.wikipedia.org/wiki/Conditional_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "# ------------------\n",
    "for model, probs in [('shape', Y_shape_probs[phase]), ('color', Y_color_probs[phase])]:\n",
    "    entropy = -probs * np.log(probs)\n",
    "    print(f\"{model} entropy:\")\n",
    "    print(entropy.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joint dist ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = {}\n",
    "mean_pred_prob = {}\n",
    "\n",
    "for phase in ['train', 'test']:\n",
    "    p_x_p_y = np.take_along_axis(Y_VisProd_probs[phase], Y_comb[phase].reshape(-1,1), axis=1)\n",
    "    mean_pred_prob[phase] = p_x_p_y.mean()\n",
    "    mutual_info[phase] = np.log(1/p_x_p_y).mean()\n",
    "print(\"mutual_info:\", mutual_info)\n",
    "print(\"mean probability of the predicted joint combination:\", mean_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "# ------------------\n",
    "info_df = pd.DataFrame(index=range(len(Y_comb[phase])))\n",
    "info_df['comb idx'] = Y_comb[phase]\n",
    "info_df['MI'] = np.log(1/np.take_along_axis(Y_VisProd_probs[phase], Y_comb[phase].reshape(-1,1), axis=1))\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dists_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ = label_dists_df[['shape', 'color', f'{phase} samples', f'VisProd: {phase} acc']].copy()\n",
    "dist_[f'{phase} mean MI'] = info_df.groupby('comb idx').mean()\n",
    "with pd.option_context('display.float_format',\"{:,.1e}\".format):\n",
    "    df = dist_.pivot(index=label_cols[0], columns=label_cols[1], values=f'{phase} mean MI')\n",
    "    print(f'{phase} mean MI - using g.t. comb labels:')\n",
    "    display(df)\n",
    "    print(\"micro mean: %.1e\"%(info_df['MI'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using joint dist estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "joint_probs = Y_comb_probs[phase]\n",
    "outer_prod_probs = Y_VisProd_probs[phase]\n",
    "# joint_probs = Y_comb_probs['train'][:, seen_combs]\n",
    "# outer_prod_probs = Y_VisProd_probs['train'][:, seen_combs]\n",
    "eps = 0\n",
    "# ---------------\n",
    "eps_vect = eps * np.ones(outer_prod_probs.shape[1])\n",
    "# copula = (joint_probs) / (outer_prod_probs + eps_vect)\n",
    "copula = (outer_prod_probs) / (joint_probs + eps_vect)\n",
    "copula.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullify_unseen = True\n",
    "# -------------------------------\n",
    "\n",
    "dist_ = label_dists_df[['shape', 'color', f'{phase} samples', f'VisProd: {phase} acc']].copy()\n",
    "dist_[f'{phase} VisProd / joint prob'] = copula.mean(0)\n",
    "\n",
    "mutual_info = outer_prod_probs * np.log(copula)\n",
    "mutual_info[outer_prod_probs==0] = 0\n",
    "dist_[f'{phase} MI'] = mutual_info.mean(0)\n",
    "\n",
    "if nullify_unseen:\n",
    "    dist_.loc[unseen_combs, f'{phase} VisProd / joint prob'] = np.NaN\n",
    "    dist_.loc[unseen_combs, f'{phase} MI'] = np.NaN\n",
    "    mutual_info = mutual_info[:, seen_combs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format',\"{:,.1e}\".format):\n",
    "    df = dist_.pivot(index=label_cols[0], columns=label_cols[1], values=f'{phase} VisProd / joint prob')\n",
    "    print(f'{phase} mean VisProd prob / estimated comb prob:')\n",
    "    display(df)\n",
    "\n",
    "    print(f'{phase} mean MI - using joint prob estimator:')\n",
    "    df = dist_.pivot(index=label_cols[0], columns=label_cols[1], values=f'{phase} MI')\n",
    "    display(df)\n",
    "    if nullify_unseen:\n",
    "        print(\"seen - micro mean: %.1e\"%(mutual_info.mean()))\n",
    "    else:\n",
    "        print(\"micro mean: %.1e\"%(mutual_info.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noise to unseen combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_meta_df = label_dists_df[['train freq', 'test freq']].copy()\n",
    "EM_meta_df['mean test prob'] = Y_comb_probs['test'].mean(0)\n",
    "EM_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_mean_prob = EM_meta_df.loc[~EM_meta_df['train freq'].isna() ,'mean test prob'].mean()\n",
    "unseen_mean_prob = EM_meta_df.loc[EM_meta_df['train freq'].isna() ,'mean test prob'].mean()\n",
    "print(\"mean prob - seen, unseen = %.2e, %.2e\"%(seen_mean_prob, unseen_mean_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor = np.ones_like(Y_comb_probs['test'])\n",
    "# factor[:, label_dists_df['train freq'].isna().values] = 100\n",
    "# factor[0][label_dists_df['train freq'].isna().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_iterations = 100\n",
    "# EM_iterations = 10\n",
    "\n",
    "init_prior = 'source prior'\n",
    "# init_prior = 'uniform'\n",
    "\n",
    "prior_source_phase = 'est. soft train'\n",
    "# prior_source_phase = 'est. soft test'\n",
    "# prior_source_phase = 'true train'\n",
    "# prior_source_phase = 'true test'\n",
    "# prior_source_phase = 'uniform'\n",
    "\n",
    "soft_prior = True # proper EM: SOFT prior_shift_pred = shift_probs.mean(axis=0)\n",
    "# soft_prior = False # improper EM: HARD prior_shift_pred (calculated from hist)\n",
    "over_update_probs = False # proper EM shift_probs = shift_probs_0 * prior_shift_pred / prior_train\n",
    "# over_update_probs = True # improper EM shift_probs = shift_probs * prior_shift_pred / prior_train\n",
    "\n",
    "n_combs = len(label_dists_df)\n",
    "# -------------------------------------\n",
    "    \n",
    "if (not soft_prior) or over_update_probs or (prior_source_phase != 'est. soft train'):\n",
    "    confirm_ui = input(\"confirm performing an IMPROPER EM for debugging, \" \\\n",
    "        \"(for a proper EM, set: soft_prior = True, over_update_probs = False, \"\\\n",
    "        \"prior_source_phase = 'est. soft train'): y/[n] \")\n",
    "    if confirm_ui != 'y':\n",
    "        raise RuntimeError(\"terminated by user\")\n",
    "\n",
    "if 'est. soft' in prior_source_phase:\n",
    "    Y_probs_source = Y_comb_probs['test' if 'test' in prior_source_phase else 'train']\n",
    "    prior_source = Y_probs_source.mean(axis=0)\n",
    "elif 'true' in prior_source_phase:\n",
    "    Y_source = Y_comb['test' if 'test' in prior_source_phase else 'train']\n",
    "    hist = np.bincount(Y_source, minlength=n_combs)\n",
    "    prior_source = hist / hist.sum()\n",
    "elif prior_source_phase == 'uniform':\n",
    "    prior_source = 1/n_combs * np.ones(n_combs)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(f\"source prior ('{prior_source_phase}'):\")\n",
    "display(prior_source)\n",
    "\n",
    "# init\n",
    "EM_results = pd.DataFrame(index=range(EM_iterations))\n",
    "EM_results.index.name = 'iteration'\n",
    "shift_labels = Y_comb['test']\n",
    "shift_probs_0 = Y_comb_probs['test']\n",
    "shift_probs = shift_probs_0\n",
    "\n",
    "if init_prior == 'source prior':\n",
    "    prior_shift_pred = prior_source\n",
    "elif init_prior == 'uniform':\n",
    "    prior_shift_pred = 1/n_combs * np.ones(n_combs)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# eval\n",
    "shift_preds = shift_probs.argmax(axis=1)\n",
    "positives = shift_preds==shift_labels\n",
    "# EM_results.loc[0, [f'accuracy ({label})' for label in range(args.n_labels)]] = positives.mean(axis=0)\n",
    "EM_results.loc[0, 'accuracy'] = positives.mean()\n",
    "# likelihood\n",
    "Y_true_pos_probs = np.take_along_axis(shift_probs, Y_comb['test'].reshape(-1,1), axis=1)\n",
    "EM_results.loc[0, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "# EM\n",
    "for i_EM in tqdm(range(1, 1+EM_iterations)):        \n",
    "    # E-step\n",
    "    if over_update_probs:\n",
    "        shift_probs = shift_probs * prior_shift_pred / prior_source\n",
    "    else:\n",
    "        shift_probs = shift_probs_0 * prior_shift_pred / prior_source\n",
    "    shift_probs = (shift_probs.T / shift_probs.sum(axis=1)).T\n",
    "    \n",
    "    shift_preds = shift_probs.argmax(axis=1)\n",
    "    \n",
    "    # eval\n",
    "    positives = shift_preds==shift_labels\n",
    "    EM_results.loc[i_EM, 'accuracy'] = positives.mean()\n",
    "    # likelihood\n",
    "    Y_true_pos_probs = np.take_along_axis(shift_probs, Y_comb['test'].reshape(-1,1), axis=1)\n",
    "    EM_results.loc[i_EM, 'log-likelihood'] = np.log(Y_true_pos_probs).sum()\n",
    "\n",
    "    # M-step\n",
    "    if soft_prior:\n",
    "        prior_shift_pred = shift_probs.mean(axis=0)\n",
    "    else:\n",
    "        hist_shift_pred = np.bincount(shift_preds, minlength=args.n_classes_per_label * args.n_labels)\n",
    "        prior_shift_pred = hist_shift_pred / hist_shift_pred.sum()\n",
    "\n",
    "assert (~np.isfinite(shift_probs)).sum() == 0\n",
    "# full_EM_evolution, full_EM_shift_probs = EM(Y=Y_comb, Y_probs=Y_comb_probs, EM_iterations=EM_iterations,\n",
    "#     prior_source_phase=prior_source_phase, init_prior=init_prior, n_combs=len(label_dists_df),\n",
    "#     soft_prior=soft_prior, over_update_probs=over_update_probs)\n",
    "full_EM_evolution = EM_results\n",
    "full_EM_shift_probs = shift_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dists_df['full EM: test pred prior'] = pd.Series(full_EM_shift_probs.mean(0))\n",
    "print('full EM - test predicted test prior:')\n",
    "with pd.option_context('display.float_format',\"{:,.1e}\".format):\n",
    "    display(label_dists_df.pivot(index='shape', columns='color',\n",
    "    values='full EM: test pred prior'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'accuracy'\n",
    "metric = 'log-likelihood'\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(4,3))\n",
    "full_EM_evolution[metric].plot(ax=plt.gca(), linewidth=2)\n",
    "plt.ylabel(metric)\n",
    "plt.title(\"full EM evolution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_EM_preds = full_EM_shift_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in ['test']:\n",
    "    y = Y_comb[phase]\n",
    "    y_pred = full_EM_preds\n",
    "    true_pos = pd.Series(y[y==y_pred]).value_counts()\n",
    "    label_dists_df[f'full EM: {phase} true pos'] = true_pos\n",
    "    label_dists_df[f'full EM: {phase} acc'] = label_dists_df[f'multi-class: {phase} true pos'] / label_dists_df[f'{phase} samples']\n",
    "label_dists_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03ad60beb30bb83b358f912f6d215553ccbadb86cc2c38cd3e235690c289a64e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Comp1101')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
