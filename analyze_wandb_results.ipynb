{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.802962Z",
     "start_time": "2021-12-20T13:31:23.779059Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import scipy.stats\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FormatStrFormatter, PercentFormatter, FuncFormatter, ScalarFormatter\n",
    "matplotlib.rcParams['figure.dpi'] = 150\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "import seaborn as sns\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s (%(levelname)s): %(message)s',\n",
    "                   datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger=logging.getLogger('main logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.818951Z",
     "start_time": "2021-12-20T13:31:23.805953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.5.1\n",
      "wandb version: 0.12.14\n"
     ]
    }
   ],
   "source": [
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"wandb version:\", wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.850834Z",
     "start_time": "2021-12-20T13:31:23.822928Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.866791Z",
     "start_time": "2021-12-20T13:31:23.853826Z"
    }
   },
   "outputs": [],
   "source": [
    "user = 'ozziko'\n",
    "project = 'MLLS_VisProd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing multiple runs and sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.898737Z",
     "start_time": "2021-12-20T13:31:23.868786Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "runs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:23.914695Z",
     "start_time": "2021-12-20T13:31:23.900714Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_paths = []\n",
    "# run_ids = ['2e5ginyh', '35cdvm28']\n",
    "# run_paths = ['%s/%s/%s'%(user, project, run_id) for run_id in run_ids]\n",
    "# run_paths = ['ozziko/DLS_4/7i4eh931', 'ozziko/DLS_4/a58owmkf', 'ozziko/DLS_4/2f3hswja']\n",
    "# -----------------------------------------------------------------------------------------\n",
    "runs.extend([api.run(run) for run in run_paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:26.476888Z",
     "start_time": "2021-12-20T13:31:23.917679Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:31 (INFO): downloading from wandb api all runs of sweep 'cnes1zjo'\n",
      "2022-05-02 17:59:34 (INFO): completed downloading runs\n"
     ]
    }
   ],
   "source": [
    "# sweep_ids = ['rgvkiytn', 'ggar8ann']\n",
    "# sweep_name = 'cifar100 res18_v3 PEN train=val=0.1 feature noise'\n",
    "sweep_name = ''\n",
    "sweep_ids = []\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "if len(sweep_ids)==0 and len(run_paths)==0:\n",
    "    sweep_id_ui = input(\"sweep_ids and run_paths are empty, enter sweep id to download or leave empty to load: \")\n",
    "    if len(sweep_id_ui)>0:\n",
    "        sweep_ids = [sweep_id_ui]\n",
    "    # else:\n",
    "    #     raise RuntimeError(\"sweep_ids and run_paths are empty, no sweep id entered manually -> aborting\")\n",
    "    # elif len(run_paths)==0:\n",
    "    #     raise RuntimeError(\"sweep_ids=[], no sweep id entered, run_paths=[] -> aborting\")\n",
    "\n",
    "if len(sweep_ids)>0:\n",
    "    sweep_paths = ['%s/%s/%s'%(user, project, sweep_id) for sweep_id in sweep_ids]\n",
    "    # for sweep_path in sweep_paths:\n",
    "    #     sweep = api.sweep(sweep_path)\n",
    "    #     runs.extend([run for run in sweep.runs])\n",
    "\n",
    "    for sweep_id in sweep_ids:\n",
    "        logger.info(f\"downloading from wandb api all runs of sweep '{sweep_id}'\")\n",
    "        runs.extend(api.runs(path=f\"{user}/{project}\", filters={\"sweep\": sweep_id}))\n",
    "    logger.info(\"completed downloading runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming analysis, creating dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.379708Z",
     "start_time": "2021-12-20T13:31:26.479903Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auto_name_if_possible = True\n",
    "# auto_name_if_possible = False\n",
    "# -----------------------------------------------\n",
    "\n",
    "if 'sweep_ids' in globals() and len(sweep_ids)==1:\n",
    "    sweep = api.sweep(sweep_paths[0])\n",
    "    sweep_name = sweep.config['name']\n",
    "    sweep_id = sweep.id\n",
    "    if auto_name_if_possible:\n",
    "        analysis_name = f'{sweep_id} - {sweep_name}'\n",
    "    else:\n",
    "        msg = f\"analysis name (hit enter to use sweep id + name: '{sweep_id} - {sweep_name}'): \"\n",
    "        analysis_name = input(msg)\n",
    "        if analysis_name == '':\n",
    "            analysis_name = f'{sweep_id} - {sweep_name}'\n",
    "else:\n",
    "    if auto_name_if_possible and len(sweep_ids)>0:\n",
    "        analysis_name = f\"{'+'.join(sweep_ids)} - \"\n",
    "        if sweep_name=='':\n",
    "            analysis_name += input(f\"analysis name: {analysis_name}\")\n",
    "        else:\n",
    "            analysis_name += sweep_name\n",
    "    else:\n",
    "        analysis_name = input(\"analysis name: \")\n",
    "    \n",
    "analysis_path = pjoin('analysis', analysis_name)\n",
    "try:\n",
    "    os.makedirs(analysis_path)\n",
    "    logger.info(f\"created '{analysis_path}'\")\n",
    "except FileExistsError:\n",
    "    if input(\n",
    "        f\"'{analysis_path}' already exists, continue and possibly overwrite results? [y]/n \")=='n':\n",
    "        raise RuntimeError(\"aborted by user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data & meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.411649Z",
     "start_time": "2021-12-20T13:31:28.382701Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:35 (INFO): parsing runs data & meta\n",
      "2022-05-02 17:59:36 (INFO): saved runs_args_df\n",
      "2022-05-02 17:59:36 (INFO): saved runs_meta_df\n",
      "2022-05-02 17:59:36 (INFO): total - runs: 60, compute duration: 8.4 hr, mean: 8.4 min\n"
     ]
    }
   ],
   "source": [
    "if len(runs)>0:\n",
    "    logger.info(\"parsing runs data & meta\")\n",
    "    runs_args_dict = {}\n",
    "    runs_meta_dict = {}\n",
    "    for run in runs:\n",
    "        run_config = json.loads(run.json_config)\n",
    "        run_args_dict = {arg: run_config[arg]['value'] for arg in run_config}\n",
    "        run_args_dict_flat = pd.json_normalize(run_args_dict).iloc[0].to_dict()\n",
    "        # keeping parsed args - no need for sweep (yaml) args that start with 'args.':\n",
    "        run_args_dict_flat = {key:val for key,val in run_args_dict_flat.items() if not 'args.' in key}\n",
    "        runs_args_dict[run.id] = run_args_dict_flat\n",
    "\n",
    "        runs_meta_dict[run.id] = {\n",
    "            'name': run.name,\n",
    "            'state': run.state,\n",
    "            'duration (s)': (pd.to_datetime(run.heartbeat_At) - pd.to_datetime(run.created_At)).total_seconds()\n",
    "        }\n",
    "\n",
    "        if hasattr(run, 'sweep') and run.sweep is not None:\n",
    "            runs_meta_dict[run.id]['sweep_id'] = run.sweep.id\n",
    "            runs_meta_dict[run.id]['sweep name'] = run.sweep.config['name']\n",
    "\n",
    "    runs_args_df = pd.DataFrame.from_dict(runs_args_dict)\n",
    "    runs_args_df.index.name = 'arg'\n",
    "    # runs_args_df.to_excel(pjoin(analysis_path, 'runs_args_df.xlsx'))\n",
    "    runs_args_df.to_pickle(pjoin(analysis_path, 'runs_args_df.pickle'))\n",
    "    # torch.save(runs_args_df, pjoin(analysis_path, 'runs_args_df.pickle'))\n",
    "    logger.info(\"saved runs_args_df\")\n",
    "\n",
    "    runs_meta_df = pd.DataFrame.from_dict(runs_meta_dict, orient='index')\n",
    "    runs_meta_df.index.name = 'run id'\n",
    "    runs_meta_df.to_excel(pjoin(analysis_path, 'runs_meta_df.xlsx'))\n",
    "    runs_meta_df.to_pickle(pjoin(analysis_path, 'runs_meta_df.pickle'))\n",
    "    logger.info(\"saved runs_meta_df\")\n",
    "else:\n",
    "    logger.info(\"len(runs)==0 -> loading runs data & meta\")\n",
    "    runs_args_df = pd.read_pickle(pjoin(analysis_path, 'runs_args_df.pickle'))\n",
    "    runs_meta_df = pd.read_pickle(pjoin(analysis_path, 'runs_meta_df.pickle'))\n",
    "    logger.info(\"loaded runs_args_df and runs_meta_df\")\n",
    "\n",
    "logger.info(\"total - runs: %d, compute duration: %.1f hr, mean: %.1f min\"%(\n",
    "    len(runs_meta_df), runs_meta_df['duration (s)'].sum()/3600, runs_meta_df['duration (s)'].mean()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.427592Z",
     "start_time": "2021-12-20T13:31:28.413631Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# qshow(runs_meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.459501Z",
     "start_time": "2021-12-20T13:31:28.431571Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:37 (INFO): parsing run metrics\n",
      "2022-05-02 17:59:37 (INFO): saved runs_metrics_df\n"
     ]
    }
   ],
   "source": [
    "add_names = True\n",
    "# ------------------------------\n",
    "\n",
    "if len(runs)>0:\n",
    "    logger.info(\"parsing run metrics\")\n",
    "    runs_metrics_dict = {run.id:run.summary._json_dict for run in runs}\n",
    "    runs_metrics_df = pd.DataFrame.from_dict(runs_metrics_dict)\n",
    "    runs_metrics_df.index.name = 'metric'\n",
    "    if add_names:\n",
    "        runs_metrics_df.loc['RUN NAME (not from summary)'] = [run.name for run in runs]\n",
    "\n",
    "    runs_metrics_df['n_uniques str'] = runs_metrics_df.astype(str).nunique(axis=1)\n",
    "    # n_cols = len(runs_metrics_df.columns)\n",
    "    # eq = [runs_metrics_df.iloc[:,i_col]==runs_metrics_df.iloc[:,0] for i_col in range(1,n_cols)]\n",
    "    # eq = pd.concat(eq, axis=1)\n",
    "    # runs_metrics_df['runs eq to 1st run'] = eq.sum(axis=1)\n",
    "    # runs_metrics_df.sort_values(by='runs eq to 1st run', inplace=True)\n",
    "    runs_metrics_df.sort_values(by='n_uniques str', ascending=False, inplace=True)\n",
    "\n",
    "    runs_metrics_df.to_excel(pjoin(analysis_path, 'runs_metrics_df.xlsx'))\n",
    "    runs_metrics_df.to_pickle(pjoin(analysis_path, 'runs_metrics_df.pickle'))\n",
    "    logger.info(\"saved runs_metrics_df\")\n",
    "else:\n",
    "    logger.info(\"len(runs)==0 -> loading runs_metrics_df\")\n",
    "    runs_metrics_df = pd.read_pickle(pjoin(analysis_path, 'runs_metrics_df.pickle'))\n",
    "    logger.info(\"loaded runs_metrics_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.475453Z",
     "start_time": "2021-12-20T13:31:28.462488Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to compare runs open runs_metrics_df.xlsx, NOT qshow - qgrid doens't show things written inside <>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unfinished runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.490420Z",
     "start_time": "2021-12-20T13:31:28.477476Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_states_series = runs_meta_df['state']\n",
    "finished_runs_series = run_states_series[run_states_series == 'finished']\n",
    "unfinished_runs_series = run_states_series[run_states_series != 'finished']\n",
    "if len(unfinished_runs_series)>0:\n",
    "    logger.warning(f\"unfinished_runs_series (len: {len(unfinished_runs_series)}):\")\n",
    "    pprint(unfinished_runs_series)\n",
    "    unfinished_ui = input(\"\\nhow to deal with unfinished runs: drop/[abort]/continue \")\n",
    "    if unfinished_ui not in ['drop', 'continue']:\n",
    "        raise RuntimeError(\"aborted by user\")\n",
    "\n",
    "    runs_to_drop = unfinished_runs_series.index\n",
    "    crashed_runs_series = run_states_series[run_states_series == 'crashed']\n",
    "    if unfinished_ui=='drop' and 0 < len(crashed_runs_series) < len(unfinished_runs_series):\n",
    "            drop_ui = input(\"drop all unfinished runs (failed+crashed), or only crashed ones? all/[crashed] \")\n",
    "            if drop_ui!='all':\n",
    "                runs_to_drop = crashed_runs_series.index\n",
    "    runs_to_keep = run_states_series.index[~run_states_series.index.isin(runs_to_drop)]\n",
    "\n",
    "if len(unfinished_runs_series)>0 and unfinished_ui == 'drop':\n",
    "    runs_args_df = runs_args_df[runs_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-inferring swept_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.584693Z",
     "start_time": "2021-12-20T13:31:28.495399Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:37 (INFO): auto-inferring swept_args\n",
      "2022-05-02 17:59:37 (INFO): removed cfg.run_name from swept_args (arg in non_swept_non_constant_args)\n",
      "2022-05-02 17:59:37 (INFO): removed cfg.output_dir from swept_args (arg in non_swept_non_constant_args)\n",
      "2022-05-02 17:59:37 (INFO): auto-recognized cfg.seed = -1 in sweep, so its vals are like data.seed -> removed 'cfg.seed' from swept_args\n",
      "2022-05-02 17:59:37 (INFO): removed 'data.seed' from swept_args since it's decided by 'data.num_split'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto inferred swept_args:\n",
      "['VP.lambda_decoupling', 'VP.lr', 'data.num_split']\n"
     ]
    }
   ],
   "source": [
    "non_swept_non_constant_args = 'cfg.run_name, cfg.output_dir, data.dir, ' \\\n",
    "    'wandb.sweep_name, wandb.run_naming_args, ' \\\n",
    "    'cfg.last_git_commit'.split(', ') # the only non-swept args that are expected to change between runs\n",
    "ignore_NaNs = True\n",
    "# ------------------------------------------------------------\n",
    "logger.info(\"auto-inferring swept_args\")\n",
    "\n",
    "runs_args_df_ext = runs_args_df.copy()\n",
    "unique_vals = {}\n",
    "for arg in runs_args_df_ext.index:\n",
    "    unique_vals[arg] = set(runs_args_df_ext.loc[arg].astype(str).unique())\n",
    "#     if 'nan' in unique_vals[arg]:\n",
    "#         logger.info(f\"'{arg}' unique vals contain 'nan'\")\n",
    "#     if ignore_NaNs and 'nan' in unique_vals[arg]:\n",
    "#         unique_vals[arg] -= {'nan'}\n",
    "#         logger.info(f\"'nan' was removed from '{arg}' unique vals (ignore_NaNs == True)\")\n",
    "\n",
    "nunique_vals = {arg:len(unique_vals[arg]) for arg in unique_vals}\n",
    "runs_args_df_ext['n_uniques'] = pd.Series(nunique_vals)\n",
    "# runs_args_df_ext['n_uniques'] = runs_args_df_ext.astype(str).nunique(axis=1)\n",
    "# runs_args_df_ext['n_uniques'] = runs_args_df_ext.nunique(axis=1)\n",
    "runs_args_df_ext.sort_values(by='n_uniques', ascending=False, inplace=True)\n",
    "\n",
    "experiment_alpha_on = []\n",
    "if len(runs_args_df.columns) > 2:\n",
    "    swept_args = runs_args_df_ext.query(\"n_uniques>1\").index.tolist()\n",
    "    swept_args_original = swept_args.copy()\n",
    "    for arg in non_swept_non_constant_args:\n",
    "        if arg in swept_args:\n",
    "            swept_args.remove(arg)\n",
    "            logger.info(f\"removed {arg} from swept_args (arg in non_swept_non_constant_args)\")\n",
    "\n",
    "    if ('cfg.seed' in swept_args) and ('data.seed' in swept_args):\n",
    "        seeds_joint_dist = runs_args_df.T.groupby(['cfg.seed', 'data.seed']).size()\n",
    "        if len(seeds_joint_dist) == nunique_vals['cfg.seed']:\n",
    "            swept_args.remove('cfg.seed')\n",
    "            logger.info(f\"auto-recognized cfg.seed = -1 in sweep, so its vals are like data.seed -> removed 'cfg.seed' from swept_args\")\n",
    "    if 'data.seed' in swept_args:\n",
    "        swept_args.remove('data.seed')\n",
    "        logger.info(f\"removed 'data.seed' from swept_args since it's decided by 'data.num_split'\")\n",
    "    \n",
    "    # if auto_reduce_alphas_by_exp_alpha and 'experiment_alpha' in swept_args:\n",
    "    #     if 'experiment_alpha_on' in runs_args_df.index:\n",
    "    #         experiment_alpha_on = runs_args_df.loc['experiment_alpha_on'].unique().tolist()\n",
    "    #         if len(experiment_alpha_on)==1:\n",
    "    #             experiment_alpha_on = experiment_alpha_on[0]\n",
    "    #             logger.info(f\"auto_reduce_alphas_by_exp_alpha==True, experiment_alpha_on: '{experiment_alpha_on}'\")\n",
    "    #             if 'train' in experiment_alpha_on:\n",
    "    #                 swept_args.remove('dataloading.train.Dirichlet_alpha')\n",
    "    #                 logger.info(\"removed 'dataloading.train.Dirichlet_alpha' from swept_args\")\n",
    "    #             if 'val' in experiment_alpha_on:\n",
    "    #                 swept_args.remove('dataloading.val.Dirichlet_alpha')\n",
    "    #                 logger.info(\"removed 'dataloading.val.Dirichlet_alpha' from swept_args\")\n",
    "    #             if 'test' in experiment_alpha_on:\n",
    "    #                 swept_args.remove('data.test.Dirichlet_alpha')\n",
    "    #                 logger.info(\"removed 'data.test.Dirichlet_alpha' from swept_args\")\n",
    "        # else:\n",
    "        #     raise RuntimeError(\"auto_reduce_alphas_by_exp_alpha==True, 'experiment_alpha' in swept_args but 'experiment_alpha_on' not in runs_args_df.index\")\n",
    "        \n",
    "    print(\"auto inferred swept_args:\")\n",
    "    pprint(swept_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying constant non-swept args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:28.981058Z",
     "start_time": "2021-12-20T13:31:28.603637Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:38 (INFO): verifying constant non-swept args\n",
      "2022-05-02 17:59:38 (INFO): all non-swept args are the same in all runs :-)\n",
      "2022-05-02 17:59:38 (INFO): saved runs_args_df_ext\n"
     ]
    }
   ],
   "source": [
    "show_run_name = True\n",
    "# show_run_name = False\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "if 'swept_args' in locals():\n",
    "    logger.info(\"verifying constant non-swept args\")\n",
    "    runs_args_df_ext['is_swept'] = runs_args_df_ext.index.isin(swept_args)\n",
    "\n",
    "    # verifying constant non-swept args\n",
    "    runs_args_df_inconsistent = runs_args_df_ext.query(\n",
    "        f\"not is_swept & n_uniques==2 & arg not in {non_swept_non_constant_args} & arg not in {swept_args_original}\")\n",
    "    if len(runs_args_df_inconsistent)>0:\n",
    "        logger.warning(\"not all non-swept args are the same in all runs! check runs_args_df_inconsistent\")\n",
    "        pprint(runs_args_df_inconsistent)\n",
    "        if input(\"continue? y/[n]\")!='y':\n",
    "            raise RuntimeError(\"aborted by user\")\n",
    "\n",
    "    else:\n",
    "        logger.info(\"all non-swept args are the same in all runs :-)\")\n",
    "\n",
    "runs_args_df_ext.to_excel(pjoin(analysis_path, 'runs_args_df_ext.xlsx'))\n",
    "logger.info(\"saved runs_args_df_ext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring search grid, verifying there are no holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:29.013013Z",
     "start_time": "2021-12-20T13:31:28.983048Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:38 (INFO): search grid has no holes: swept args unique values product (60) == len(runs_meta_df) (60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search grid:\n",
      "{'VP.lambda_decoupling': {10000000, 1000000, 100000, 1000, 10000},\n",
      " 'VP.lr': {0.1, 0.0001, 0.01, 0.001},\n",
      " 'data.num_split': {5000, 5001, 5002}}\n"
     ]
    }
   ],
   "source": [
    "search_grid_dict = {} # if not supplied as arg:set(vals) - inferred from swept_args in runs\n",
    "ignore_NaNs = True\n",
    "# ----------------------------------------------------------\n",
    "if len(search_grid_dict)==0:\n",
    "    search_grid_dict = {arg:set(runs_args_df.loc[arg].unique())-{np.nan} for arg in swept_args}\n",
    "    print(\"Search grid:\")\n",
    "    pprint(search_grid_dict)\n",
    "#     swept_args_nuniques_prod = runs_args_df_ext.query(\"is_swept\")['n_uniques'].prod()\n",
    "# else:\n",
    "swept_args_nuniques_prod = np.prod([len(vals_set) for vals_set in search_grid_dict.values()])\n",
    "\n",
    "if 'cfg.inference' in search_grid_dict and str(search_grid_dict['cfg.inference'])==\"{nan, ''}\":\n",
    "    logger.info(\"search_grid_dict['cfg.inference']=={'', np.nan} -> skipped (some runs ran \"\n",
    "                \"before cfg.inference was added to params)\")\n",
    "    del search_grid_dict['cfg.inference']\n",
    "    swept_args.remove('cfg.inference')\n",
    "    swept_args_nuniques_prod /= 2\n",
    "\n",
    "if 'cfg.seed' in search_grid_dict:\n",
    "    cfg_seeds_sorted = sorted(search_grid_dict['cfg.seed'])\n",
    "if 'data.seed' in search_grid_dict:\n",
    "    data_seeds_sorted = sorted(search_grid_dict['data.seed'])\n",
    "    if np.array_equal(np.array(cfg_seeds_sorted)+5000, np.array(data_seeds_sorted)):\n",
    "        if input(f\"data seeds = cfg seeds +5000, was args.data.seed=-1 so data seeds are to be ignored \"\n",
    "              \"in verifying no holes in search grid (since each data seed = cfg seed + 5000)? [y]/n\")!='n':\n",
    "            swept_args_nuniques_prod /= len(data_seeds_sorted)\n",
    "\n",
    "if swept_args_nuniques_prod == len(runs_meta_df):\n",
    "    logger.info(f\"search grid has no holes: swept args unique values product ({swept_args_nuniques_prod}) == len(runs_meta_df) ({len(runs_meta_df)})\")\n",
    "else:\n",
    "    warning = f\"search grid has holes: swept args unique values product ({swept_args_nuniques_prod}) != len(runs_meta_df) ({len(runs_meta_df)})\"\n",
    "    if input(f\"{warning} -> continue? y/[n]\")!='y':\n",
    "        raise RuntimeError(\"aborted by user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:29.028969Z",
     "start_time": "2021-12-20T13:31:29.015008Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VP.lambda_decoupling': {10000000, 1000000, 100000, 1000, 10000}, 'VP.lr': {0.1, 0.0001, 0.01, 0.001}, 'data.num_split': {5000, 5001, 5002}}\n"
     ]
    }
   ],
   "source": [
    "print(search_grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:29.043929Z",
     "start_time": "2021-12-20T13:31:29.030964Z"
    }
   },
   "outputs": [],
   "source": [
    "analyze_overfitting = False\n",
    "# analyze_overfitting = True\n",
    "# ----------------------------\n",
    "\n",
    "if analyze_overfitting:\n",
    "    histories = {'accuracy - micro <val>':[],\n",
    "                 'accuracy - micro <train-val diff>':[]}\n",
    "    for run in tqdm(runs):\n",
    "        for metric in histories:\n",
    "            run_hist_df = run.history()\n",
    "            series = run_hist_df[metric].dropna()\n",
    "            series.index = range(len(series))\n",
    "            series.name = run.id\n",
    "            histories[metric].append(series)\n",
    "    for metric in histories:\n",
    "        histories[metric] = pd.concat(histories[metric], axis=1)\n",
    "\n",
    "    runs_swept_args_df = runs_args_df_ext.query(\"is_swept\").T\n",
    "    runs_swept_args_df.drop(index=['is_swept', 'n_uniques'], inplace=True)\n",
    "    runs_swept_args_df.sort_values(by=list(runs_swept_args_df.columns), ascending=True, inplace=True)\n",
    "    display(runs_swept_args_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:29.074286Z",
     "start_time": "2021-12-20T13:31:29.046921Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_size = (4,4)\n",
    "line_opacity = 0.7\n",
    "line_width = 1.1\n",
    "x_label = 'accuracy - micro <train-val diff>'\n",
    "y_label = 'accuracy - micro <val>'\n",
    "\n",
    "format_as_percents = True\n",
    "\n",
    "args_to_keep = {}\n",
    "# args_to_keep['training.wd'] = [0, 1]\n",
    "\n",
    "legend_font_size = 10\n",
    "legend_loc='upper left'\n",
    "legend_anchor_bbox=(0.5, -0.85)\n",
    "# ylim = [0.3, 0.45]\n",
    "# xlim = [0, 0.2]\n",
    "ylim = []\n",
    "xlim = []\n",
    "# -----------------------------------\n",
    "if analyze_overfitting:\n",
    "    for arg in args_to_keep:\n",
    "        if arg not in runs_swept_args_df.columns:\n",
    "            raise RuntimeError(f\"'{arg}' is in args_to_keep but not in swept args, remove from args_to_keep\")\n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "\n",
    "    for run_id in runs_swept_args_df.index:\n",
    "        skip = False\n",
    "        for arg in args_to_keep:\n",
    "            val = runs_swept_args_df.loc[run_id, arg]\n",
    "            if val not in args_to_keep[arg]:\n",
    "                skip = True\n",
    "        if skip:\n",
    "            continue\n",
    "        label = ', '.join(runs_swept_args_df.loc[run_id].astype(str).tolist())\n",
    "        x = histories[x_label][run_id].dropna()\n",
    "        y = histories[y_label][run_id].dropna()\n",
    "        if len(x)<len(y):\n",
    "            y = y[x.index]\n",
    "        else:\n",
    "            x = x[y.index]\n",
    "        plt.plot(x, y, '-', label=label, alpha=line_opacity, linewidth=line_width)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    legend_title = ', '.join(runs_swept_args_df.columns.tolist())\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    \n",
    "    plt.legend(title=legend_title,\n",
    "               fontsize=legend_font_size, title_fontsize=legend_font_size,\n",
    "               loc=legend_loc, bbox_to_anchor=legend_anchor_bbox,\n",
    "              )\n",
    "    if format_as_percents:\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(1.))\n",
    "        ax.xaxis.set_major_formatter(PercentFormatter(1.))\n",
    "\n",
    "    if len(xlim)>0:\n",
    "        plt.xlim(xlim)\n",
    "    if len(ylim)>0:\n",
    "        plt.ylim(ylim)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Analysis:\", os.path.split(analysis_path)[-1])\n",
    "    print(\"Search grid:\")\n",
    "    pprint(search_grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing selected run metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RUN NAME (not from summary)', 'harmonic/VisProd+EM: val',\n",
       "       'harmonic/VisProd+EM: test', 'total acc/adapted VisProd: test',\n",
       "       'seen/adapted VisProd: test', 'seen/VisProd+EM: test', '_timestamp',\n",
       "       'seen/VisProd: train', 'total acc/VisProd+unseen-EM: val',\n",
       "       'total acc/VisProd: val', 'harmonic/VisProd+unseen-EM: val',\n",
       "       'train-val time (s)', 'total acc/VisProd: train',\n",
       "       'harmonic/adapted VisProd: test', 'harmonic/VisProd: val',\n",
       "       'total acc/VisProd+unseen-EM: test', 'harmonic/VisProd: test',\n",
       "       'seen/VisProd: test', 'total acc/VisProd+EM: val',\n",
       "       'total acc/VisProd+EM: test', 'seen/VisProd+unseen-EM: test',\n",
       "       'harmonic/VisProd+unseen-EM: test', 'joint acc/val',\n",
       "       'seen/VisProd: val', 'seen/VisProd+EM: val', 'closed/VisProd: test ',\n",
       "       'seen/VisProd+unseen-EM: val', 'closed/adapted VisProd: test ',\n",
       "       'unseen/adapted VisProd: test', 'unseen/VisProd+EM: val',\n",
       "       'total acc/VisProd: test', 'closed/VisProd+EM: val ',\n",
       "       'closed/VisProd+unseen-EM: val ', 'unseen/VisProd+unseen-EM: val',\n",
       "       'closed/VisProd+unseen-EM: test ', 'closed/VisProd+EM: test ',\n",
       "       'unseen/VisProd+unseen-EM: test', 'unseen/VisProd: val',\n",
       "       'joint acc/train', 'unseen/VisProd+EM: test', 'seen acc/train',\n",
       "       'unseen/VisProd: test', '_wandb', 'harmonic acc/val', 'seen acc/val',\n",
       "       'unseen acc/val', 'shape acc/train', 'shape acc/val', '_runtime',\n",
       "       'color acc/val', 'color acc/train', 'best epoch', 'last epoch', '_step',\n",
       "       'epoch', 'color lr/train', 'shape lr/train',\n",
       "       'total acc/VisProd+EM: test ', 'seen/VisProd+unseen-EM: test ',\n",
       "       'loss_was_finite', 'harmonic/VisProd+EM: test ',\n",
       "       'harmonic/VisProd: train', 'harmonic acc/train',\n",
       "       'total acc/VisProd: test ', 'seen/VisProd+EM: val ',\n",
       "       'total acc/VisProd+EM: val ', 'unseen/adapted VisProd: test ',\n",
       "       'total acc/VisProd+unseen-EM: test ', 'phase',\n",
       "       'unseen/VisProd+EM: val ', 'unseen/VisProd+EM: test ',\n",
       "       'harmonic/VisProd+EM: val ', 'harmonic/VisProd+unseen-EM: val ',\n",
       "       'seen/adapted VisProd: test ', 'seen/VisProd: test ',\n",
       "       'seen/VisProd+EM: test ', 'total acc/VisProd+unseen-EM: val ',\n",
       "       'early_stopped', 'closed/VisProd: test', 'unseen/VisProd: train',\n",
       "       'closed/adapted VisProd: test', 'harmonic/adapted VisProd: test ',\n",
       "       'total acc/adapted VisProd: test ', 'unseen/VisProd: test ',\n",
       "       'closed/VisProd+EM: test', 'seen/VisProd+unseen-EM: val ',\n",
       "       'unseen/VisProd+unseen-EM: test ', 'harmonic/VisProd+unseen-EM: test ',\n",
       "       'unseen/VisProd+unseen-EM: val ', 'closed/VisProd+unseen-EM: test',\n",
       "       'closed/VisProd: val', 'closed/VisProd+EM: val',\n",
       "       'harmonic/VisProd: test ', 'closed/VisProd+unseen-EM: val',\n",
       "       'closed/VisProd: train', 'unseen acc/train'],\n",
       "      dtype='object', name='metric')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_metrics_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.359346Z",
     "start_time": "2021-12-20T13:31:29.077283Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:59:39 (INFO): analyzing selected metrics\n",
      "2022-05-02 17:59:39 (INFO): saved runs_swept_args_and_selected_metrics_df\n"
     ]
    }
   ],
   "source": [
    "selected_metrics = \"\"\"\n",
    "harmonic/VisProd: val, harmonic/VisProd+EM: val, harmonic/VisProd+unseen-EM: val,\n",
    "harmonic/VisProd: test, harmonic/VisProd+EM: test, harmonic/VisProd+unseen-EM: test,\n",
    "last epoch, best epoch\n",
    "\"\"\"\n",
    "# selected_metrics = \"\"\"\n",
    "# accuracy - micro <train>, accuracy - micro <val>, accuracy - micro <test>,\n",
    "# accuracy - macro <val>, accuracy - macro <test>,\n",
    "# mean prior RMSE <train>, mean prior RMSE <val>,\n",
    "# mean prior RMSE <test>, mean prior MAE <val>, mean prior MAE <test>, last epoch\n",
    "# \"\"\"\n",
    "# ----------------------------------------------------------------\n",
    "logger.info(\"analyzing selected metrics\")\n",
    "\n",
    "runs_metrics_df_T = runs_metrics_df.T\n",
    "runs_metrics_df_T.drop(index='n_uniques str', inplace=True)\n",
    "selected_metrics = [splt.strip() for splt in selected_metrics.replace('\\n','').split(',')]\n",
    "\n",
    "runs_selected_metrics_df = runs_metrics_df_T[selected_metrics]\n",
    "\n",
    "# runs_selected_metrics_df.to_excel(pjoin(analysis_path, 'runs_selected_metrics_df.xlsx'))\n",
    "# logger.info(\"saved runs_selected_metrics_df\")\n",
    "\n",
    "runs_swept_args_df = runs_args_df_ext.query(\"is_swept\").T\n",
    "runs_swept_args_and_selected_metrics_df = runs_swept_args_df.merge(runs_selected_metrics_df, left_index=True, right_index=True)\n",
    "runs_swept_args_and_selected_metrics_df.to_excel(pjoin(analysis_path, 'runs_swept_args_and_selected_metrics_df.xlsx'))\n",
    "logger.info(\"saved runs_swept_args_and_selected_metrics_df\")\n",
    "#\n",
    "# runs_summary_swept_args_and_selected_metrics_df = runs_summary_df.merge(runs_swept_args_and_selected_metrics_df, left_index=True, right_index=True)\n",
    "# runs_summary_swept_args_and_selected_metrics_df.to_excel(pjoin(analysis_path, 'runs_summary_swept_args_and_selected_metrics_df.xlsx'))\n",
    "# logger.info(\"saved runs_summary_swept_args_and_selected_metrics_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.375304Z",
     "start_time": "2021-12-20T13:31:30.361369Z"
    }
   },
   "outputs": [],
   "source": [
    "# runs_selected_metrics_df_non_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.423182Z",
     "start_time": "2021-12-20T13:31:30.409214Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# baseline_df_path = pjoin('analysis', 'cifar10_test_size=8000_with_acc.xlsx')\n",
    "# baseline_df = pd.read_excel(baseline_df_path, index_col=0, header=[0,1,2])\n",
    "# baseline_df = pd.read_excel(baseline_df_path, index_col=0)\n",
    "# baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.439150Z",
     "start_time": "2021-12-20T13:31:30.426169Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = 'cifar10'\n",
    "# baseline = 'em:BCTS'\n",
    "# metric = 'adapted_acc_micro'\n",
    "\n",
    "# query = f\"dataset=='{dataset}' & model=='{baseline}'\"\n",
    "# bs_model_df = baseline_df.query(query).copy()\n",
    "\n",
    "# for metric in ['adapted_acc_micro', 'delta_acc']:\n",
    "#     bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[1:-1].split())\\\n",
    "#         .apply(lambda x: np.array([float(val) for val in x]))\n",
    "#     bs_model_df[metric+' mean'] = bs_model_df[metric].apply(np.mean)\n",
    "# bs_model_df['unadapted acc mean'] = bs_model_df['adapted_acc_micro mean'] - bs_model_df['delta_acc mean']\n",
    "# bs_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis defitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating seeds & optimizing hyper-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.455094Z",
     "start_time": "2021-12-20T13:31:30.445119Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# qshow(runs_swept_args_and_selected_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding optimal runs:\n",
    "\n",
    "Optimizing metric_to_optimize by searching over the optiaml swept_args that are not in args_to_fix, keeping args_to_fix fixed in the search, such that each set of values in the outer product of args_to_fix gets a row in the final opt_df, of the (optimal) hyper-parameters that gave the optimal metric_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.502967Z",
     "start_time": "2021-12-20T13:31:30.461079Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aggs_to_use = ['median', 'mean', 'sem']\n",
    "aggs_to_use = ['mean', 'sem']\n",
    "# aggs_to_use = ['mean', 'std']\n",
    "# aggs_to_use = ['mean', 'std', 'sem']\n",
    "# aggs_to_use = ['mean']\n",
    "\n",
    "ddof = 1\n",
    "show_ddof = False\n",
    "# show_ddof = True\n",
    "\n",
    "# short_arg_names = True # taking the last part in name, split by dots, e.g. 'training.lr' - > 'lr'\n",
    "short_arg_names = False\n",
    "# ---------------------------------------------------------------------\n",
    "def agg():\n",
    "    assert direction in [-1, 1]\n",
    "    std_col, sem_col = '', ''\n",
    "    non_reduced_args = list(set(swept_args) - {'data.num_split', 'cfg.seed'})\n",
    "    if len(non_reduced_args)==0:\n",
    "        logger.error(\"len(non_reduced_args)==0!\")\n",
    "    else:\n",
    "        logger.info(\"aggregating seeds\")\n",
    "        df = runs_swept_args_and_selected_metrics_df.copy()\n",
    "\n",
    "        agg_size = df.groupby(non_reduced_args, as_index=True).size()\n",
    "\n",
    "        selected_metrics_split = []\n",
    "        selected_metrics_no_phases = []\n",
    "        for full_metric in selected_metrics:\n",
    "            if '/' in full_metric:\n",
    "                metric, phase = full_metric.split('/')\n",
    "            else:\n",
    "                metric = full_metric\n",
    "                phase = ''\n",
    "            selected_metrics_split.append((metric, phase))\n",
    "            selected_metrics_no_phases.append(metric)\n",
    "\n",
    "        # good only if no NaNs in metric columns:\n",
    "        # agg_mean = df.groupby(non_reduced_args, as_index=True).mean()\n",
    "        # agg_mean.columns = pd.MultiIndex.from_tuples([[col, 'mean'] for col in selected_metrics])\n",
    "        # agg_sem = df.groupby(non_reduced_args, as_index=True).sem(ddof=0)\n",
    "        # agg_sem.columns = pd.MultiIndex.from_tuples([[col, 'sem'] for col in selected_metrics])\n",
    "        # agg_median = df.groupby(non_reduced_args, as_index=True).median()\n",
    "        # agg_median.columns = pd.MultiIndex.from_tuples([[col, 'median'] for col in selected_metrics])\n",
    "\n",
    "        # dropping NaNs per metric column, with all swept_args columns:\n",
    "        dfs_partial_no_na = []\n",
    "        for metric in selected_metrics:\n",
    "            df_ = df[swept_args+[metric]].dropna(how='any')\n",
    "            df_[metric] = df_[metric].astype(float)\n",
    "            dfs_partial_no_na.append(df_)\n",
    "\n",
    "        agg_dfs = []\n",
    "        for agg in aggs_to_use:\n",
    "            if agg=='mean':\n",
    "                agg_means = [df_.groupby(non_reduced_args, as_index=True).mean() for df_ in dfs_partial_no_na]\n",
    "                agg_mean = pd.concat(agg_means, axis=1, join='outer')\n",
    "                agg_mean.columns = pd.MultiIndex.from_tuples(\n",
    "                                                [[metric, phase, 'mean'] for metric, phase in selected_metrics_split])\n",
    "                agg_dfs.append(agg_mean)\n",
    "\n",
    "            elif agg=='std':\n",
    "                agg_stds = [df_.groupby(non_reduced_args, as_index=True).std(ddof=ddof) for df_ in dfs_partial_no_na]\n",
    "                agg_std = pd.concat(agg_stds, axis=1, join='outer')\n",
    "                if show_ddof:\n",
    "                    std_col = f'std(ddof={ddof})'\n",
    "                else:\n",
    "                    std_col = 'std'\n",
    "                agg_std.columns = pd.MultiIndex.from_tuples(\n",
    "                                                [[metric, phase, std_col] for metric, phase in selected_metrics_split])\n",
    "                agg_dfs.append(agg_std)\n",
    "\n",
    "            elif agg=='sem':\n",
    "                agg_sems = [df_.groupby(non_reduced_args, as_index=True).sem(ddof=ddof) for df_ in dfs_partial_no_na]\n",
    "                agg_sem = pd.concat(agg_sems, axis=1, join='outer')\n",
    "                if show_ddof:\n",
    "                    sem_col = f'sem(ddof={ddof})'\n",
    "                else:\n",
    "                    sem_col = 'sem'\n",
    "                agg_sem.columns = pd.MultiIndex.from_tuples(\n",
    "                                                [[metric, phase, sem_col] for metric, phase in selected_metrics_split])\n",
    "                agg_dfs.append(agg_sem)\n",
    "\n",
    "            elif agg == 'median':\n",
    "                agg_medians = [df_.groupby(non_reduced_args, as_index=True).median() for df_ in dfs_partial_no_na]\n",
    "                agg_median = pd.concat(agg_medians, axis=1, join='outer')\n",
    "                agg_median.columns = pd.MultiIndex.from_tuples(\n",
    "                                                [[metric, phase, 'median'] for metric, phase in selected_metrics_split])\n",
    "                agg_dfs.append(agg_median)\n",
    "\n",
    "        # grouping levels\n",
    "        agg_df = pd.concat(agg_dfs, axis=1, join='outer')\n",
    "        seeds_col = 'seeds (cfg & data)'\n",
    "        agg_df[('', '', seeds_col)] = agg_size\n",
    "        agg_df = agg_df.reindex(columns=set(agg_df.columns.get_level_values(level=0)), level=0)  # group level 0\n",
    "        agg_df = agg_df.reindex(columns=set(agg_df.columns.get_level_values(level=1)), level=1)  # group level 1\n",
    "\n",
    "        # re-ordering columns\n",
    "        non_reduced_arg_cols = []\n",
    "        non_reduced_arg_cols_renamed = []\n",
    "        agg_df.reset_index(inplace=True)\n",
    "        for arg in args_to_fix:\n",
    "            non_reduced_arg_cols.append((arg, '', ''))\n",
    "            non_reduced_arg_cols_renamed.append(('', '', arg))\n",
    "\n",
    "        args_to_opt = list(set(non_reduced_args) - set(args_to_fix))\n",
    "        for arg in args_to_opt:\n",
    "            non_reduced_arg_cols.append((arg, '', ''))\n",
    "            non_reduced_arg_cols_renamed.append(('', '', arg))\n",
    "\n",
    "        metric_cols = []\n",
    "        for metric, phase in selected_metrics_split:\n",
    "            for agg in aggs_to_use:\n",
    "                if show_ddof:\n",
    "                    if agg == 'sem':\n",
    "                        agg_ = sem_col\n",
    "                    if agg == 'std':\n",
    "                        agg_ = std_col\n",
    "                else:\n",
    "                    agg_ = agg\n",
    "                metric_cols.append((metric, phase, agg_))\n",
    "\n",
    "        ordered_cols = non_reduced_arg_cols + [('', '', seeds_col)] + metric_cols\n",
    "\n",
    "        # cols_no_size = agg_df.columns.to_list()\n",
    "        # cols_no_size.remove(('seeds (model+data)', '', ''))\n",
    "        # agg_df = agg_df[[('seeds (model+data)', '', '')]+cols_no_size] # re-ordering\n",
    "        # first_cols = ['seeds (model+data)', 'accuracy - micro', 'mean prior RMSE', 'mean prior MAE']\n",
    "        # cols_no_first = agg_df.columns.to_list()\n",
    "        # cols_no_first.remove(first_cols)\n",
    "        agg_df = agg_df[ordered_cols]\n",
    "        # renaming arg cols from (arg, '', '') to ('', '', arg)\n",
    "        if short_arg_names:\n",
    "            non_reduced_arg_cols_renamed = [('','', arg.split('.')[-1].strip()) for _,_,arg in non_reduced_arg_cols_renamed]\n",
    "        agg_df.columns = pd.MultiIndex.from_tuples(\n",
    "            non_reduced_arg_cols_renamed + [('', '', seeds_col)] + metric_cols)\n",
    "\n",
    "        agg_df.to_excel(pjoin(analysis_path, 'agg_df.xlsx'))\n",
    "        logger.info(\"saved agg_df\")\n",
    "\n",
    "        # optimizing -------------------------------------------\n",
    "        logger.info(\"optimizing\")\n",
    "        opt_df_indices = []\n",
    "\n",
    "        args_to_fix_vals = {arg: search_grid_dict[arg] for arg in args_to_fix}\n",
    "        args_to_fix_vals_grid = ParameterGrid(args_to_fix_vals)\n",
    "\n",
    "        for args_to_fix_vals_dict in args_to_fix_vals_grid:\n",
    "            filtered_df = agg_df.copy()\n",
    "            for arg, val in args_to_fix_vals_dict.items():\n",
    "                arg_ = arg\n",
    "                if short_arg_names:\n",
    "                    arg_ = arg.split('.')[-1].strip()\n",
    "                filtered_df = filtered_df[filtered_df[('', '', arg_)] == val]\n",
    "            opt_row = direction * filtered_df[metric_to_optimize].argmax()\n",
    "            opt_idx = filtered_df.index[opt_row]\n",
    "            opt_df_indices.append(opt_idx)\n",
    "\n",
    "        opt_df = agg_df.iloc[opt_df_indices].copy()\n",
    "\n",
    "        for arg in args_to_opt:\n",
    "            arg_ = arg\n",
    "            if short_arg_names:\n",
    "                arg_ = arg.split('.')[-1].strip()\n",
    "            opt_df.rename(columns={arg_: f'OPT {arg_}'}, level=2, inplace=True)\n",
    "        args_to_fix_short = [arg.split('.')[-1].strip() for arg in args_to_fix]\n",
    "        args_to_fix_short_str = ', '.join(args_to_fix_short)\n",
    "        agg_filename = f'opt_df by {metric_to_optimize} grouped by {args_to_fix_short_str}.xlsx'.replace(':', '')\n",
    "        opt_df.to_excel(pjoin(analysis_path, agg_filename))\n",
    "        logger.info(\"saved opt_df\")\n",
    "        return agg_df, opt_df, std_col, sem_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.518920Z",
     "start_time": "2021-12-20T13:31:30.506957Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# x = [1,12,120,1020]\n",
    "# plt.plot(x, [1,2,3,4])\n",
    "# ax = plt.gca()\n",
    "\n",
    "# plt.xscale('log')\n",
    "\n",
    "# formatter = ScalarFormatter()\n",
    "# formatter.set_scientific(False)\n",
    "# ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# plt.xticks(ticks=x)\n",
    "\n",
    "# fig.patch.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x = args_to_fix[0]\n",
    "* y = optimal 'best val' results of metric_to_plot = (metric_to_optimize[0], phase_to_plot, metric_to_optimize[2])\n",
    "* hues (seaborn terminology) = all combinations of args_to_fix[1:]\n",
    "* line style by arg_for_line_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:30.597211Z",
     "start_time": "2021-12-20T13:31:30.523914Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot():\n",
    "    df = opt_df.copy()\n",
    "    plotted_series_list = []\n",
    "\n",
    "    if error_bands != 'none':\n",
    "        if error_bands not in aggs_to_use:\n",
    "            raise RuntimeError(f\"got error_bands='{error_bands}' which are not in aggs_to_use='{aggs_to_use}'!\")\n",
    "    \n",
    "    phase = metric_to_plot[1]\n",
    "    if len(baselines)>0:\n",
    "        if phase != 'test':\n",
    "            raise RuntimeError(\n",
    "                f\"got baseline_compare=True but phase_to_plot='{phase}' (comparison - only on 'test')!\")\n",
    "    \n",
    "    if short_arg_names:\n",
    "        args_to_fix_ = args_to_fix_short\n",
    "    else:\n",
    "        args_to_fix_ = args_to_fix\n",
    "    x_col = args_to_fix_[0]\n",
    "\n",
    "    if len(set(vals_to_keep.keys()) - set(args_to_fix_))>0:\n",
    "        raise RuntimeError(\"there are keys in vals_to_keep that does not appear in args_to_fix_!\")\n",
    "\n",
    "    hues_dict = {}\n",
    "    if len(args_to_fix) == 1:\n",
    "        hues = [None]\n",
    "    else:\n",
    "        for i_hue_col in range(1, len(args_to_fix_)):\n",
    "            hue_col = args_to_fix_[i_hue_col]\n",
    "            hues_dict[hue_col] = df[('', '', hue_col)].unique()\n",
    "            if i_hue_col>0 and hue_col in vals_to_keep:\n",
    "                non_existing_vals_to_keep = set(vals_to_keep[hue_col]) - set(hues_dict[hue_col])\n",
    "                if len(non_existing_vals_to_keep)>0:\n",
    "                    raise RuntimeError(f\"remove {non_existing_vals_to_keep} from vals_to_keep['{hue_col}'] to avoid confusion, they don't appear in opt_df!\")\n",
    "                hues_dict[hue_col] = set(hues_dict[hue_col]) & set(vals_to_keep[hue_col])\n",
    "            hues_dict[hue_col] = list(hues_dict[hue_col])\n",
    "            hues_dict[hue_col].sort()\n",
    "        hues =  ParameterGrid(hues_dict)\n",
    "        # since ParameterGrid is ordered by keys abc and not by key order:\n",
    "        hues_df = pd.DataFrame(hues).sort_values(by=args_to_fix_[1:])[args_to_fix_[1:]]\n",
    "\n",
    "    line_style = '-o' # in case there are no other\n",
    "    if arg_for_line_style in hues_dict:\n",
    "        hue_vals = hues_dict[arg_for_line_style]\n",
    "        line_styles = {hue_val: '-.o' for hue_val in hue_vals}\n",
    "        line_styles[hue_vals[0]] = '-o'\n",
    "        if len(hue_vals)>1: line_styles[hue_vals[1]] = '--o'\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "\n",
    "    for i_hue in range(len(hues)):\n",
    "        if len(args_to_fix) == 1:\n",
    "            hue_df = df.copy()\n",
    "            if 'model.variant' in swept_args:\n",
    "                label = 'DLS'\n",
    "            else: # architecture is the same for all runs\n",
    "                architecture = runs_args_df.loc['model.architecture'].iloc[0]\n",
    "                label = architecture\n",
    "        else:\n",
    "            hue = hues_df.iloc[i_hue].to_dict()\n",
    "            hue_df = df.copy()\n",
    "            line_style = '-o'\n",
    "            for hue_col, hue_val in hue.items():\n",
    "                hue_df = hue_df[hue_df[('', '', hue_col)]==hue_val]\n",
    "                if hue_col==arg_for_line_style and hue_col in hues_dict:\n",
    "                    line_style = line_styles[hue_val]\n",
    "    #         label = legend_separator.join([str(hue_val) for hue_col, hue_val in hue.items() \n",
    "    #                            if len(hues_dict[hue_col])>1 or len(baselines)>1])\n",
    "            # label = prefix+legend_separator.join([str(hue_val) for hue_col, hue_val in hue.items()])\n",
    "            label = []\n",
    "            for hue_col, hue_val in hue.items():\n",
    "                key = hue_val\n",
    "                if minimal_mode:\n",
    "#                     if hue_col == 'model.variant':\n",
    "#                         key = hue_val.split(' ')[1].replace('->', '-')\n",
    "#                         key = hue_val.replace('->', '-')\n",
    "#                         key = hue_val.replace('*', '')\n",
    "#                     print(f\"hue_col: '{hue_col}'\")\n",
    "                    if hue_col == 'data.input_source':\n",
    "                        if key == 'output_layer=-1 logits':\n",
    "                            key = 'dim = 100'\n",
    "                        elif key == 'output_layer=-2 logits':\n",
    "                            key = 'dim = 512'\n",
    "                        elif key == 'output_layer=-4 logits':\n",
    "                            key = 'dim = 1024'\n",
    "                    elif hue_col == 'data.dataset':\n",
    "                        if 'synthetic 1D' in hue_val:\n",
    "                            if 'train-CLIPPED' in hue_val:\n",
    "                                key = 'train-clipped'\n",
    "                            else:\n",
    "                                key = 'no clipping'\n",
    "                \n",
    "                if isinstance(key, float):\n",
    "#                     key = '%.1e'%key\n",
    "                    key = str(key)\n",
    "                else:\n",
    "                    key = str(key)\n",
    "                label.append(key)\n",
    "            \n",
    "            if prefix =='':\n",
    "                label = legend_separator.join(label)\n",
    "            else:\n",
    "                label = f'{prefix}: '+ legend_separator.join(label)\n",
    "\n",
    "        if sort_x:\n",
    "#             if x_col=='training.train.rand_transforms':\n",
    "#                 hue_df[('', '', x_col)] = hue_df[('', '', x_col)].str.replace('off', '0')\n",
    "#                 hue_df[('', '', x_col)] = hue_df[('', '', x_col)].str.replace(' mean cls std, zero clip', '')\n",
    "            hue_df.sort_values(('', '', x_col), ascending=True, inplace=True)\n",
    "\n",
    "        x = hue_df[('', '', x_col)].copy()\n",
    "        x_ = x.copy()\n",
    "        if log10_xvals:\n",
    "            x = x.astype(float)\n",
    "            x[x!=0] = np.log10(x[x!=0])\n",
    "#         if x_col in vals_to_keep:\n",
    "#             x = x[x.isin(vals_to_keep[x_col])]\n",
    "\n",
    "        y = hue_df[metric_to_plot]\n",
    "        if one_minus_metric:\n",
    "            y = 1-y\n",
    "        base_line = plt.plot(x, y, line_style, label=label, alpha=line_opacity, linewidth=line_width)\n",
    "        plotted_series = y.copy()\n",
    "        plotted_series.index = x.copy()\n",
    "        plotted_series.index.name = x_col\n",
    "        if error_bands=='none':\n",
    "            plotted_series.name = label\n",
    "        else:\n",
    "            plotted_series.name = (label, metric_to_plot[-1])\n",
    "        plotted_series_list.append(plotted_series)\n",
    "\n",
    "        if error_bands == 'std':\n",
    "            err_name = std_col\n",
    "        elif error_bands == 'sem':\n",
    "            err_name = sem_col\n",
    "        elif error_bands == 'none':\n",
    "            err_name = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if err_name is not None:\n",
    "            err_col = (metric_to_plot[0], metric_to_plot[1], err_name)\n",
    "            err = hue_df[err_col]\n",
    "            plt.fill_between(x, y + err, y - err, facecolor=base_line[0].get_color(), alpha=err_opacity)\n",
    "\n",
    "            if err_in_table:\n",
    "                plotted_series = err.copy()\n",
    "                plotted_series.index = x.copy()\n",
    "                plotted_series.index.name = x_col\n",
    "                plotted_series.name = (label, err_name)\n",
    "                plotted_series_list.append(plotted_series)\n",
    "\n",
    "    # plotting baseline\n",
    "    if len(baselines)>0:\n",
    "        if args_to_fix[0] not in ['experiment_alpha', 'data.test.Dirichlet_alpha']:\n",
    "            raise RuntimeError(\"baseline comparison requires x = alpha, set args_to_fix to have experiment_alpha as first arg\")\n",
    "\n",
    "        baseline_df = pd.read_excel(baseline_df_path, index_col=0)\n",
    "\n",
    "        for baseline in baselines:\n",
    "\n",
    "            if baseline != 'unadapted':\n",
    "                if metric_to_plot[0]=='accuracy - micro':\n",
    "                    metric = 'adapted_acc_micro'\n",
    "                elif metric_to_plot[0]=='accuracy - macro':\n",
    "                    metric = 'adapted_acc_macro'\n",
    "                elif metric_to_plot[0]=='epoch prior loss':\n",
    "                    metric = 'KLDiv'\n",
    "                elif metric_to_plot[0]=='epoch prior MAE':\n",
    "                    metric = 'prior_MAE'\n",
    "                elif metric_to_plot[0]=='epoch prior MSE':\n",
    "                    metric = 'prior_MSE'\n",
    "                elif metric_to_plot[0]=='calibration MSE':\n",
    "                    metric = 'calib_MSE'\n",
    "                elif metric_to_plot[0]=='calibration MAE':\n",
    "                    metric = 'calib_MAE'\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                if baseline_x_as_DLS:\n",
    "                    query = f\"dataset=='{dataset}' & model=='{baseline}' & alpha in @x_\"\n",
    "                else:\n",
    "                    query = f\"dataset=='{dataset}' & model=='{baseline}'\"\n",
    "                \n",
    "#                 print(\"query:\", query)\n",
    "                bs_model_df = baseline_df.query(query)[['alpha', metric]].copy()\n",
    "#                 print(\"bs_model_df:\", bs_model_df)\n",
    "\n",
    "                x_bs = bs_model_df['alpha'].copy()\n",
    "                if log10_xvals:\n",
    "                        x_bs[x_bs!=0] = np.log10(x_bs[x_bs!=0])\n",
    "                \n",
    "                if 'synthetic' in dataset:\n",
    "                    bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[1:-1].split(', '))\\\n",
    "                        .apply(lambda x: np.array([float(val) for val in x]))\n",
    "                else:\n",
    "                    bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[1:-1].split())\\\n",
    "                        .apply(lambda x: np.array([float(val) for val in x]))\n",
    "                if baseline_tests_as_DLS:\n",
    "                    if n_loaded_tests_ == 0:\n",
    "                        if zero_n_loaded_tests_ui != 'y':\n",
    "                            raise RuntimeError(\"aborted according to user decision\")\n",
    "                    else:\n",
    "                        bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[:n_loaded_tests_])\n",
    "\n",
    "                y_bs = bs_model_df[metric]\n",
    "                y_bs_mean = y_bs.apply(np.mean)\n",
    "\n",
    "            else: # baseline == 'unadapted':\n",
    "                if metric_to_plot[0]!='accuracy - micro':\n",
    "                    logger.info(\"skipping baseline=='unadapted' since metric_to_plot[0]!='accuracy - micro'\")\n",
    "                    continue\n",
    "                else:\n",
    "                    model = baseline_df['model'].unique().tolist()[0] # selecting the 1st model since unadapted accuracy is the same for all models\n",
    "                    if baseline_x_as_DLS:\n",
    "                        query = f\"dataset=='{dataset}' & model=='{model}' & alpha in @x_\"\n",
    "                    else:\n",
    "                        query = f\"dataset=='{dataset}' & model=='{model}'\"\n",
    "\n",
    "                    bs_model_df = baseline_df.query(query)[\n",
    "                        ['alpha', 'adapted_acc_micro', 'delta_acc']].copy()\n",
    "                    x_bs = bs_model_df['alpha'].copy()\n",
    "                    if log10_xvals:\n",
    "                        x_bs[x_bs!=0] = np.log10(x_bs[x_bs!=0])\n",
    "                    for metric in ['adapted_acc_micro', 'delta_acc']:\n",
    "                        bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[1:-1].split())\\\n",
    "                            .apply(lambda x: np.array([float(val) for val in x]))\n",
    "                        \n",
    "                        if baseline_tests_as_DLS:\n",
    "                            if n_loaded_tests_ == 0:\n",
    "                                if zero_n_loaded_tests_ui != 'y':\n",
    "                                    raise RuntimeError(\"aborted according to user decision\")\n",
    "                            else:\n",
    "                                bs_model_df[metric] = bs_model_df[metric].apply(lambda x: x[:n_loaded_tests_])\n",
    "\n",
    "                    y_bs = bs_model_df['adapted_acc_micro'] - bs_model_df['delta_acc']\n",
    "                    y_bs_mean = y_bs.apply(np.mean)\n",
    "\n",
    "\n",
    "            if one_minus_metric:\n",
    "                y_bs_mean = 1-y_bs_mean\n",
    "            \n",
    "            bs_label = baseline\n",
    "            if baseline == 'em:VS':\n",
    "                bs_label = 'VS+EM'\n",
    "                base_line = plt.plot(x_bs, y_bs_mean, '-o', label=bs_label, alpha=line_opacity,\n",
    "                                 linewidth=line_width, color='k')\n",
    "            elif baseline == 'unadapted':\n",
    "                base_line = plt.plot(x_bs, y_bs_mean, '--o', label=bs_label, alpha=line_opacity,\n",
    "                                 linewidth=line_width, color='k')\n",
    "            else:\n",
    "                base_line = plt.plot(x_bs, y_bs_mean, '-o', label=bs_label, alpha=line_opacity,\n",
    "                                 linewidth=line_width)\n",
    "\n",
    "            plotted_series = y_bs_mean.copy()\n",
    "            plotted_series.index = x_bs.copy()\n",
    "            plotted_series.index.name = x_col\n",
    "            if error_bands=='none':\n",
    "                plotted_series.name = bs_label\n",
    "            else:\n",
    "                plotted_series.name = (bs_label, metric_to_plot[-1])\n",
    "            plotted_series_list.append(plotted_series)\n",
    "\n",
    "            if err_name is not None:\n",
    "                if error_bands=='std':\n",
    "                    err_bs = y_bs.apply(lambda x: np.std(x, ddof=1))\n",
    "                elif error_bands=='sem':\n",
    "                    err_bs = y_bs.apply(lambda x: scipy.stats.sem(x, ddof=1))\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                plt.fill_between(x_bs, y_bs_mean + err_bs, y_bs_mean - err_bs,\n",
    "                                 facecolor=base_line[0].get_color(), alpha=err_opacity)\n",
    "                if err_in_table:\n",
    "                    plotted_series = err_bs.copy()\n",
    "                    plotted_series.index = x_bs.copy()\n",
    "                    plotted_series.index.name = x_col\n",
    "                    plotted_series.name = (baseline, err_name)\n",
    "                    plotted_series_list.append(plotted_series)\n",
    "\n",
    "    \n",
    "    # figure configuration\n",
    "    \n",
    "    x_label = x_col\n",
    "    if minimal_mode:\n",
    "        if x_col=='experiment_alpha':\n",
    "            if isinstance(experiment_alpha_on, str):\n",
    "                x_label = r'$\\alpha_{%s}$'%experiment_alpha_on\n",
    "            else:\n",
    "                x_label = r'$\\alpha_{exp}$'\n",
    "        elif x_col=='data.test.Dirichlet_alpha':\n",
    "            x_label = r'$\\alpha_{test}$'\n",
    "        elif x_col=='training.batch_size':\n",
    "            x_label = r'batch size'\n",
    "        elif x_col=='training.lr':\n",
    "            x_label = r'learning rate'\n",
    "        elif x_col=='training.loss_prior_lambda':\n",
    "            x_label = r'$\\lambda_{prior}$'\n",
    "        elif x_col=='training.mixup_alpha':\n",
    "            x_label = 'mixup alpha'\n",
    "        elif x_col=='training.rand_transform_strength':\n",
    "            x_label = 'random transform strength'\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    if minimal_figure_box:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "    if one_minus_metric:\n",
    "        if minimal_mode and metric_to_plot[0]=='accuracy - micro':\n",
    "            y_label = f\"{metric_to_plot[1]} error\"\n",
    "        else:\n",
    "            y_label = f\"1 - ({metric_to_plot[0]}) <{metric_to_plot[1]}>\"\n",
    "    else:\n",
    "        y_label = f\"{metric_to_plot[0]} <{metric_to_plot[1]}>\"\n",
    "#     if show_y_lower_is_better:\n",
    "#         y_label = y_label + '\\n' + r\"$better\\ <----\\ worse$\"\n",
    "    plt.yscale(yscale)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    if not minimal_mode:\n",
    "        title = f\"hyper-tuned {'+' if direction==1 else '-'}{metric_to_optimize}\"\n",
    "        # if len(vals_to_keep)>0 and len(baselines)==0 and:\n",
    "        #     title += f\"\\nvals_to_keep: {vals_to_keep}\"\n",
    "        if error_bands != 'none':\n",
    "            title += f\"\\nerror bands: {error_bands}\"\n",
    "        plt.title(title)\n",
    "\n",
    "    if format_y_as_percents:\n",
    "        # ax.yaxis.set_major_formatter(PercentFormatter(1.))\n",
    "        # ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f %%'))\n",
    "        format = '{:.%s%%}'%(y_percents_precision)\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: format.format(y)))\n",
    "\n",
    "    if n_y_ticks>0:\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(n_y_ticks))\n",
    "\n",
    "    if xscale!='linear' and pd.api.types.is_numeric_dtype(x):\n",
    "        if xscale=='symlog':\n",
    "            plt.xscale('symlog', linthresh=symlog_linthresh)\n",
    "        elif xscale=='log':\n",
    "            plt.xscale(xscale)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        if log10_xvals:\n",
    "            if scientific_xaxis:\n",
    "    #             plt.xticks(ticks=x.values, labels=['$10^{%d}$'%val if val!=0 else '0' for val in x.values])\n",
    "                plt.xticks(ticks=x.values, labels=['$10^{%d}$'%val for val in x.values])\n",
    "            else:\n",
    "                plt.xticks(ticks=x.values, labels=x_.values)\n",
    "        else:\n",
    "            plt.xticks(ticks=x.values, labels=x.values)\n",
    "            formatter = ScalarFormatter()\n",
    "            formatter.set_scientific(scientific_xaxis)\n",
    "            ax.xaxis.set_major_formatter(formatter)\n",
    "        \n",
    "        \n",
    "#     if xticks_by_x:\n",
    "#         plt.xticks(ticks=x.values, labels=x_.values)\n",
    "    if x_precision>=0:\n",
    "        ax.xaxis.set_major_formatter(FormatStrFormatter(f'%.{x_precision}f'))\n",
    "\n",
    "    # plt.locator_params(axis='x', nbins=4)\n",
    "    # ax.xaxis.set_major_locator(ticker.MaxNLocator(4)) # https://matplotlib.org/stable/gallery/ticks_and_spines/tick-locators.html\n",
    "\n",
    "    legend_title = ''\n",
    "    if len(args_to_fix)>1:\n",
    "        legend_keys = []\n",
    "        for arg in hue.keys():\n",
    "        #     if len(hues_dict[arg])>1:\n",
    "    #         if arg=='experiment_alpha':\n",
    "    #             key = 'alpha'\n",
    "            key = str(arg)\n",
    "            if minimal_mode:\n",
    "                if arg=='loss_prior_lambda':\n",
    "                    key = r'$\\lambda_{prior}$'\n",
    "                elif arg=='experiment_alpha':\n",
    "#                     key = r'$\\alpha_{%s}$'%experiment_alpha_on\n",
    "                    if isinstance(experiment_alpha_on, str):\n",
    "                        key = r'$\\alpha_{%s}$'%experiment_alpha_on\n",
    "                    else:\n",
    "                        key = r'$\\alpha_{exp}$'\n",
    "                elif arg=='model.variant':\n",
    "                    key = 'model'\n",
    "                elif arg=='data.test.shuffle_sampled_indices':\n",
    "                    key = 'shuffled tests'\n",
    "                elif arg=='data.normalize_by_softmax':\n",
    "                    key = 'normalized inputs'\n",
    "                elif arg=='training.batch_size':\n",
    "                    key = 'batch size'\n",
    "                elif arg=='training.rand_transform_strength':\n",
    "                    key = 'random transform strength'\n",
    "                elif arg=='data.calibration':\n",
    "                    key = 'calibrated'\n",
    "                elif arg=='data.test.reorder_by_clustering' or arg=='eval.test.reorder_by':\n",
    "                    key = 'test reordered by'\n",
    "                elif arg=='training.loss_labels_weights':\n",
    "                    key = 'label loss weights'\n",
    "                elif arg=='data.dataset':\n",
    "                    key = 'dataset'\n",
    "                elif arg=='data.input_source':\n",
    "                    key = 'inputs'\n",
    "                elif arg=='training.wd':\n",
    "                    key = 'wd'\n",
    "#                 elif arg=='training.mixup_alpha':\n",
    "#                     key = 'mixup alpha'\n",
    "                elif arg=='training.mixup.alpha':\n",
    "                    key = 'mixup.alpha'\n",
    "                elif arg=='training.mixup.label_mix':\n",
    "                    key = r'mixup.label_mix'\n",
    "\n",
    "            legend_keys.append(key) \n",
    "        legend_title = legend_separator.join(legend_keys)\n",
    "    \n",
    "    if show_legend:\n",
    "        if show_legend_title:\n",
    "            legend_title_ = legend_title\n",
    "        else:\n",
    "            legend_title_ = ''\n",
    "        \n",
    "        if manual_legend_labels is None:\n",
    "            ax.legend(title=legend_title_, frameon=legend_box,\n",
    "                       loc=legend_loc,\n",
    "                       bbox_to_anchor=legend_bbox,\n",
    "        #                bbox_transform=fig.transFigure,\n",
    "                       fontsize=title_font_size, title_fontsize=title_font_size)\n",
    "        else:\n",
    "            if input(\"use manual legend labels? y/[n] \")=='y':\n",
    "                ax.legend(manual_legend_labels,\n",
    "                       title=legend_title_, frameon=legend_box,\n",
    "                       loc=legend_loc,\n",
    "                       bbox_to_anchor=legend_bbox,\n",
    "                       fontsize=title_font_size, title_fontsize=title_font_size)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    if grid:\n",
    "        plt.grid()\n",
    "    # plt.ylim([0.01,0.04])\n",
    "    # plt.xlim(right=2e5)\n",
    "    fig.patch.set_facecolor('white')\n",
    "#     plt.show()\n",
    "\n",
    "    if error_bands=='none':\n",
    "        print(metric_to_plot)\n",
    "    else:\n",
    "        print(metric_to_plot[:-1])\n",
    "    plotted_df = pd.concat(plotted_series_list, axis=1)\n",
    "    if minimal_mode:\n",
    "        if plotted_df.index.name=='data.test.Dirichlet_alpha':\n",
    "            plotted_df.index.name = 'test alpha'\n",
    "    if log10_xvals:\n",
    "        plotted_df.index.name = f'log10({plotted_df.index.name})'\n",
    "    plotted_df.columns.name = legend_title\n",
    "    \n",
    "    if yscale=='linear':\n",
    "        display(plotted_df.T.style.format(\"{:.1%}\".format))\n",
    "    else:\n",
    "        display(plotted_df.T.style.format(\"{:.1e}\".format))\n",
    "    return fig, plotted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:55.446161Z",
     "start_time": "2021-12-20T13:31:53.797963Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = input(\"cifar 10/[100] v4/100 (v3)/(s)ynthetic sigma=1.5? \")\n",
    "# if dataset=='10':\n",
    "#     baseline_df_path = pjoin('analysis', 'cifar10 test_size=8e3 shuffled tests RAW.xlsx')\n",
    "#     dataset='cifar10'\n",
    "# elif dataset=='s':\n",
    "#     baseline_df_path = pjoin('analysis', 'synthetic 1D RAW.xlsx')\n",
    "#     dataset='synthetic 1D uniform sigma=1.5'\n",
    "# elif dataset=='v3':\n",
    "#     baseline_df_path = pjoin('analysis',\n",
    "#                              'cifar100 test_size=1e4 DLS tests multinomial resnet18 v3 RAW.xlsx')\n",
    "#     dataset='cifar100 resnet 18 v3'\n",
    "# elif dataset=='':\n",
    "#     baseline_df_path = pjoin('analysis',\n",
    "#                              'cifar100 test_size=1e4 DLS tests multinomial resnet18 v4 RAW.xlsx')\n",
    "#     dataset='cifar100 resnet 18 v4'\n",
    "# else:\n",
    "#     raise NotImplementedError\n",
    "\n",
    "###### plotting ################\n",
    "line_opacity = 0.6\n",
    "err_opacity = 0.2\n",
    "line_width = 2\n",
    "\n",
    "show_legend=True\n",
    "grid = False\n",
    "minimal_figure_box = True\n",
    "\n",
    "xscale = 'linear' # for log scale change xlog=True\n",
    "# xscale = 'log' # DON'T USE - it messes things, my hack: if log10_xvals: x = log(x) before plotting\n",
    "# xscale = 'symlog'\n",
    "# symlog_linthresh = 1\n",
    "log10_xvals = True\n",
    "sort_x = True\n",
    "\n",
    "# err_in_table = True\n",
    "err_in_table = False\n",
    "\n",
    "fig_size = (5,4)\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "# matplotlib.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:56.696117Z",
     "start_time": "2021-12-20T13:31:56.679132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VP.lambda_decoupling', 'VP.lr', 'data.num_split']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swept_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:57.725151Z",
     "start_time": "2021-12-20T13:31:57.297828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 18:11:04 (INFO): aggregating seeds\n",
      "2022-05-02 18:11:04 (INFO): saved agg_df\n",
      "2022-05-02 18:11:04 (INFO): optimizing\n",
      "2022-05-02 18:11:04 (INFO): saved opt_df\n"
     ]
    }
   ],
   "source": [
    "metric_to_optimize = ('harmonic', 'VisProd+EM: val', 'mean')\n",
    "direction = 1  # metric optimization direction: 1 = max, -1 = min\n",
    "\n",
    "# metric_to_optimize = ('epoch prior loss', 'best val', 'mean')\n",
    "# direction = -1  # metric optimization direction: 1 = max, -1 = min\n",
    "# ------------------------------------------------------------------------\n",
    "# args_to_fix = swept_args\n",
    "\n",
    "extra_args_to_fix = []\n",
    "# extra_args_to_fix = ['model.variant']\n",
    "\n",
    "extra_args_to_fix = ['VP.lr']\n",
    "base_args_to_fix = ['VP.lambda_decoupling']\n",
    "\n",
    "args_to_fix = base_args_to_fix + extra_args_to_fix\n",
    "# ------------------------------------------------------------------------\n",
    "agg_df, opt_df, std_col, sem_col = agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:58.227583Z",
     "start_time": "2021-12-20T13:31:58.221600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i_arg in range(1, len(args_to_fix)):\n",
    "    arg = args_to_fix[i_arg]\n",
    "    vals = opt_df[('', '', arg.split('.')[-1] if short_arg_names else arg)].unique()\n",
    "    print(f\"i_arg={i_arg} ({arg}) vals: {sorted(vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:58.702172Z",
     "start_time": "2021-12-20T13:31:58.478734Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.architecture'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_engine\u001B[39m.\u001B[39;49mget_loc(casted_key)\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'model.architecture'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32md:\\PythonProjects\\causal_comp\\analyze_wandb_results.ipynb Cell 59'\u001B[0m in \u001B[0;36m<cell line: 91>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=80'>81</a>\u001B[0m manual_legend_labels \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=81'>82</a>\u001B[0m \u001B[39m# manual_legend_labels = [\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=82'>83</a>\u001B[0m \u001B[39m#             'DLS: reordered by EM+VS y_pred' ,\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=83'>84</a>\u001B[0m \u001B[39m#             'DLS: not reordered',\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=87'>88</a>\u001B[0m \n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=88'>89</a>\u001B[0m \u001B[39m# ------------------------------------------------\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000062?line=90'>91</a>\u001B[0m fig, plotted_df \u001B[39m=\u001B[39m plot()\n",
      "\u001B[1;32md:\\PythonProjects\\causal_comp\\analyze_wandb_results.ipynb Cell 51'\u001B[0m in \u001B[0;36mplot\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000054?line=55'>56</a>\u001B[0m         label \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mDLS\u001B[39m\u001B[39m'\u001B[39m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000054?line=56'>57</a>\u001B[0m     \u001B[39melse\u001B[39;00m: \u001B[39m# architecture is the same for all runs\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000054?line=57'>58</a>\u001B[0m         architecture \u001B[39m=\u001B[39m runs_args_df\u001B[39m.\u001B[39;49mloc[\u001B[39m'\u001B[39;49m\u001B[39mmodel.architecture\u001B[39;49m\u001B[39m'\u001B[39;49m]\u001B[39m.\u001B[39miloc[\u001B[39m0\u001B[39m]\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000054?line=58'>59</a>\u001B[0m         label \u001B[39m=\u001B[39m architecture\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProjects/causal_comp/analyze_wandb_results.ipynb#ch0000054?line=59'>60</a>\u001B[0m \u001B[39melse\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=963'>964</a>\u001B[0m axis \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39maxis \u001B[39mor\u001B[39;00m \u001B[39m0\u001B[39m\n\u001B[0;32m    <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=965'>966</a>\u001B[0m maybe_callable \u001B[39m=\u001B[39m com\u001B[39m.\u001B[39mapply_if_callable(key, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mobj)\n\u001B[1;32m--> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=966'>967</a>\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_getitem_axis(maybe_callable, axis\u001B[39m=\u001B[39;49maxis)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\indexing.py:1202\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1199'>1200</a>\u001B[0m \u001B[39m# fall thru to straight lookup\u001B[39;00m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1200'>1201</a>\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_key(key, axis)\n\u001B[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1201'>1202</a>\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_get_label(key, axis\u001B[39m=\u001B[39;49maxis)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001B[0m, in \u001B[0;36m_LocIndexer._get_label\u001B[1;34m(self, label, axis)\u001B[0m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1150'>1151</a>\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_get_label\u001B[39m(\u001B[39mself\u001B[39m, label, axis: \u001B[39mint\u001B[39m):\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1151'>1152</a>\u001B[0m     \u001B[39m# GH#5667 this will fail if the label is not present in the axis.\u001B[39;00m\n\u001B[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexing.py?line=1152'>1153</a>\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mobj\u001B[39m.\u001B[39;49mxs(label, axis\u001B[39m=\u001B[39;49maxis)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\generic.py:3876\u001B[0m, in \u001B[0;36mNDFrame.xs\u001B[1;34m(self, key, axis, level, drop_level)\u001B[0m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/generic.py?line=3873'>3874</a>\u001B[0m             new_index \u001B[39m=\u001B[39m index[loc]\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/generic.py?line=3874'>3875</a>\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/generic.py?line=3875'>3876</a>\u001B[0m     loc \u001B[39m=\u001B[39m index\u001B[39m.\u001B[39;49mget_loc(key)\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/generic.py?line=3877'>3878</a>\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(loc, np\u001B[39m.\u001B[39mndarray):\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/generic.py?line=3878'>3879</a>\u001B[0m         \u001B[39mif\u001B[39;00m loc\u001B[39m.\u001B[39mdtype \u001B[39m==\u001B[39m np\u001B[39m.\u001B[39mbool_:\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\Comp1101\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_engine\u001B[39m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m err:\n\u001B[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mKeyError\u001B[39;00m(key) \u001B[39mfrom\u001B[39;00m \u001B[39merr\u001B[39;00m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mTypeError\u001B[39;00m:\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001B[0m     \u001B[39m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001B[0m     \u001B[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001B[0m     \u001B[39m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/envs/Comp1101/lib/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'model.architecture'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimal_mode = True\n",
    "# minimal_mode = False\n",
    "prefix = ''\n",
    "# prefix = 'DLS'\n",
    "# prefix = 'EM'\n",
    "replace_arc_in_variant = True\n",
    "\n",
    "# error_bands = 'std'\n",
    "# error_bands = 'sem'\n",
    "error_bands = 'none'\n",
    "line_opacity = 0.6\n",
    "err_opacity = 0.2\n",
    "line_width = 2\n",
    "\n",
    "# phase_to_plot = 'best train'\n",
    "# phase_to_plot = 'best val'\n",
    "phase_to_plot = 'VisProd+EM: val'\n",
    "metric_to_plot = metric_to_optimize[0]\n",
    "# metric_to_plot = 'epoch prior loss'\n",
    "\n",
    "metric_to_plot = (metric_to_plot, phase_to_plot, 'mean')\n",
    "\n",
    "vals_to_keep = {} # vals to keep for each arg in args_to_fix elements[1:]\n",
    "# vals_to_keep['variant'] = [val for val in y_vals if val not in \n",
    "#         ['mean2 b->24->12', 'mean2 a->8']]\n",
    "# vals_to_keep['loss_prior_lambda'] = [val for val in y_vals if val <1e6]\n",
    "# vals_to_keep['training.rand_transforms'] = [val for val in y_vals if val != '30 mean cls std, zero clip']\n",
    "# vals_to_keep['experiment_alpha'] = ['0.1']\n",
    "\n",
    "baselines = [\n",
    "]\n",
    "\n",
    "baseline_x_as_DLS = True\n",
    "baseline_tests_as_DLS = True # calculating mean and error only for n_loaded_tests baseline results, for paired comparison with DLS tests; n_loaded_tests_ = n_loaded_tests * len(search_grid_dict['cfg.seed'])\n",
    "# baseline_tests_as_DLS = False\n",
    "\n",
    "# arg_for_line_style = 'model.variant'\n",
    "# arg_for_line_style = 'data.input_source'\n",
    "arg_for_line_style = ''\n",
    "\n",
    "one_minus_metric = True # y = 1-y\n",
    "yscale = 'linear'\n",
    "format_y_as_percents = True\n",
    "\n",
    "# one_minus_metric = False\n",
    "# yscale = 'log'\n",
    "# format_y_as_percents = False\n",
    "\n",
    "# y_percents_precision = 1\n",
    "y_percents_precision = 0\n",
    "n_y_ticks = 4\n",
    "\n",
    "show_legend=True\n",
    "# show_legend=False\n",
    "# show_legend_title = True\n",
    "show_legend_title = False\n",
    "# legend_box = True\n",
    "legend_box = False\n",
    "legend_loc = 'best'\n",
    "legend_separator = ' | '\n",
    "legend_bbox = None\n",
    "# legend_loc = 'upper left'\n",
    "# legend_bbox = (0, -0.2)\n",
    "# legend_bbox = (-0.2, -0.2)\n",
    "\n",
    "\n",
    "log10_xvals = True\n",
    "# log10_xvals = False\n",
    "xscale = 'linear'\n",
    "# xscale = 'symlog'\n",
    "symlog_linthresh = 0.1\n",
    "x_precision = -1\n",
    "scientific_xaxis = False\n",
    "# scientific_xaxis = True\n",
    "xticks_by_x = True\n",
    "# xticks_by_x = False\n",
    "sort_x = True\n",
    "\n",
    "title_font_size=14\n",
    "\n",
    "manual_legend_labels = None\n",
    "# manual_legend_labels = [\n",
    "#             'DLS: reordered by EM+VS y_pred' ,\n",
    "#             'DLS: not reordered',\n",
    "#             'DLS: reordered by y_pred',\n",
    "#             'DLS: reordered by y_true',\n",
    "#             'EM+VS', 'unadapted']\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "fig, plotted_df = plot()\n",
    "# plt.xlim([0,3])\n",
    "# plt.ylim([0.34,0.41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T13:31:32.420645Z",
     "start_time": "2021-12-20T13:31:32.408667Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Analysis:\", os.path.split(analysis_path)[-1])\n",
    "print(\"Search grid:\")\n",
    "pprint(search_grid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading & analyzing result files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T20:19:19.289478Z",
     "start_time": "2021-10-17T20:19:19.277481Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # vals_to_keep_ = {}\n",
    "# vals_to_keep_ = {'data.test.Dirichlet_alpha': 0.1}\n",
    "# # vals_to_keep_ = {'experiment_alpha': 0.1}\n",
    "# # ----------------------\n",
    "# opt_df_ = opt_df.copy()\n",
    "# for arg,val in vals_to_keep_.items():\n",
    "#     opt_df_ = opt_df_[opt_df_[('','',arg)]==val]\n",
    "# display(opt_df_)\n",
    "# assert len(opt_df_)==1, f\"len(opt_df_) = {len(opt_df_)} -> change args_to_fix or vals_to_keep_ to have a single row for opt args!\"\n",
    "\n",
    "# opt_args = {}\n",
    "# for arg in args_to_opt:\n",
    "#     arg_ = 'OPT ' + arg.split('.')[-1].strip()\n",
    "#     opt_args[arg] = (opt_df_[('','',arg_)].iloc[0])\n",
    "# print(\"opt_args:\", opt_args)\n",
    "\n",
    "# opt_runs = runs_args_df.T.copy()\n",
    "# for arg,val in vals_to_keep_.items():\n",
    "#     if arg=='variant':\n",
    "#         arg = 'model.variant'\n",
    "#     opt_runs = opt_runs[opt_runs[arg]==val]\n",
    "# for arg,val in opt_args.items():\n",
    "#     opt_runs = opt_runs[opt_runs[arg]==val]\n",
    "# display(opt_runs)\n",
    "# opt_run_ids = list(opt_runs.index)\n",
    "# print(\"opt_runs:\", opt_run_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T20:27:35.617373Z",
     "start_time": "2021-10-17T20:27:35.604405Z"
    }
   },
   "outputs": [],
   "source": [
    "# # val_to_keep_ = {}\n",
    "# val_to_keep_ = {'data.test.Dirichlet_alpha': 0.1}\n",
    "# # ----------------------\n",
    "# agg_df_ = agg_df.copy()\n",
    "# for arg,val in val_to_keep_.items():\n",
    "#     agg_df_ = agg_df[agg_df[('', '', arg)] == val]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading results for selected runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T20:59:09.372427Z",
     "start_time": "2021-10-30T20:59:01.937525Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# selected_runs = opt_run_ids\n",
    "selected_runs = ['1f2woqmw']\n",
    "# ------------------------------\n",
    "results_dict = {}\n",
    "for run_id in tqdm(selected_runs):\n",
    "    run = api.run('%s/%s/%s'%(user, project, run_id))\n",
    "    file = run.file(\"results.torch\").download(root='wandb/temp', replace=(True))\n",
    "    results = torch.load(file.name)\n",
    "    results_dict[run_id] = results\n",
    "    file.close()\n",
    "    os.remove(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `UNCOMPLETED` Plotting training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, use section 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:06.116618Z",
     "start_time": "2021-10-18T11:54:06.105525Z"
    }
   },
   "outputs": [],
   "source": [
    "# metric = 'accuracy - micro'\n",
    "# # -----------------------------\n",
    "\n",
    "# training_stats = {f'{metric} <train>':[], f'{metric} <val>':[]}\n",
    "# for run_id in selected_runs:\n",
    "#     results = results_dict[run_id]\n",
    "#     for phase in ['train', 'val']:\n",
    "#         metric_dict = {}\n",
    "#         for epoch in results[phase]:\n",
    "#             metric_dict[epoch] = results[phase][epoch][metric]\n",
    "#         metric_df = pd.DataFrame(metric_dict)\n",
    "#         raise\n",
    "# # label_dists = pd.concat(tests_stats['label dist'], axis=1)\n",
    "# # accuracies = pd.concat(tests_stats['accuracy'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:06.131490Z",
     "start_time": "2021-10-18T11:54:06.119586Z"
    }
   },
   "outputs": [],
   "source": [
    "# tests_stats = {'label dist':[], 'accuracy':[]}\n",
    "# for run_id in selected_runs:\n",
    "#     results = results_dict[run_id]\n",
    "#     for key in results:\n",
    "#         if 'test ' in key:\n",
    "#             cls_stats_df = pd.DataFrame(results[key]['class'])\n",
    "#             cls_stats_df['label dist'] = cls_stats_df['positives']/cls_stats_df['positives'].sum()\n",
    "#             cls_stats_df.sort_values(by='positives', ascending=False, inplace=True)\n",
    "#             cls_stats_df.index = range(len(cls_stats_df))\n",
    "#             cls_stats_df.rename(columns={\n",
    "#                 col:f'{col} <{key}> <{run_id}>' for col in ['label dist', 'accuracy']}, inplace=True)\n",
    "#             for col in ['label dist', 'accuracy']:\n",
    "#                 tests_stats[col].append(cls_stats_df[f'{col} <{key}> <{run_id}>'])\n",
    "# label_dists = pd.concat(tests_stats['label dist'], axis=1)\n",
    "# accuracies = pd.concat(tests_stats['accuracy'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T20:59:10.479654Z",
     "start_time": "2021-10-30T20:59:10.389861Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phase = 'test'\n",
    "# phase = 'train'\n",
    "# phase = 'val'\n",
    "epoch = -1 # if phase != 'test'\n",
    "# ----------------------------------------------------\n",
    "if phase!='test':\n",
    "    assert len(selected_runs) == 1\n",
    "    epoch = sorted(results_dict[run_id][phase].keys())[epoch]\n",
    "\n",
    "stats = {'label dist':[], 'accuracy':[]}\n",
    "if phase == 'test':\n",
    "    for run_id in selected_runs:\n",
    "        results = results_dict[run_id]\n",
    "        for key in results:\n",
    "            if 'test ' in key:\n",
    "                cls_stats_df = pd.DataFrame(results[key]['class'])\n",
    "                cls_stats_df['label dist'] = cls_stats_df['positives']/cls_stats_df['positives'].sum()\n",
    "                cls_stats_df.sort_values(by='positives', ascending=False, inplace=True)\n",
    "                cls_stats_df.index = range(len(cls_stats_df))\n",
    "                cls_stats_df.rename(columns={\n",
    "                    col:f'{col} <{key}> <{run_id}>' for col in ['label dist', 'accuracy']}, inplace=True)\n",
    "                for col in ['label dist', 'accuracy']:\n",
    "                    stats[col].append(cls_stats_df[f'{col} <{key}> <{run_id}>'])\n",
    "    label_dists = pd.concat(stats['label dist'], axis=1)\n",
    "    accuracies = pd.concat(stats['accuracy'], axis=1)\n",
    "else:\n",
    "    for run_id in selected_runs:\n",
    "        results = results_dict[run_id][phase][epoch]\n",
    "        cls_stats_df = pd.DataFrame(results['class'])\n",
    "        cls_stats_df['label dist'] = cls_stats_df['positives']/cls_stats_df['positives'].sum()\n",
    "        cls_stats_df.sort_values(by='positives', ascending=False, inplace=True)\n",
    "        cls_stats_df.index = range(len(cls_stats_df))\n",
    "        cls_stats_df.rename(columns={\n",
    "            col:f'{col} <{phase}> <{run_id}>' for col in ['label dist', 'accuracy']}, inplace=True)\n",
    "        for col in ['label dist', 'accuracy']:\n",
    "            stats[col].append(cls_stats_df[f'{col} <{phase}> <{run_id}>'])\n",
    "    label_dists = pd.concat(stats['label dist'], axis=1)\n",
    "    accuracies = pd.concat(stats['accuracy'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T20:59:22.256307Z",
     "start_time": "2021-10-30T20:59:21.933158Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figsize = (4,4)\n",
    "# figsize = None\n",
    "# style = '-o'\n",
    "style = '-'\n",
    "line_opacity = 0.8\n",
    "err_opacity = 0.4\n",
    "line_width = 1.5\n",
    "# ------------------------------\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=figsize)\n",
    "ax1.tick_params(axis='y', colors='r')\n",
    "ax1.set_ylabel('class error', color='r')\n",
    "label_dists_mean = label_dists.mean(axis=1)\n",
    "df = accuracies\n",
    "means = (1-df).mean(axis=1)\n",
    "x = means.index\n",
    "y = means\n",
    "err = (1-df).std(axis=1)\n",
    "\n",
    "base_line = ax1.plot(x, y, style, label='class error', color='r',\n",
    "                     alpha=line_opacity, linewidth=line_width)\n",
    "ax1.fill_between(x, y + err, y - err, facecolor=base_line[0].get_color(), alpha=err_opacity)\n",
    "\n",
    "x = label_dists_mean.index\n",
    "y = label_dists_mean\n",
    "err = label_dists.std(axis=1)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('label distribution', color='b')\n",
    "ax2.tick_params(axis='y', colors='b')\n",
    "base_line = ax2.plot(x, y, style, label='label distribution', color='b',\n",
    "                     alpha=line_opacity, linewidth=line_width)\n",
    "ax2.fill_between(x, y + err, y - err, facecolor=base_line[0].get_color(), alpha=err_opacity)\n",
    "# plt.yscale('log')\n",
    "# fig.legend()\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "format = '{:.%s%%}'%(0)\n",
    "ax1.yaxis.set_major_formatter(FuncFormatter(lambda y, _: format.format(y)))\n",
    "# ax1.locator_params(nbins=4)\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(lambda y, _: format.format(y)))\n",
    "ax1.locator_params(axis='y', nbins=4)\n",
    "ax2.locator_params(axis='y', nbins=4)\n",
    "# plt.locator_params(axis='x', nbins=3)\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "ax1.set_xlabel(\"class index (descending probability)\")\n",
    "# if 'vals_to_keep_' in globals() and opt_args in globals():\n",
    "#     print(f\"class stats for {vals_to_keep_} {opt_args}\")\n",
    "if phase == 'test':\n",
    "    plt.title(\"test class distributions\")\n",
    "else:\n",
    "    plt.title(f\"{phase} class distributions on epoch {epoch}\")\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a single run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading/selecting run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:07.432717Z",
     "start_time": "2021-10-18T11:54:07.418722Z"
    }
   },
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:10.071614Z",
     "start_time": "2021-10-18T11:54:09.293907Z"
    }
   },
   "outputs": [],
   "source": [
    "# run_path = 'ozziko/DLS_6/14u6z3n4'\n",
    "# run = api.run(run_path)\n",
    "# --------------------------------------------------------------\n",
    "if run.state != 'finished':\n",
    "    raise RuntimeError(\"run is unfinished!\")\n",
    "run_history_df = run.history()\n",
    "run_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:10.086574Z",
     "start_time": "2021-10-18T11:54:10.073611Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# best_run_row = runs_selected_metrics_df['accuracy - micro <test>'].argmax()\n",
    "\n",
    "# metric = 'accuracy - micro <best val>'\n",
    "# metric = 'accuracy - micro <test>'\n",
    "# i_top = 0\n",
    "# run_idx = runs_selected_metrics_df[metric].sort_values(ascending=False).index[i_top]\n",
    "\n",
    "# run_idx = '3uabw9aj'\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# run_idx\n",
    "\n",
    "# run_is_selected = False\n",
    "# for run in runs:\n",
    "#     if run.id==run_idx:\n",
    "#         run_is_selected = True\n",
    "\n",
    "# if run_is_selected:\n",
    "#     run_history_df = run.history()\n",
    "#     display(run_history_df)\n",
    "# else:\n",
    "#     print(\"run_idx not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:54:11.493631Z",
     "start_time": "2021-10-18T11:54:11.315081Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_selected_metrics = \"\"\"accuracy - micro <train>, accuracy - micro <val>\"\"\"\n",
    "# run_selected_metrics = \"\"\"accuracy - macro <train>, accuracy - macro <val>\"\"\"\n",
    "# run_selected_metrics = \"\"\"accuracy - micro <train>, accuracy - micro <val>,\n",
    "# accuracy - macro <train>, accuracy - macro <val>\"\"\"\n",
    "# run_selected_metrics = \"\"\" loss per sample <train>, loss per sample <val>\"\"\"\n",
    "# run_selected_metrics = \"\"\"label loss per sample <train>, label loss per sample <val>,\n",
    "# lambda * prior loss per sample <train>, lambda * prior loss per sample <val>\n",
    "# \"\"\"\n",
    "# run_selected_metrics = \"\"\"loss per sample <train>, loss per sample <val>\"\"\"\n",
    "format_y_as_percents = True\n",
    "# format_y_as_percents = False\n",
    "# log_y = True\n",
    "log_y = False\n",
    "y_lim = []\n",
    "# y_lim = [0.9,1]\n",
    "# y_lim = [0,0.5]\n",
    "opacity = 0.7\n",
    "fig_size = (4,4)\n",
    "# fig_size = None\n",
    "# style = '-x'\n",
    "style = '-'\n",
    "linewidth = 2\n",
    "# ---------------------------\n",
    "\n",
    "run_selected_metrics = [splt.strip() for splt in run_selected_metrics.replace('\\n','').split(',')]\n",
    "selected_run_history_df = pd.DataFrame()\n",
    "for metric in run_selected_metrics:\n",
    "    metric_history_series = run_history_df[metric].dropna()\n",
    "    metric_history_series.index = range(len(metric_history_series))\n",
    "    selected_run_history_df[metric] = metric_history_series\n",
    "selected_run_history_df.index.name = 'epoch'\n",
    "\n",
    "parsed_run_selected_metrics = []\n",
    "for metric in run_selected_metrics:\n",
    "    metric_ = metric[:metric.find(' <')]\n",
    "    phase = metric[metric.find('<')+1:metric.find('>')]\n",
    "    parsed_run_selected_metrics.append((metric_, phase))\n",
    "\n",
    "if len(set([metric for metric,phase in parsed_run_selected_metrics]))==1:\n",
    "    same_metric_different_phases = True\n",
    "else:\n",
    "    same_metric_different_phases = False\n",
    "\n",
    "fig = plt.figure(figsize=fig_size)\n",
    "for i, (metric,phase) in enumerate(parsed_run_selected_metrics):\n",
    "    if same_metric_different_phases:\n",
    "        label = phase\n",
    "    else:\n",
    "        label = run_selected_metrics[i]\n",
    "        \n",
    "    plt.plot(selected_run_history_df.iloc[:,i], style, label=label, alpha=opacity, linewidth=linewidth)\n",
    "\n",
    "\n",
    "if same_metric_different_phases:\n",
    "    plt.ylabel(metric)\n",
    "    legend_title = 'phase'\n",
    "else:\n",
    "    legend_title = ''\n",
    "\n",
    "plt.legend(loc='best',\n",
    "#            title='phase',\n",
    "           fontsize=12, title_fontsize=12)\n",
    "    \n",
    "plt.xlabel('epoch')\n",
    "if format_y_as_percents:\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "if log_y:\n",
    "    plt.yscale('log')\n",
    "if len(y_lim)>0:\n",
    "    plt.ylim(y_lim)\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.show()\n",
    "display(selected_run_history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T14:23:37.019089Z",
     "start_time": "2021-11-29T14:23:36.951399Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('analysis/synthetic data comparison.xlsx')\n",
    "df['clip'] = df['dataset'].apply(lambda x:'clip' if 'CLIPPED' in x else 'no clipping')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T14:27:00.835516Z",
     "start_time": "2021-11-29T14:27:00.814602Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['architecture', 'inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T14:25:33.711869Z",
     "start_time": "2021-11-29T14:25:33.691891Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# sns.lineplot(df, x='test alpha', y='test mean micro accuracy', hue='architecture', style='inputs')\n",
    "sns.lineplot(df, x='test alpha', y='n_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T13:32:19.844573Z",
     "start_time": "2021-10-11T13:32:19.805646Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('analysis/resnet3 vs wide resnet.xlsx', index_col='test alpha')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T13:34:49.944343Z",
     "start_time": "2021-10-11T13:34:49.421181Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "df.plot(ax=plt.gca(), style='-o')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('test error')\n",
    "fig.patch.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# agg_std = df.groupby(non_reduced_args, as_index=True).std()\n",
    "# agg_median = df.groupby(non_reduced_args, as_index=True).median()\n",
    "\n",
    "# pivot_df = df.pivot_table(\n",
    "#     index=non_reduced_args, values=selected_metrics, aggfunc=['median', 'mean', 'std'])\n",
    "# if not as_index:\n",
    "#     pivot_df = pivot_df.reset_index()\n",
    "#\n",
    "# # re-indexing column levels by metric -> agg, instead of agg -> metric\n",
    "# pivot_df = pivot_df.reorder_levels(axis=1, order=[1,0])\n",
    "# pivot_df = pivot_df.reindex(selected_metrics, axis=1, level=0)\n",
    "\n",
    "# runs_swept_args_and_selected_metrics_pivot_df = pivot_df\n",
    "# runs_swept_args_and_selected_metrics_pivot_df.to_excel(pjoin(analysis_path, 'runs_swept_args_and_selected_metrics_pivot_df.xlsx'))\n",
    "\n",
    "# # max on cfg.seed\n",
    "# agg_df = df.groupby(list(set(swept_args) - {'cfg.seed'}), as_index=as_index).max()\n",
    "# del agg_df['cfg.seed']\n",
    "#\n",
    "# agg_df1 = agg_df.groupby(non_swept_args, as_index=as_index).mean()\n",
    "# agg_df1['agg'] = 'max(cfg.seed) -> mean(data.seed)'\n",
    "# del agg_df1['data.seed']\n",
    "#\n",
    "# # mean and std on data.seed\n",
    "# agg_df2 = agg_df.groupby(non_swept_args, as_index=as_index).std()\n",
    "# agg_df2['agg'] = 'max(cfg.seed) -> std(data.seed)'\n",
    "# del agg_df2['data.seed']\n",
    "#\n",
    "#\n",
    "# cols = non_swept_args + ['agg'] + selected_metrics\n",
    "# agg_runs_swept_args_and_selected_metrics_df = pd.concat([agg_df1, agg_df2], axis=0)\n",
    "# agg_runs_swept_args_and_selected_metrics_df.sort_values(by=non_swept_args, inplace=True)\n",
    "# agg_runs_swept_args_and_selected_metrics_df = agg_runs_swept_args_and_selected_metrics_df[cols]\n",
    "# agg_runs_swept_args_and_selected_metrics_df.to_excel(pjoin(analysis_path, 'agg_runs_swept_args_and_selected_metrics_df.xlsx'))\n",
    "\n",
    "\n",
    "\n",
    "# #%% early stopping epoch histogram\n",
    "# plt.figure(dpi=150)\n",
    "# early_stop_epoch = runs_selected_metrics_df['best epoch']\n",
    "# early_stop_epoch.hist(ax=plt.gca())\n",
    "# plt.title(\"early stopping epoch histogram\\nmean: %1.1f, std: %1.1f\"%(early_stop_epoch.mean(), early_stop_epoch.std()))\n",
    "# plt.ylabel('counts')\n",
    "# plt.xlabel(\"early stopping epoch\")\n",
    "# plt.grid('on')\n",
    "# plt.show()\n",
    "#\n",
    "# #%%\n",
    "# (1.3**2 + 1.6**2)**0.5\n",
    "#\n",
    "#\n",
    "# #%% comparing sweep args\n",
    "# analysis_name1 = \"simple baseline\"\n",
    "# analysis_name2 = \"simple DLS\"\n",
    "# # -----------------------------------------------------------------------------------------\n",
    "# import pandas as pd\n",
    "# from os.path import join as pjoin\n",
    "#\n",
    "# analysis_path1 = pjoin('analysis', analysis_name1, 'runs_args_df_ext.xlsx')\n",
    "# analysis_path2 = pjoin('analysis', analysis_name2, 'runs_args_df_ext.xlsx')\n",
    "#\n",
    "# runs_args_df_ext1 = pd.read_excel(analysis_path1)\n",
    "# runs_args_df_ext1.set_index(['arg'], inplace=True)\n",
    "# runs_args_df_ext2 = pd.read_excel(analysis_path2)\n",
    "# runs_args_df_ext2.set_index(['arg'], inplace=True)\n",
    "#\n",
    "# non_swept_args1 = runs_args_df_ext1.query(\"~is_swept\").iloc[:,0]\n",
    "# non_swept_args2 = runs_args_df_ext2.query(\"~is_swept\").iloc[:,0]\n",
    "#\n",
    "# non_swept_args_comparison_df = pd.concat([non_swept_args1,non_swept_args2], axis=1, join='outer')\n",
    "# non_swept_args_comparison_df.columns = [analysis_name1,analysis_name2]\n",
    "# non_swept_args_comparison_df.index.name = 'arg'\n",
    "# # non_swept_args_comparison_df['n_unique'] = non_swept_args_comparison_df.astype(str).nunique(axis=1)\n",
    "# non_swept_args_comparison_df['is_equal'] = non_swept_args_comparison_df[analysis_name1] == non_swept_args_comparison_df[analysis_name2]\n",
    "# non_swept_args_comparison_df['is_equal'] = non_swept_args_comparison_df['is_equal'].astype(int)\n",
    "# non_swept_args_comparison_df.to_excel(pjoin('analysis', f'args - {analysis_name1} -VS- {analysis_name2}.xlsx'))\n",
    "#\n",
    "#\n",
    "# #%% reducing by optimal_run_id\n",
    "# reduced_args = 'cfg.seed, data.seed'.split(', ')\n",
    "# reducing_args = list(optimal_sept_args[~optimal_sept_args.index.isin(reduced_args)].index)\n",
    "# optimal_runs_df = runs_summary_and_metrics_df.copy()\n",
    "# for arg in reducing_args:\n",
    "#     optimal_runs_df = optimal_runs_df[optimal_runs_df[arg]==optimal_sept_args[arg]]\n",
    "# optimal_runs_df\n",
    "#\n",
    "#\n",
    "# #%%\n",
    "# metric = 'accuracy - micro <test>'\n",
    "#\n",
    "# logger.info(f\"{metric} mean, std, median: %.3f, %.4f, %.3f\"%(\n",
    "#     optimal_runs_df[metric].mean(), optimal_runs_df[metric].std(), optimal_runs_df[metric].median()\n",
    "# ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "425px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}