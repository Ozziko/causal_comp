{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"vscode\"\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=120\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join as pjoin\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.parametrize as nn_parametrize\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring, Callback\n",
    "from skorch.dataset import ValidSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from ATTOP.data.dataset import sample_negative as ATTOP_sample_negative\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from COSMO_utils import temporary_random_numpy_seed\n",
    "\n",
    "\n",
    "def get_and_update_num_calls(func_ptr):\n",
    "    try:\n",
    "        get_and_update_num_calls.num_calls_cnt[func_ptr] += 1\n",
    "    except AttributeError as e:\n",
    "        if 'num_calls_cnt' in repr(e):\n",
    "            get_and_update_num_calls.num_calls_cnt = defaultdict(int)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    return get_and_update_num_calls.num_calls_cnt[func_ptr]\n",
    "\n",
    "def categorical_histogram(data, labels_list, plot=True, frac=True, plt_show=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    s_counts = pd.Series(data).value_counts()\n",
    "    s_frac = s_counts/s_counts.sum()\n",
    "    hist_dict = s_counts.to_dict()\n",
    "    if frac:\n",
    "        hist_dict = s_frac.to_dict()\n",
    "    hist = []\n",
    "    for ix, _ in enumerate(labels_list):\n",
    "        hist.append(hist_dict.get(ix, 0))\n",
    "\n",
    "    if plot:\n",
    "        pd.Series(hist, index=labels_list).plot(kind='bar')\n",
    "        if frac:\n",
    "            plt.ylim((0,1))\n",
    "        if plt_show:\n",
    "            plt.show()\n",
    "    else:\n",
    "        return np.array(hist, dtype='float32')\n",
    "\n",
    "class DataItem(NamedTuple):\n",
    "    \"\"\" A NamedTuple for returning a Dataset item \"\"\"\n",
    "    feat: torch.Tensor\n",
    "    pos_attr_id: int\n",
    "    pos_obj_id: int\n",
    "    neg_attr_id: int\n",
    "    neg_obj_id: int\n",
    "    image_fname: str\n",
    "\n",
    "\n",
    "class CompDataFromDict():\n",
    "    # noinspection PyMissingConstructor\n",
    "    def __init__(self, dict_data: dict, data_subset: str, data_dir: str):\n",
    "\n",
    "        # define instance variables to be retrieved from struct_data_dict\n",
    "        self.split: str = 'TBD'\n",
    "        self.phase: str = 'TBD'\n",
    "        self.feat_dim: int = -1\n",
    "        self.objs: list = []\n",
    "        self.attrs: list = []\n",
    "        self.attr2idx: dict = {}\n",
    "        self.obj2idx: dict = {}\n",
    "        self.pair2idx: dict = {}\n",
    "        self.seen_pairs: list = []\n",
    "        self.all_open_pairs: list = []\n",
    "        self.closed_unseen_pairs: list = []\n",
    "        self.unseen_closed_val_pairs: list = []\n",
    "        self.unseen_closed_test_pairs: list = []\n",
    "        self.train_data: tuple = tuple()\n",
    "        self.val_data: tuple = tuple()\n",
    "        self.test_data: tuple = tuple()\n",
    "\n",
    "        self.data_dir: str = data_dir\n",
    "\n",
    "        # retrieve instance variables from struct_data_dict\n",
    "        vars(self).update(dict_data)\n",
    "        self.data = dict_data[data_subset]\n",
    "\n",
    "        self.activations = {}\n",
    "        features_dict = torch.load(Path(data_dir) / 'features.t7')\n",
    "        for i, img_filename in enumerate(features_dict['files']):\n",
    "            self.activations[img_filename] = features_dict['features'][i]\n",
    "\n",
    "        self.input_shape = (self.feat_dim,)\n",
    "        self.num_objs = len(self.objs)\n",
    "        self.num_attrs = len(self.attrs)\n",
    "        self.num_seen_pairs = len(self.seen_pairs)\n",
    "        self.shape_obj_attr = (self.num_objs, self.num_attrs)\n",
    "\n",
    "        self.flattened_seen_pairs_mask = self.get_flattened_pairs_mask(self.seen_pairs)\n",
    "        self.flattened_closed_unseen_pairs_mask = self.get_flattened_pairs_mask(self.closed_unseen_pairs)\n",
    "        self.flattened_all_open_pairs_mask = self.get_flattened_pairs_mask(self.all_open_pairs)\n",
    "        self.seen_pairs_joint_class_ids = np.where(self.flattened_seen_pairs_mask)\n",
    "\n",
    "        self.y1_freqs, self.y2_freqs, self.pairs_freqs = self._calc_freqs()\n",
    "        self._just_load_labels = False\n",
    "\n",
    "        self.train_pairs = self.seen_pairs\n",
    "\n",
    "    def sample_negative(self, attr, obj):\n",
    "        return ATTOP_sample_negative(self, attr, obj)\n",
    "\n",
    "    def get_flattened_pairs_mask(self, pairs):\n",
    "        pairs_ids = np.array([(self.obj2idx[obj], self.attr2idx[attr]) for attr, obj in pairs])\n",
    "        flattened_pairs = np.zeros(self.shape_obj_attr, dtype=bool)  # init an array of False\n",
    "        flattened_pairs[tuple(zip(*pairs_ids))] = True\n",
    "        flattened_pairs = flattened_pairs.flatten()\n",
    "        return flattened_pairs\n",
    "\n",
    "    def just_load_labels(self, just_load_labels=True):\n",
    "        self._just_load_labels = just_load_labels\n",
    "\n",
    "    def get_all_labels(self):\n",
    "        attrs = []\n",
    "        objs = []\n",
    "        joints = []\n",
    "        self.just_load_labels(True)\n",
    "        for attrs_batch, objs_batch in self:\n",
    "            if isinstance(attrs_batch, torch.Tensor):\n",
    "                attrs_batch = attrs_batch.cpu().numpy()\n",
    "            if isinstance(objs_batch, torch.Tensor):\n",
    "                objs_batch = objs_batch.cpu().numpy()\n",
    "            joint = self.to_joint_label(objs_batch, attrs_batch)\n",
    "\n",
    "            attrs.append(attrs_batch)\n",
    "            objs.append(objs_batch)\n",
    "            joints.append(joint)\n",
    "\n",
    "        self.just_load_labels(False)\n",
    "        attrs = np.array(attrs)\n",
    "        objs = np.array(objs)\n",
    "        return attrs, objs, joints\n",
    "\n",
    "    def _calc_freqs(self):\n",
    "        y2_train, y1_train, ys_joint_train = self.get_all_labels()\n",
    "        y1_freqs = categorical_histogram(y1_train, range(self.num_objs), plot=False, frac=True)\n",
    "        y1_freqs[y1_freqs == 0] = np.nan\n",
    "        y2_freqs = categorical_histogram(y2_train, range(self.num_attrs), plot=False, frac=True)\n",
    "        y2_freqs[y2_freqs == 0] = np.nan\n",
    "\n",
    "        pairs_freqs = categorical_histogram(ys_joint_train,\n",
    "                                            range(self.num_objs * self.num_attrs),\n",
    "                                            plot=False, frac=True)\n",
    "        pairs_freqs[pairs_freqs == 0] = np.nan\n",
    "        return y1_freqs, y2_freqs, pairs_freqs\n",
    "\n",
    "    def get(self, name):\n",
    "        return vars(self).get(name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_fname, attr, obj = self.data[idx]\n",
    "        pos_attr_id, pos_obj_id = self.attr2idx[attr], self.obj2idx[obj]\n",
    "        if self._just_load_labels:\n",
    "            return pos_attr_id, pos_obj_id\n",
    "\n",
    "        num_calls_cnt = get_and_update_num_calls(self.__getitem__)\n",
    "\n",
    "        negative_attr_id, negative_obj_id = -1, -1  # default values\n",
    "        if self.phase == 'train':\n",
    "            # we set a temp np seed to override a weird issue with\n",
    "            # sample_negative() at __getitem__, where the sampled pairs\n",
    "            # could not be deterministically reproduced:\n",
    "            # Now at each call to _getitem_ we set the seed to a 834276 (chosen randomly) + the number of calls to _getitem_\n",
    "            with temporary_random_numpy_seed(834276 + num_calls_cnt):\n",
    "                # draw a negative pair\n",
    "                negative_attr_id, negative_obj_id = self.sample_negative(attr, obj)\n",
    "\n",
    "        item = DataItem(\n",
    "            feat=self.activations[image_fname],\n",
    "            pos_attr_id=pos_attr_id,\n",
    "            pos_obj_id=pos_obj_id,\n",
    "            neg_attr_id=negative_attr_id,\n",
    "            neg_obj_id=negative_obj_id,\n",
    "            image_fname=image_fname,\n",
    "        )\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def to_joint_label(self, y1_batch, y2_batch):\n",
    "        return (y1_batch * self.num_attrs + y2_batch)\n",
    "\n",
    "\n",
    "def get_data_loaders(train_dataset, valid_dataset, test_dataset, batch_size,\n",
    "                     num_workers=10, test_batchsize=None, shuffle_eval_set=True):\n",
    "    if test_batchsize is None:\n",
    "        test_batchsize = batch_size\n",
    "\n",
    "    pin_memory = True\n",
    "    if num_workers == 0:\n",
    "        pin_memory = False\n",
    "    print('num_workers = ', num_workers)\n",
    "    print('pin_memory = ', pin_memory)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                                   pin_memory=pin_memory)\n",
    "    valid_loader = None\n",
    "    if valid_dataset is not None and len(valid_dataset) > 0:\n",
    "        valid_loader = data.DataLoader(valid_dataset, batch_size=test_batchsize, shuffle=shuffle_eval_set,\n",
    "                                       num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=test_batchsize, shuffle=shuffle_eval_set,\n",
    "                                  num_workers=num_workers, pin_memory=pin_memory)\n",
    "    return test_loader, train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting global combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ao_clevr'\n",
    "data_dir = f'data/{dataset_name}'\n",
    "label_cols = ['shape', 'color']\n",
    "# --------------------------------\n",
    "\n",
    "meta_df = pd.read_csv(pjoin(data_dir, 'objects_metadata.csv'))\n",
    "global_label_combs = meta_df[label_cols].groupby(label_cols).size()\n",
    "global_label_combs.name = 'samples'\n",
    "global_label_combs = global_label_combs.reset_index()\n",
    "global_label_combs.index.name = 'comb idx'\n",
    "global_label_combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_variant = 'VT'\n",
    "seen_seed = 0\n",
    "num_split = 2000\n",
    "# --------------------------------\n",
    "meta_path = Path(f\"{data_dir}/metadata_pickles\")\n",
    "random_state_path = Path(f\"{data_dir}/np_random_state_pickles\")\n",
    "# meta_path = meta_path.expanduser()\n",
    "\n",
    "dict_data = dict()\n",
    "\n",
    "for subset in ['train', 'valid', 'test']:\n",
    "    metadata_full_filename = meta_path / f\"metadata_{dataset_name}__{dataset_variant}_random__comp_seed_{num_split}__seen_seed_{seen_seed}__{subset}.pkl\"\n",
    "    dict_data[f'{subset}'] = deepcopy(pickle.load(open(metadata_full_filename, 'rb')))\n",
    "\n",
    "np_rnd_state_fname = random_state_path / f\"np_random_state_{dataset_name}__{dataset_variant}_random__comp_seed_{num_split}__seen_seed_{seen_seed}.pkl\"\n",
    "np_seed_state = pickle.load(open(np_rnd_state_fname, 'rb'))\n",
    "np.random.set_state(np_seed_state)\n",
    "\n",
    "datasets = {}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    datasets[phase] = CompDataFromDict(dict_data[phase if phase!='val' else 'valid'],\n",
    "        data_subset=f'{phase}_data', data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y_comb = {}, {}\n",
    "\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    dataset = datasets[phase]\n",
    "    data_df = pd.DataFrame(dataset.data, columns=['filename', 'color', 'shape'])\n",
    "    data_df = data_df.merge(global_label_combs.reset_index(), on=label_cols)\n",
    "    Y_comb[phase] = data_df['comb idx'].values.astype('int64')\n",
    "    print(f\"{phase} contains {len(data_df)} samples\")\n",
    "\n",
    "    features = dataset.activations\n",
    "    X[phase] = torch.cat([features[filename].unsqueeze(0)\n",
    "        for filename in data_df['filename']], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03ad60beb30bb83b358f912f6d215553ccbadb86cc2c38cd3e235690c289a64e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Comp1101')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
